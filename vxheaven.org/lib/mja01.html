<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>John Aycock 'Computer Viruses and Malware' (VX heaven)</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
<meta name="Author" content="John Aycock"/>
<meta name="KeyWords" lang="en" content="computer virus, virus, virii,vx, компьютерные вирусы, вирус, вири, Aycock, John,Computer Viruses and Malware, worm, virus, worms, malware, bulletin, stack, conference, memory, buffer, file, viruses, emulator, code, pages, attacker"/>
<meta name="Description" content="Our Internet-connected society increasingly relies on computers.  As a result, attacks on computers from malicious software have never been a bigger concern. Computer Viruses and Malware draws together hundreds of sources to provide an unprecedented view of malicious software and its countermeasures. This book discusses both the technical and human factors involved in computer viruses, worms, and anti-virus software.  It also looks at the application of malicious software to computer crime and information warfare.Computer Viruses and Malware is designed for a professional audience composed of researchers and practitioners in industry. This book is also suitable as a secondary text for advanced-level students in computer science.Written for:Computer and information security practitioners"/>
<script type="text/javascript">
//<![CDATA[
try{if (!window.CloudFlare) {var CloudFlare=[{verbose:0,p:0,byc:0,owlid:"cf",bag2:1,mirage2:0,oracle:0,paths:{cloudflare:"/cdn-cgi/nexp/dok3v=1613a3a185/"},atok:"047a5bcbf67431883fc9ed25fba33612",petok:"a9c07d8f749a8cf2c21d23c516292de84357a3de-1498755886-1800",zone:"vxheaven.org",rocket:"a",apps:{}}];document.write('<script type="text/javascript" src="//ajax.cloudflare.com/cdn-cgi/nexp/dok3v=85b614c0f6/cloudflare.min.js"><'+'\/script>');}}catch(e){};
//]]>
</script>
<link rel="icon" href="/favicon.ico" type="image/x-icon"/>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"/>
<link rel="stylesheet" type="text/css" href="/style.css"/><link rel="canonical" href="http://vxheaven.org/lib/mja01.html"/>
<script type="text/rocketscript" data-rocketsrc="https://apis.google.com/js/plusone.js">{"parsetags": "explicit"}</script>
</head>
<body bgcolor="#dbc8a0" text="#302000" link="#225599" vlink="#113366">
<div class="s1">
<div style="float:right;"><a href="/lib/index.php?tbs=1"><img src="/img/max.gif" alt="Maximize"/></a></div> <form id="lf" style="margin: 0; float: right;" method="get" action="/index.php"><input type="hidden" name="action" value="set"/><select name="lang" onchange="javascript:document.getElementById('lf').submit();"><option value="ru">Русский</option><option selected="selected" value="en">English</option><option value="ua">Українська</option><option value="de">Deutsch</option><option value="es">Español</option><option value="fr">Fran&ccedil;ais</option><option value="it">Italiano</option><option value="pl">Polski</option></select></form>
<div style="float: right;"><div id="plusone"></div></div>
<script type="text/rocketscript">gapi.plusone.render("plusone", {"size":"small","count":"true"});</script>
<div style="float: right;" class="addthis_toolbox addthis_default_style">
<script type="text/rocketscript">var addthis_config = { ui_click: true }</script>
<a style="text-decoration: none; font-size: 10pt;" href="/?action=addthis" class="addthis_button_compact">Bookmark</a>
<script type="text/rocketscript" data-rocketsrc="http://s7.addthis.com/js/250/addthis_widget.js#username=herm1t"></script>
</div>
<div style="float: right;">
<script type="text/rocketscript" data-rocketsrc="http://www.google.com/cse/brand?form=cse-search-box&amp;lang=en"></script>
<form action="/search.php" id="cse-search-box">
<input type="hidden" name="cx" value="002577580816726040001:z9_irkorydo"/>
<input type="hidden" name="cof" value="FORID:10"/>
<input type="hidden" name="ie" value="UTF-8"/>
<input type="text" name="q" size="12" value=" "/>
<input type="submit" name="sa" value="Search"/>
</form>
</div><h1><a href="/" style="text-decoration: none; color: #000000;">VX Heaven</a></h1>
<span class="nav"><a href="/lib/">Library</a> <a href="/vl.php">Collection</a> <a href="/src.php">Sources</a> <a href="/vx.php?id=eidx">Engines</a> <a href="/vx.php?id=tidx">Constructors</a> <a href="/vx.php?id=sidx">Simulators</a> <a href="/vx.php?id=uidx">Utilities</a> <a href="/links.php">Links</a> <a href="/donate.php" style="color: #706020" id="donate">Donate</a> <a href="/forum" style="text-decoration: underline;">Forum</a> </span><br clear="all"/>
</div>
<div class="s2"><h1>Computer Viruses and Malware</h1><p><a href="/lib/?lang=en&amp;author=Aycock%2C%20John">John Aycock</a><br/> <em>Advances in Information Security, Vol. 22</em><br/> <em>ISBN 978-0-387-30236-2</em><br/> <em> 2006</em></p><script type="text/rocketscript">var disqus_url = 'http://vxheaven.org/lib/mja01.html';</script><div class="ci"><a href="/lib/?ci=mja01">1</a></div><img src="/img/pdf.gif" alt="PDF"/><a href="/lib/pdf/Computer%20Viruses%20and%20Malware.pdf">Download</a> PDF (10.8Mb) (You need to be registered on <a href="/forum">forum</a>)<br/>[<a style="" href="/lib/?lang=EN&amp;index=AV#mja01">Back to index</a>] [<a href="/lib/mja01.html#disqus_thread">Comments</a>]<br/> <form method="post" action="">
<img src="/img/cache/0b9fd596a90421f9f1f68a9760275737.gif" alt="\text{T_EX size}" valign="middle"/>
<select name="TeX_size"><option value="-2">-2</option><option value="-1">-1</option><option value="0" selected="selected">0</option><option value="1">1</option><option value="2">2</option><option value="3">3</option><option value="4">4</option><option value="5">5</option> </select>
<input type="submit" value="Scale"/>
</form><form method="post" action=""><input type="hidden" name="pages" value="1"/><input type="submit" value="Turn on page numbers"/></form>
<img src="img/mja01/cover.jpg" alt="Computer Viruses and Malware (book cover)"/>
 
 
<div align="right">
<em>To all the two-legged critters<br/>
in my house</em>
</div>
 
<h2>Contents</h2>
<ul>
<li>Dedication</li>
<li>List of figures</li>
<li>Preface</li>
<li><a href="#c1">Chapter 1 We've got problems</a>
<ul>
<li><a href="#c11">1.1 Dramatis Personae</a></li>
<li><a href="#c12">1.2 The Myth of Absolute Security</a></li>
<li><a href="#c13">1.3 The cost of malware</a></li>
<li><a href="#c14">1.4 The number of threats</a></li>
<li><a href="#c15">1.5 Speed of propagation</a></li>
<li><a href="#c16">1.6 People</a></li>
<li><a href="#c17">1.7 About this book</a></li>
<li><a href="#c18">1.8 Some words of warning</a></li>
</ul></li>
<li><a href="#c2">Chapter 2 Definitions and timeline</a>
<ul>
<li><a href="#c21">2.1 Malware types</a>
<ul>
<li><a href="#c211">2.1.1 Logic bomb</a></li>
<li><a href="#c212">2.1.2 Trojan horse</a></li>
<li><a href="#c213">2.1.3 Back door</a></li>
<li><a href="#c214">2.1.4 Virus</a></li>
<li><a href="#c215">2.1.5 Worm</a></li>
<li><a href="#c216">2.1.6 Rabbit</a></li>
<li><a href="#c217">2.1.7 Spyware</a></li>
<li><a href="#c218">2.1.8 Adware</a></li>
<li><a href="#c219">2.1.9 Hybrids, Droppers, and Blended Threats</a></li>
<li><a href="#c21a">2.1.10 Zombies</a></li>
</ul></li>
<li><a href="#c22">2.2 Naming</a></li>
 
<li><a href="#c23">2.3 Authorship</a></li>
<li><a href="#c24">2.4 Timeline</a></li>
</ul></li>
<li><a href="#c3">Chapter 3 Viruses</a>
<ul>
<li><a href="#c31">3.1 Classification by Target</a>
<ul>
<li><a href="#c311">3.1.1 Boot-Sector Infectors</a></li>
<li><a href="#c312">3.1.2 File Infectors</a>
<ul>
<li><a href="#c3121">3.1.2.1 Beginning of File</a></li>
<li><a href="#c3122">3.1.2.2 End of File</a></li>
<li><a href="#c3123">3.1.2.3 Overwritten into File</a></li>
<li><a href="#c3124">3.1.2.4 Inserted into File</a></li>
<li><a href="#c3125">3.1.2.5 Not in File</a></li>
</ul></li>
<li><a href="#c313">3.1.3 Macro Viruses</a></li>
</ul></li>
<li><a href="#c32">3.2 Classification by Concealment Strategy</a>
<ul>
<li><a href="#c321">3.2.1 No Concealment</a></li>
<li><a href="#c322">3.2.2 Encryption</a></li>
<li><a href="#c323">3.2.3 Stealth</a></li>
<li><a href="#c324">3.2.4 Oligomorphism</a></li>
<li><a href="#c325">3.2.5 Polymorphism</a>
<ul>
<li><a href="#c3251">3.2.5.1 Self-Detection</a></li>
<li><a href="#c3252">3.2.5.2 Changing the Decryptor Loop</a></li>
</ul></li>
<li><a href="#c326">3.2.6 Metamorphism</a></li>
<li><a href="#c327">3.2.7 Strong Encryption</a></li>
</ul></li>
<li><a href="#c33">3.3 Virus Kits</a></li>
</ul></li>
<li><a href="#c4">Chapter 4 Anti-virus techniques</a>
<ul>
<li><a href="#c41">4.1 Detection: Static Methods</a>
<ul>
<li><a href="#c411">4.1.1 Scanners</a>
<ul>
<li><a href="#c4111">4.1.1.1 Algorithm: Aho-Corasick</a></li>
<li><a href="#c4112">4.1.1.2 Algorithm: Veldman</a></li>
<li><a href="#c4113">4.1.1.3 Algorithm: Wu-Manber</a></li>
<li><a href="#c4114">4.1.1.4 Testing</a></li>
<li><a href="#c4115">4.1.1.5 Improving Performance</a></li>
</ul></li>
<li><a href="#c412">4.1.2 Static Heuristics</a></li>
<li><a href="#c413">4.1.3 Integrity Checkers</a></li>
</ul></li>
<li><a href="#c42">4.2 Detection: Dynamic Methods</a>
<ul>
<li><a href="#c421">4.2.1 Behavior Monitors/Blockers</a></li>
<li><a href="#c422">4.2.2 Emulation</a>
<ul>
<li><a href="#c4221">4.2.2.1 Emulator Anatomy</a></li>
<li><a href="#c4222">4.2.2.2 If at First You Don't Succeed</a></li>
<li><a href="#c4223">4.2.2.3 Emulator Optimizations</a></li>
</ul></li>
</ul></li>
<li><a href="#c43">4.3 Comparison of Anti-Virus Detection Techniques</a></li>
<li><a href="#c44">4.4 Verification, Quarantine, and Disinfection</a>
<ul>
<li><a href="#c441">4.4.1 Verification</a></li>
<li><a href="#c442">4.4.2 Quarantine</a></li>
<li><a href="#c443">4.4.3 Disinfection</a></li>
</ul></li>
<li><a href="#c45">4.5 Virus Databases and Virus Description Languages</a></li>
<li><a href="#c46">4.6 Short Subjects</a>
<ul>
<li><a href="#c461">4.6.1 Anti-Stealth Techniques</a></li>
<li><a href="#c462">4.6.2 Macro Virus Detection</a></li>
<li><a href="#c463">4.6.3 Compiler Optimization</a></li>
</ul></li>
</ul></li>
 
<li><a href="#c5">Chapter 5 Anti-anti-virus techniques</a>
<ul>
<li><a href="#c51">5.1 Retroviruses</a></li>
<li><a href="#c52">5.2 Entry Point Obfuscation</a></li>
<li><a href="#c53">5.3 Anti-Emulation</a>
<ul>
<li><a href="#c531">5.3.1 Outlast</a></li>
<li><a href="#c532">5.3.2 Outsmart</a></li>
<li><a href="#c533">5.3.3 Overextend</a></li>
</ul></li>
<li><a href="#c54">5.4 Armoring</a>
<ul>
<li><a href="#c541">5.4.1 Anti-Debugging</a></li>
<li><a href="#c542">5.4.2 Anti-Disassembly</a></li>
</ul></li>
<li><a href="#c55">5.5 Tunneling</a></li>
<li><a href="#c56">5.6 Integrity Checker Attacks</a></li>
<li><a href="#c57">5.7 Avoidance</a></li>
</ul></li>
<li><a href="#c6">Chapter 6 Weakness exploited</a>
<ul>
<li><a href="#c61">6.1 Technical Weaknesses</a>
<ul>
<li><a href="#c611">6.1.1 Background</a></li>
<li><a href="#c612">6.1.2 Buffer Overflows</a>
<ul>
<li><a href="#c6121">6.1.2.1 Stack Smashing</a></li>
<li><a href="#c6122">6.1.2.2 Frame Pointer Overwriting</a></li>
<li><a href="#c6123">6.1.2.3 Returns into Libraries</a></li>
<li><a href="#c6124">6.1.2.4 Heap Overflows</a></li>
<li><a href="#c6125">6.1.2.5 Memory Allocator Attacks</a></li>
</ul></li>
<li><a href="#c613">6.1.3 Integer Overflows</a></li>
<li><a href="#c614">6.1.4 Format String Vulnerabilities</a></li>
<li><a href="#c615">6.1.5 Defenses</a>
<ul>
<li><a href="#c6151">6.1.5.1 Vulnerability-Specific Defenses</a></li>
<li><a href="#c6152">6.1.5.2 General Defenses</a></li>
</ul></li>
<li><a href="#c616">6.1.6 Finding Weaknesses</a></li>
</ul></li>
<li><a href="#c62">6.2 Human Weaknesses</a>
<ul>
<li><a href="#c621">6.2.1 Virus Hoaxes</a></li>
</ul></li>
</ul></li>
<li><a href="#c7">Chapter 7 Worms</a>
<ul>
<li><a href="#c71">7.1 Worm History</a>
<ul>
<li><a href="#c711">7.1.1 Xerox PARC, c. 1982</a></li>
<li><a href="#c712">7.1.2 The Internet Worm, November 1988</a></li>
</ul></li>
<li><a href="#c72">7.2 Propagation</a>
<ul>
<li><a href="#c721">7.2.1 Initial Seeding</a></li>
<li><a href="#c722">7.2.2 Finding Targets</a></li>
</ul></li>
</ul></li>
<li><a href="#c8">Chapter 8 Deworming</a>
<ul>
<li><a href="#c81">8.1 Defense</a>
<ul>
<li><a href="#c811">8.1.1 User</a></li>
<li><a href="#c812">8.1.2 Host</a>
<ul>
<li><a href="#c8121">8.1.2.1 Patching</a></li>
<li><a href="#c8122">8.1.2.2 Limiting Available Services</a></li>
<li><a href="#c8123">8.1.2.3 Countermeasures against Specific Weaknesses</a></li>
<li><a href="#c8124">8.1.2.4 Anti-Virus Software</a></li>
<li><a href="#c8125">8.1.2.5 Memory Scanning</a></li>
</ul></li>
<li><a href="#c813">8.1.3 Perimeter</a>
<ul>
<li><a href="#c8131">8.1.3.1 Firewalls</a></li>
<li><a href="#c8132">8.1.3.2 Intrusion Detection Systems</a></li>
</ul></li>
</ul></li>
<li><a href="#c82">8.2 Capture and Containment</a>
 
<ul>
<li><a href="#c821">8.2.1 Honeypots</a></li>
<li><a href="#c822">8.2.2 Reverse Firewalls</a></li>
<li><a href="#c823">8.2.3 Throttling</a></li>
</ul></li>
<li><a href="#c83">8.3 Automatic Countermeasures</a></li>
</ul></li>
<li><a href="#c9">Chapter 9 Applications</a>
<ul>
<li><a href="#c91">9.1 Benevolent Malware</a></li>
<li><a href="#c92">9.2 Spam</a></li>
<li><a href="#c93">9.3 Access-for-Sale Worms</a></li>
<li><a href="#c94">9.4 Cryptovirology</a></li>
<li><a href="#c95">9.5 Information Warfare</a></li>
<li><a href="#c96">9.6 Cyberterrorism</a></li>
</ul></li>
<li><a href="#ca">Chapter 10 People and communities</a>
<ul>
<li><a href="#ca1">10.1 Malware Authors</a>
<ul>
<li><a href="#ca11">10.1.1 Who?</a></li>
<li><a href="#ca12">10.1.2 Why?</a></li>
</ul></li>
<li><a href="#ca2">10.2 The Anti-Virus Community</a>
<ul>
<li><a href="#ca21">10.2.1 Perceptions</a></li>
<li><a href="#ca22">10.2.2 Another Day in Paradise</a></li>
<li><a href="#ca23">10.2.3 Customer Demands</a></li>
<li><a href="#ca24">10.2.4 Engineering</a></li>
<li><a href="#ca25">10.2.5 Open Questions</a></li>
</ul></li>
</ul></li>
<li><a href="#cb">Chapter 11 What should we do?</a></li>
<li>References</li>
<li>Index</li>
</ul>
 
 
<h2>Preface</h2>
<p>It seemed like a good idea at the time. In 2003,1 started teaching a course on computer viruses and malicious software to senior undergraduate and graduate students at the University of Calgary. It's been an interesting few years. Computer viruses are a controversial and taboo topic, despite having such a huge impact on our society; needless to say, there was some backlash about this course from outside the University.</p>
<p>One of my initial practical concerns was whether or not I could find enough detailed material to teach a 13-week course at this level. There were some books on the topic, but (with all due respect to the authors of those books) there were none that were suitable for use as a textbook.</p>
<p>I was more surprised to find out that there was a lot of information about viruses and doing "bad" things, but there was very little information about anti-virus software. A few quality minutes with your favorite web search engine will yield virus writing tutorials, virus source code, and virus creation toolkits. In contrast, although it's comprised of some extremely nice people, the anti-virus community tends to be very industry-driven and insular, and isn't in the habit of giving out its secrets. Unless you know where to look.</p>
<p>Several years, a shelf full of books, and a foot-high stack of printouts later, I've ferreted out a lot of detailed material which I've assembled in this book. It's a strange type of research for a computer scientist, and I'm sure that my academic colleagues would cringe at some of the sources that I've had to use. Virus writers don't tend to publish in peer-reviewed academic journals, and anti-virus companies don't want to tip their hand. I would tend to characterize this detective work more like historical research than standard computer science research: your sources are limited, so you try and authenticate them; you piece a sentence in one document together with a sentence in another document, and you're able to make a useful connection. It's painstaking and often frustrating.</p>
<p>Technical information goes out of date very quickly, and in writing this book I've tried to focus on the concepts more than details. My hope is that the
 
concepts will still be useful years from now, long after the minute details of operating systems and programming languages have changed. Having said that, I've included detail where it's absolutely necessary to explain what's going on, and used specific examples of viruses and malicious software where it's useful to establish precedents for certain techniques. Depending on why you're reading this, a book with more concrete details might be a good complement to this material.</p>
<p>Similarly, if you're using this as a textbook, I would suggest supplementing it with details of the latest and greatest malicious software that's making the rounds. Unfortunately there will be plenty of examples to choose from. In my virus course, I also have a large segment devoted to the law and ethics surrounding malicious software, which I haven't incorporated here - law is constantly changing and being reinterpreted, and there are already many excellent sources on ethics. Law and ethics are very important topics for any computer professional, but they are especially critical for creating a secure environment in which to work with malicious software.</p>
<p>I should point out that I've only used information from public sources to write this book. I've deliberately excluded any information that's been told to me in private conversations, and I'm not revealing anyone's trade secrets that they haven't already given away themselves.</p>
<p>I'd like to thank the students I've taught in my virus course, who pushed me with their excellent questions, and showed much patience as I was organizing all this material into some semi-coherent form. Thanks too to those in the anti-virus community who kept an open mind. I'd also like to thank the people who read drafts of this book: Jorg Denzinger, Richard Ford, Sarah Gordon, Shannon Jaeger, Cliff Marcellus, Jim Uhl, James Wolfe, and Mike Zastre. Their suggestions and comments helped improve the book as well as encourage me. Finally, Alan Aycock suggested some references for Chapter 10, Stefania Bertazzon answered my questions about rational economics, Moustafa Hammad provided an Arabic translation, and Maryam Mehri Dehnavi translated some Persian text for me. Of course, any errors that remain are my own.</p>
<div align="right">John Aycock</div>
 
<h2><a name="c1"></a>Chapter 1 We've got problems</h2>
<p>In ancient times, people's needs were simple: food, water, shelter, and the occasional chance to propagate the species. Our basic needs haven't changed, but the way we fulfill them has. Food is bought in stores which are fed by supply chains with computerized inventory systems; water is dispensed through computer-controlled water systems; parts for new shelters come from suppliers with computer-ridden supply chains, and old shelters are bought and sold by computer-wielding realtors. The production and transmission of energy to run all of these systems is controlled by computer, and computers manage financial transactions to pay for it all.</p>
<p>It's no secret that our society's infrastructure relies on computers now. Unfortunately, this means that a threat to computers is a threat to society. But how do we protect our critical infrastructure? What are the problems it faces?</p>
<h3><a name="c11"></a>1.1 Dramatis Personae</h3>
<p>There are four key threats to consider. These are the four horsemen of the electronic apocalypse: spam, bugs, denials of service, and malicious software.</p>
<dl>
<dt><strong>Spam</strong></dt><dd>The term commonly used to describe the abundance of unsolicited bulk email which plagues the mailboxes of Internet users worldwide. The statistics vary over time, but suggest that over 70% of email traffic currently falls into this category.<sup><a href="#f1_1" name="b1_1">1)</a></sup></dd>
<dt><strong>Bugs</strong></dt><dd>These are software errors which, when they crop up, can kill off your software immediately, if you're lucky. They can also result in data corruption, security weaknesses, and spurious, hard-to-find problems.</dd>
<dt><strong>Denials of service</strong></dt><dd>Denial-of-service attacks, or DoS attacks,<sup><a href="#f1_2" name="b1_2">2</a></sup> starve legitimate usage of resources or services. For example, a DoS attack could use
 
up all available disk space on a system, so that other users couldn't make use of it; generating reams of network traffic so that real traffic can't get through would also be a denial of service. Simple DoS attacks are relatively easy to mount by simply overwhelming a machine with requests, as a toddler might overwhelm their parents with questions. Sophisticated DoS attacks can involve more finesse, and may trick a machine into shutting a service down instead of flooding it.</dd>
<dt><strong>Malicious software</strong></dt><dd>The real war is waged with malicious software, or malware. This is software whose intent is malicious, or whose effect is malicious. The spectrum of malware covers a wide variety of specific threats, including viruses, worms, Trojan horses, and spyware.</dd>
</dl>
<p>The focus of this book is malware, and the techniques which can be used to detect, detain, and destroy it. This is not accidental. Of the four threats listed above, malware has the deepest connection to the other three. Malware may be propagated using spam, and may also be used to send spam; malware may take advantage of bugs; malware may be used to mount DoS attacks. Addressing the problem of malware is vital for improving computer security. Computer security is vital to our society's critical infrastructure.</p>
<h3><a name="c12"></a>1.2 The Myth of Absolute Security</h3>
<p>Obviously we want our computers to be secure against threats. Unfortunately, there is no such thing as absolute security, where a computer is either secure or it's not. You may take a great deal of technical precautions to safeguard your computers, but your protection is unlikely to be effective against a determined attacker with sufficient resources. A government-funded spy agency could likely penetrate your security, should they be motivated to do so. Someone could drive a truck through the wall of your building and steal your computers. Old-fashioned ways are effective, too: there are many ways of coercing people into divulging information.<sup><a href="#f1_3" name="b1_3">3</a></sup></p>
<p>Even though there is no absolute computer security, relative computer security can be considered based on six factors:</p>
<ul>
<li>What is the importance of the information or resource being protected?</li>
<li>What is the potential impact, if the security is breached?</li>
<li>Who is the attacker likely to be?</li>
<li>What are the skills and resources available to an attacker?</li>
<li>What constraints are imposed by legitimate usage?</li>
<li>What resources are available to implement security?</li>
</ul>
 
<p>Breaking down security in this way changes the problem. Security is no longer a binary matter of secure or not-secure; it becomes a problem of risk management,"<sup><a href="#f1_4" name="b1_4">4</a></sup> and implementing security can be seen as making tradeoffs between the level of protection, the usability of the resulting system, and the cost of implementation.</p>
<p>When you assess risks for risk management, you must consider the risks posed to you by others, <em>and</em> consider the risks posed to others by you. Everybody is your neighbor on the Internet, and it isn't farfetched to think that you could be found negligent if you had insufficient computer security, and your computers were used to attack another site.<sup><a href="#f1_100" name="b1_100">100</a></sup></p>
<h3><a name="c13"></a>1.3 The cost of malware</h3>
<p>Malware unquestionably has a negative financial impact, but how big an impact does it really have?<sup><a href="#f1_101" name="b1_101">101</a></sup> It's important to know, because if computer security is to be treated as risk management, then you have to accurately assess how much damage a lapse in security could cause.</p>
<p>At first glance, gauging the cost of malware incidents would seem to be easy. After all, there are any number of figures reported on this, figures attributed to experts. They can vary from one another by an order of magnitude, so if you disagree with one number, you can locate another more to your liking. I use the gross domestic product of Austria, myself - it's a fairly large number, and it's as accurate an estimate as any other.</p>
<p>In all fairness, estimating malware cost is a very hard problem. There are two types of costs to consider: real costs and hidden costs.</p>
<dl>
<dt><strong>Real costs</strong></dt><dd>These are costs which are apparent, and which are relatively easy to calculate. If a computer virus reduced your computer to a bubbling puddle of molten slag,<sup><a href="#f1_5" name="b1_5">5</a></sup> the cost to replace it would be straightforward to assess. Similarly, if an employee can't work because their computer is having malware removed from it, then the employee's lost productivity can be computed. The time that technical support staff spend tracking down and fixing affected computers can also be computed. Not all costs are so obvious, however.</dd>
<dt><strong>Hidden costs</strong></dt><dd>Hidden costs are costs whose impact can't be measured accurately, and may not even be known. Some businesses, like banks and computer security companies, could suffer damage to their reputation from a publicized malware incident. Regardless of the business, a leak of proprietary information or customer data caused by malware could result in enormous damage to a company, no different than industrial espionage. Any downtime could drive existing customers to a competitor, or turn away new, potential customers.</dd>
</dl>
 
<p>This has been cast in terms of business, but malware presents a cost to individuals, too. Personal information stolen by malware from a computer, such as passwords, credit card numbers, and banking information, can give thieves enough for that tropical vacation they've always dreamed of, or provide a good foundation for identity theft.</p>
<h3><a name="c14"></a>1.4 The number of threats</h3>
<p>Even the exact number of threats is open to debate. A quick survey of competing anti-virus products shows that the number of threats they claim to detect can vary by as much as a factor of two. Curiously, the level of protection each affords is about the same, meaning that more is not necessarily better.</p>
<p>Why? There is no industry-wide agreement on what constitutes a "threat," to begin with. It's not surprising, given that fact alone, that different anti-virus products would have different numbers - they aren't all counting the same thing. For example, there is some dispute as to whether or not automatically-generated viruses produced by the same tool should be treated as individual threats, or as only one threat. This came to the fore in 1998, when approximately 15,000 new automatically-generated viruses appeared overnight.<sup><a href="#f1_102" name="b1_102">102</a></sup> It is also difficult to amass and correctly maintain a malware collection, <sup><a href="#f1_103" name="b1_103">103</a></sup> and inadvertent duplication or misclassification of malware samples is always a possibility. There is no single clearinghouse for malware.</p>
<p>Another consideration is that the reported numbers are only for threats that are known about. Ideally, computers should be protected from both known <em>and</em> unknown threats. It's impossible to know about unknown threats, of course, which means that it's impossible to precisely assess how well-protected your computers are against threats.</p>
<p>Different anti-virus products may employ different detection techniques, too. Not all methods of detection rely on exhaustive compilations of known threats, and generic detection techniques routinelyfindboth known and unknown threats without knowing the exact nature of what they're detecting.</p>
<p>Even for known threats, not all may endanger your computers. The majority of malware is targeted to some specific combination of computer architecture and operating system, and sometimes even to a particular application. Effectively these act as preconditions for a piece of malware to run; if any of these conditions aren't true - for instance, you use a different operating system - then that malware poses no direct threat to you. It is inert with respect to your computers.</p>
<p>Even if it can't run, malware may carry an indirect liability risk if it passes through your computers from one target to another. For example, one unaffected computer could provide a shared directory; someone else's compromised computer could deposit malware in that shared directory for later propagation. It is prudent to look for threats to all computers, not just to your own.</p>
 
<div align="center">
<img src="img/mja01/fig11.gif" alt="Figure 1.1. Worm propagation curve"/>
<p><strong>Figure 1.1. Worm propagation curve</strong></p>
</div>
<h3><a name="c15"></a>1.5 Speed of propagation</h3>
<p>Once upon a time, the speed of malware propagation was measured in terms of weeks or even months. This is no longer the case.</p>
<p>A typical worm propagation curve is shown in Figure 1.1. (For simplicity, the effects on the curve from defensive measures aren't shown.) At first, the worm spreads slowly to vulnerable machines, but eventually begins a period of exponential growth when it spreads extremely rapidly. Finally, once the majority of vulnerable machines have been compromised, the worm reaches a saturation point; any further growth beyond this point is minimal.</p>
<p>For a worm to spread more quickly, the propagation curve needs to be moved to the left. In other words, the worm author wants the period of exponential growth to occur earlier, preferably before any defenses have been deployed. This is shown in Figure 1.2a.</p>
<div align="center">
<img src="img/mja01/fig12.gif" alt="Figure 1.2. Ideal propagation curves for attackers and defenders"/>
<p><strong>Figure 1.2. Ideal propagation curves for attackers and defenders</strong></p>
</div>
 
<p>On the other hand, a defender wants to do one of two things. First, the propagation curve could be pushed to the right, buying time to construct a defense before the worm's exponential growth period. Second, the curve could be compressed downwards, meaning that not all vulnerable machines become compromised by the worm. These scenarios are shown in Figure 1.2b.</p>
<p>The time axis on these figures has been deliberately left unlabeled, because the exact propagation rate will depend on the techniques that a particular worm uses. However, the theoretical maximum speed of a carefully-designed worm from initial release until saturation is startling: 510 milliseconds to 1.3 seconds.<sup><a href="#f1_6" name="b1_6">6</a></sup> In less than two seconds, it's over. No defense that relies on any form of human intervention will be fast enough to cope with threats like this.</p>
<h3><a name="c16"></a>1.6 People</h3>
<p>Humans are the weak link on several other fronts too, all of which are taken advantage of by malware.</p>
<p>By their nature, humans are trusting, social creatures. These are excellent qualities for your friends to have, and also for your victims to possess: an entire class of attacks, called social engineering attacks, are quick to exploit these desirable human qualities.</p>
<p>Social engineering aside, many people simply aren't aware of the security consequences of their actions. For example, several informal surveys of people on the street have found them more than willing to provide enough information for identity theft (even offering up their passwords) in exchange for chocolate, theater tickets, and coffee vouchers.<sup><a href="#f1_104" name="b1_104">104</a></sup></p>
<p>Another problem is that humans - users - don't demand enough of software vendors in terms of secure software. Even for security-savvy users who want secure software, the security of any given piece of software is nearly impossible to assess.</p>
<p>Secure software is software which can't be exploited by an attacker. Just because some software hasn't been compromised is no indication that it's secure - like the stock market, past performance is no guarantee of future results. Unfortunately, that's really the only guideline users have to judge security: the absence of an attack. Software security is thus an anti-feature for vendors, because it's intangible. It's no wonder that vendors opt to add features rather than improve security. Features are easier to sell.</p>
<p>Features are also easier to buy. Humans are naturally wooed by new features, which forms a vicious cycle that gives software vendors little incentive to improve software security.</p>
 
<h3><a name="c17"></a>1.7 About this book</h3>
<p>Malware poses an enormous problem in the context of faulty humans and faulty software security. It could be that malware is the natural consequence of the presence of these faults, like vermin slipping through building cracks in the real world. Indeed, names like "computer virus" and "computer worm" bring to mind their biological real-world counterparts.</p>
<p>Whatever the root cause, malware is a problem that needs to be solved. This book looks at malware, primarily viruses and worms, and its countermeasures. The next chapter lays the groundwork with some basic definitions and a timeline of malware. Then, on to viruses: Chapters 3, 4, and 5 cover viruses, anti-virus techniques, and anti-anti-virus techniques, in that order. Chapter 6 explains the weaknesses that are exploited by malware, both technical and social - this is necessary background for the worms in Chapter 7. Defenses against worms are considered in Chapter 8. Some of the possible manifestations of malware are looked at in Chapter 9, followed by a look at the people who create malware and defend against it in Chapter 10. Some final thoughts on defense are in Chapter 11.</p>
<p>The convention used for chapter endnotes is somewhat unusual. The notes tend to fall into two categories. First, there are notes with additional content related to the text. These have endnote numbers from 1-99 within a chapter. Second, there are endnotes that provide citations and pointers to related material. This kind of endnote is numbered 100 or above. The intent is to make the two categories of endnote easily distinguishable in the text.</p>
<p>A lot of statements in this book are qualified with "can" and "could" and "may" and "might." Software is infinitely malleable and can be made to do almost anything; it is hubris to make bold statements about what malware can and can't do.</p>
<p>Finally, this is not a programming book, and some knowledge of programming (in both high- and low-level languages) is assumed, although pseudocode is used where possible. A reasonable understanding of operating systems and networks is also beneficial.</p>
<h3><a name="c18"></a>1.8 Some words of warning</h3>
<p>Self-replicating software like viruses and worms has proven itself to be very difficult to control, even from the very earliest experiments.<sup><a href="#f1_7" name="b1_7">7</a></sup> While self-replicating code may not intentionally be malicious, it can have similar effects regardless. Of course, the risks of overtly malicious software should be obvious. Any experiments with malware, or analysis of malware, should be done in a secure environment designed specifically for that purpose. While it's outside the scope of this book to describe such a secure environment - the details would
 
be quickly out of date anyway - there are a number of sources of information available.<sup><a href="#f1_105" name="b1_105">105</a></sup></p>
<p>Another thing to consider is that creation and/or distribution of malware may violate local laws. Many countries have computer crime legislation now,<sup><a href="#f1_8" name="b1_8">8</a></sup> and even if the law was violated in a different jurisdiction from where the perpetrator is physically located, extradition agreements may apply.<sup><a href="#f1_106" name="b1_106">106</a></sup> Civil remedies for victims of malware are possible as well.</p>
<p>Ironically, some dangers lurk in defensive techniques too. Some of the material in this book is derived from patent documents; the intent is to provide a wide range of information, and is not in any way meant to suggest that these patents should be infringed. While every effort has been made to cite relevant patents, it is possible that some have been inadvertently overlooked. Furthermore, patents may be interpreted very broadly, and the applicability of a patent may depend greatly on the skill and financial resources of the patent holder's legal team. Seek legal advice before rushing off to implement any of the techniques described in this book.</p>
 
<p><strong>Notes for Chapter 1</strong></p>
<p><a name="f1_1" href="#b1_1">1</a> Based on MessageLabs' sample size of 12.6 billion email messages [203]. This has a higher statistical significance than 99% of statistics you would normally find.</p>
<p><a name="f1_2" href="#b1_2">2</a> Note the capitalization - "DOS" is an operating system, "DoS" is an attack.</p>
<p><a name="f1_3" href="#b1_3">3</a> In cryptography, this has been referred to as "rubber-hose" cryptanalysis [279].</p>
<p><a name="f1_4" href="#b1_4">4</a> Schneier has argued this point of view, and that computer security is an untapped market for insurance companies, who are in the business of managing risk anyway [280].</p>
<p><a name="f1_5" href="#b1_5">5</a> Before any urban legends are started, computer viruses can't do this.</p>
<p><a name="f1_6" href="#b1_6">6</a> These numbers (510 ms for UDP-based worms, 1.3 s for TCP-based worms) are the time it takes to achieve 95% saturation of a million vulnerable machines [303].</p>
<p><a name="f1_7" href="#b1_7">7</a> For example, Cohen's first viruses progressed surprisingly quickly [74], as did Duff's shell script virus [95], and an early worm at Xerox ran amok [287].</p>
<p><a name="f1_8" href="#b1_8">8</a> Computer crime laws are not strictly necessary for prosecuting computer crimes that arejust electronic versions of "traditional" crimes like fraud [56], but the trend is definitely to enact computer-specific laws.</p>
<p><a name="f1_100" href="#b1_100">100</a> Owens [237] discusses liability potential in great detail.</p>
<p><a name="f1_101" href="#b1_101">101</a> This section is based on Garfink and Landesman [117], and Ducklin [94] touches on some of the same issues too.</p>
<p><a name="f1_102" href="#b1_102">102</a> Morley [213]. Ducklin [94] has a discussion of this issue, and of other ways to measure the extent of the virus problem.</p>
<p><a name="f1_103" href="#b1_103">103</a> Bontchev [39] talks about the care and feeding of a "clean" virus library.</p>
<p><a name="f1_104" href="#b1_104">104</a> The informal surveys were reported in [30] (chocolate), [31, 274] (theater tickets), and [184] (coffee vouchers). Less amusing, but more rigorous, surveys have been done which show similar problems [270, 305].</p>
<p><a name="f1_105" href="#b1_105">105</a> There are a wide range of opinions on working with malware, ranging from the inadequate to the paranoid. As a starting point, see [21, 75, 187, 282, 288,312].</p>
<p><a name="f1_106" href="#b1_106">106</a> Although U.S.-centric. Soma et al. [295] give a good overview of the general features of extradition treaties.</p>
 
<h2><a name="c2"></a>Chapter 2 Definitions and timeline</h2>
<p>It would be nice to present a clever taxonomy of malicious software, one that clearly shows how each type of malware relates to every other type. However, a taxonomy would give the quaint and totally incorrect impression that there is a scientific basis for the classification of malware.</p>
<p>In fact, there is no universally-accepted definition of terms like "virus" and "worm," much less an agreed-upon taxonomy, even though there have been occasional attempts to impose mathematical formalisms onto malware.<sup><a href="#f2_100" name="b2_100">100</a></sup> Instead of trying to pin down these terms precisely, the common characteristics each type of malware typically has are listed.</p>
<h3><a name="c21"></a>2.1 Malware types</h3>
<p>Malware can be roughly broken down into types according to the malware's method of operation. Anti-"virus" software, despite its name, is able to detect all of these types of malware.</p>
<p>There are three characteristics associated with these malware types.</p>
<ol>
<li><em>Self-replicating</em> malware actively attempts to propagate by creating new copies, or <em>instances</em>, of itself. Malware may also be propagated passively, by a user copying it accidentally, for example, but this isn't self-replication.</li>
<li>The <em>population growth</em> of malware describes the overall change in the number of malware instances due to self-replication. Malware that doesn't self-replicate will always have a zero population growth, but malware with a zero population growth may self-replicate.</li>
<li><em>Parasitic</em> malware requires some other executable code in order to exist. "Executable" in this context should be taken very broadly to include anything that can be executed, such as boot block code on a disk, binary code
 
in applications, and interpreted code. It also includes source code, like application scripting languages, and code that may require compilation before being executed.</li>
</ol>
<h4><a name="c211"></a>2.1.1 Logic bomb</h4>
<table summary="Characteristics of logic bomb">
<tr><td>Self-replicating:</td><td>no</td></tr>
<tr><td>Population growth:</td><td>zero</td></tr>
<tr><td>Parasitic:</td><td>possibly</td></tr>
</table>
<p>A <em>logic bomb</em> is code which consists of two parts:</p>
<ol>
<li>A <em>payload</em>, which is an action to perform. The payload can be anything, but has the connotation of having a malicious effect.</li>
<li>A <em>trigger</em>, a boolean condition that is evaluated and controls when the payload is executed. The exact trigger condition is limited only by the imagination, and could be based on local conditions like the date, the user logged in, or the operating system version. Triggers could also be designed to be set off remotely, or - like the "dead man's switch" on a train - be set off by the absence of an event.</li>
</ol>
<p>Logic bombs can be inserted into existing code, or could be standalone. A simple parasitic example is shown below, with a payload that crashes the computer using a particular date as a trigger.</p>
<pre>
	<em>legitimate code</em>
	if date is Friday the 13th:
		crash_computerO
	<em>legitimate code</em>
</pre>
<p>Logic bombs can be concise and unobtrusive, especially in millions of lines of source code, and the mere threat of a logic bomb could easily be used to extort money from a company. In one case, a disgruntled employee rigged a logic bomb on his employer's file server to trigger on a date after he was fired from his job, causing files to be deleted with no possibility of recovery. He was later sentenced to 41 months in prison.<sup><a href="#f2_101" name="b2_101">101</a></sup> Another case alleges that an employee installed a logic bomb on 1000 company computers, date-triggered to remove all the files on those machines; the person allegedly tried to profit from the downturn in the company's stock prices that occurred as a result of the damage.<sup><a href="#f2_1" name="b2_1">1</a></sup></p>
<h4><a name="c212"></a>2.1.2 Trojan horse</h4>
<table summary="Characteristics of trojan horse">
<tr><td>Self-replicating:</td><td>no</td></tr>
<tr><td>Population growth:</td><td>zero</td></tr>
<tr><td>Parasitic:</td><td>yes</td></tr>
</table>
 
<p>There was no love lost between the Greeks and the Trojans. The Greeks had besieged the Trojans, holed up in the city of Troy, for ten years. They finally took the city by using a clever ploy: the Greeks built an enormous wooden horse, concealing soldiers inside, and tricked the Trojans into bringing the horse into Troy. When night fell, the soldiers exited the horse and much unpleasantness ensued.<sup><a href="#f2_102" name="b2_102">102</a></sup></p>
<p>In computing, a <em>Trojan horse</em> is a program which purports to do some benign task, but secretly performs some additional malicious task. A classic example is a password-grabbing login program which prints authentic-looking "username" and "password" prompts, and waits for a user to type in the information. When this happens, the password grabber stashes the information away for its creator, then prints out an "invalid password" message before running the <em>real</em> login program. The unsuspecting user thinks they made a typing mistake and reenters the information, none the wiser.</p>
<p>Trojan horses have been known about since at least 1972, when they were mentioned in a well-known report by Anderson, who credited the idea to D. J. Edwards.<sup><a href="#f2_103" name="b2_103">103</a></sup></p>
<h4><a name="c213"></a>2.1.3 Back door</h4>
<table summary="Characteristics of back door">
<tr><td>Self-replicating:</td><td>no</td></tr>
<tr><td>Population growth:</td><td>zero</td></tr>
<tr><td>Parasitic:</td><td>possibly</td></tr>
</table>
<p>A <em>back door</em> is any mechanism which bypasses a normal security check. Programmers sometimes create back doors for legitimate reasons, such as skipping a time-consuming authentication process when debugging a network server.</p>
<p>As with logic bombs, back doors can be placed into legitimate code or be standalone programs. The example back door below, shown in gray [<em>italic</em> - herm1t], circumvents a login authentication process.</p>
<pre>
	username = read_username()
	password = read_password()
	<em>if username is "133t h4ck0r":
		return ALLOW_LOGIN</em>
	if username and password are valid:
		return ALLOW_LOGIN
	else:
		return DENY_LOGIN
</pre>
<p>One special kind of back door is a <em>RAT</em>, which stands for Remote Administration Tool or Remote Access Trojan, depending on who's asked. These programs allow a computer to be monitored and controlled remotely; users may deliberately install these to access a work computer from home, or to allow help desk
 
staff to diagnose and fix a computer problem from afar. However, if malware surreptitiously installs a RAT on a computer, then it opens up a back door into that machine.</p>
<h4><a name="c214"></a>2.1.4 Virus</h4>
<table summary="Characteristics of virus">
<tr><td>Self-replicating:</td><td>yes</td></tr>
<tr><td>Population growth:</td><td>positive</td></tr>
<tr><td>Parasitic:</td><td>yes</td></tr>
</table>
<p>A <em>virus</em> is malware that, when executed, tries to replicate itself into other executable code; when it succeeds, the code is said to be i<em>nfected</em><sup><a href="#f2_2" name="b2_2">2</a></sup>. The infected code, when run, can infect new code in turn. This self-replication into existing executable code is the key defining characteristic of a virus.</p>
<p>When faced with more than one virus to describe, a rather silly problem arises. There's no agreement on the plural form of "virus." The two leading contenders are "viruses" and "virii;" the latter form is often used by virus writers themselves, but it's rare to see this used in the security community, who prefer "viruses."<sup><a href="#f2_104" name="b2_104">104</a></sup></p>
<p>If viruses sound like something straight out of science fiction, there's a reason for that. They are. The early history of viruses is admittedly fairly murky, but the first mention of a computer virus is in science fiction in the early 1970s, with Gregory Benford's <em><a href="/lib/mgb00.html">The Scarred Man</a></em> in 1970, and David Gerrold's <em><a href="/lib/mdg00.html">When Harlie Was One</a></em> in 1972.<sup><a href="#f2_105" name="b2_105">105</a></sup> Both stories also mention a program which acts to counter the virus, so this is the first mention of anti-virus software as well.</p>
<p>The earliest real academic research on viruses was done by Fred Cohen in 1983, with the "virus" name coined by Len Adleman.<sup><a href="#f2_106" name="b2_106">106</a></sup> Cohen is sometimes called the "father of computer viruses," but it turns out that there were viruses written prior to his work. Rich Skrenta's Elk Cloner was circulating in 1982, and Joe Dellinger's viruses were developed between 1981-1983; all of these were for the Apple II platform.<sup><a href="#f2_107" name="b2_107">107</a></sup> Some sources mention a 1980 glitch in Arpanet as the first virus, but this was just a case of legitimate code acting badly; the only thing being propagated was data in network packets.<sup><a href="#f2_108" name="b2_108">108</a></sup> Gregory Benford's viruses were not limited to his sciencefictionstories; he wrote and released non-malicious viruses in 1969 at what is now the Lawrence Livermore National Laboratory, as well as in the early Arpanet.</p>
<p>Some computer games have featured self-replicating programs attacking one another in a controlled environment. Core War appeared in 1984, where programs written in a simple assembly language called Redcode fought one another; a combatant was assumed to be destroyed if its program counter pointed to an invalid Redcode instruction. Programs in Core War existed only in a virtual machine, but this was not the case for an earlier game, Darwin. Darwin was played in 1961, where a program could hunt and destroy another combatant
 
in a non-virtual environment using a well-defined interface.<sup><a href="#f2_109" name="b2_109">109</a></sup> In terms of strategy, successful combatants in these games were hard-to-find, innovative, and adaptive, qualities that can be used by computer viruses too.<sup><a href="#f2_3" name="b2_3">3</a></sup></p>
<p>Traditionally, viruses can propagate within a single computer, or may travel from one computer to another using human-transported media, like a floppy disk, CD-ROM, DVD-ROM, or USB flash drive. In other words, viruses don't propagate via computer networks; networks are the domain of worms instead. However, the label "virus" has been applied to malware that would traditionally be considered a worm, and the term has been diluted in common usage to refer to any sort of self-replicating malware.</p>
<p>Viruses can be caught in various stages of self-replication. A <em>germ</em> is the original form of a virus, prior to any replication. A virus which fails to replicate is called an <em>intended</em>. This may occur as a result of bugs in the virus, or encountering an unexpected version of an operating system. A virus can be <em>dormant</em>, where it is present but not yet infecting anything - for example, a Windows virus can reside on a Unix-based file server and have no effect there, but can be exported to Windows machines.<sup><a href="#f2_4" name="b2_4">4</a></sup></p>
<h4><a name="c215"></a>2.1.5 Worm</h4>
<table summary="Characteristics of worm">
<tr><td>Self-replicating:</td><td>yes</td></tr>
<tr><td>Population growth:</td><td>positive</td></tr>
<tr><td>Parasitic:</td><td>no</td></tr>
</table>
<p>A <em>worm</em> shares several characteristics with a virus. The most important characteristic is that worms are self-replicating too, but self-replication of a worm is distinct in two ways. First, worms are standalone,<sup><a href="#f2_5" name="b2_5">5</a></sup> and do not rely on other executable code. Second, worms spread from machine to machine across networks.</p>
<p>Like viruses, the first worms were fictional. The term "worm" was first used in 1975 by John Brunner in his science fiction novel <em><a href="/lib/mjb01.html">The Shockwave Rider</a></em>, (Interestingly, he used the term "virus" in the book too.)<sup><a href="#f2_6" name="b2_6">6</a></sup> Experiments with worms performing (non-malicious) distributed computations were done at Xerox PARC around 1980, but there were earlier examples. A worm called Creeper crawled around the Arpanet in the 1970s, pursued by another called Reaper which hunted and killed off Creepers.<sup><a href="#f2_7" name="b2_7">7</a></sup></p>
<p>A watershed event for the Internet happened on November 2, 1988, when a worm incapacitated thefledglingInternet. This worm is now called the Internet worm, or the Morris worm after its creator, Robert Morris, Jr. At the time, Morris hadjust started a Ph.D. at Cornell University. He had been intending for his worm to propagate slowly and unobtrusively, but what happened was just the opposite. Morris was later convicted for his worm's unauthorized computer
 
access and the costs incurred to clean up from it. He was fined, and sentenced to probation and community service.<sup><a href="#f2_8" name="b2_8">8</a></sup> Chapter 7 looks at this worm in detail.</p>
<h4><a name="c216"></a>2.1.6 Rabbit</h4>
<table summary="Characteristics of rabbit">
<tr><td>Self-replicating:</td><td>yes</td></tr>
<tr><td>Population growth:</td><td>zero</td></tr>
<tr><td>Parasitic:</td><td>no</td></tr>
</table>
<p><em>Rabbit</em> is the term used to describe malware that multiplies rapidly. Rabbits may also be called <em>bacteria</em>, for largely the same reason.</p>
<p>There are actually two kinds of rabbit.<sup><a href="#f2_110" name="b2_110">110</a></sup> The first is a program which tries to consume all of some system resource, like disk space. A "fork bomb," a program which creates new processes in an infinite loop, is a classic example of this kind of rabbit. These tend to leave painfully obvious trails pointing to the perpetrator, and are not of particular interest.</p>
<p>The second kind of rabbit, which the characteristics above describe, is a special case of a worm. This kind of rabbit is a standalone program which replicates itself across a network from machine to machine, <em>but</em> deletes the original copy of itself after replication. In other words, there is only one copy of a given rabbit on a network; it just hops from one computer to another.<sup><a href="#f2_9" name="b2_9">9</a></sup> Rabbits are rarely seen in practice.</p>
<h4><a name="c217"></a>2.1.7 Spyware</h4>
<table summary="Characteristics of spyware">
<tr><td>Self-replicating:</td><td>no</td></tr>
<tr><td>Population growth:</td><td>zero</td></tr>
<tr><td>Parasitic:</td><td>no</td></tr>
</table>
<p><em>Spyware</em> is software which collects information from a computer and transmits it to someone else. Prior to its emergence in recent years as a threat, the term "spyware" was used in 1995 as part of a joke, and in a 1994 Usenet posting looking for "spy-ware" information.<sup><a href="#f2_111" name="b2_111">111</a></sup></p>
<p>The exact information spyware gathers may vary, but can include anything which potentially has value:</p>
<ol>
<li>Usernames and passwords. These might be harvested from files on the machine, or by recording what the user types using a <em>keylogger</em>. A keylogger differs from a Trojan horse in that a keylogger passively captures keystrokes only; no active deception is involved.</li>
<li>Email addresses, which would have value to a spammer.</li>
<li>Bank account and credit card numbers.</li>
<li>Software license keys, to facilitate software pirating.</li>
</ol>
 
<p>Viruses and worms may collect similar information, but are not considered spyware, because spyware doesn't self-replicate.<sup><a href="#f2_112" name="b2_112">112</a></sup> Spyware may arrive on a machine in a variety of ways, such as bundled with other software that the user installs, or exploiting technical flaws in web browsers. The latter method causes the spyware to be installed simply by visiting a web page, and is sometimes called a <em>drive-by download</em>.</p>
<h4><a name="c218"></a>2.1.8 Adware</h4>
<table summary="Characteristics of adware">
<tr><td>Self-replicating:</td><td>no</td></tr>
<tr><td>Population growth:</td><td>zero</td></tr>
<tr><td>Parasitic:</td><td>no</td></tr>
</table>
<p><em>Adware</em> has similarities to spyware in that both are gathering information about the user and their habits. Adware is more marketing-focused, and may pop up advertisements or redirect a user's web browser to certain web sites in the hopes of making a sale. Some adware will attempt to target the advertisement to fit the context of what the user is doing. For example, a search for "Calgary" may result in an unsolicited pop-up advertisement for "books about Calgary."</p>
<p>Adware may also gather and transmit information about users which can be used for marketing purposes. As with spyware, adware does not self-replicate.</p>
<h4><a name="c219"></a>2.1.9 Hybrids, Droppers, and Blended Threats</h4>
<p>The exact type of malware encountered in practice is not necessarily easy to determine, even given these loose definitions of malware types. The nature of software makes it easy to create hybrid malware which has characteristics belonging to several different types.<sup><a href="#f2_10" name="b2_10">10</a></sup></p>
<p>A classic hybrid example was presented by Ken Thompson in his ACM Turing award lecture.<sup><a href="#f2_11" name="b2_11">11</a></sup> He prepared a special C compiler executable which, besides compiling C code, had two additional features:</p>
<ol>
<li>When compiling the login source code, his compiler would insert a back door to bypass password authentication.</li>
<li>When compiling the compiler's source code, it would produce a special compiler executable with these same two features.</li>
</ol>
<p>His special compiler was thus a Trojan horse, which replicated like a virus, and created back doors. This also demonstrated the vulnerability of the compiler tool chain: since the original source code for the compiler and login programs wasn't changed, none of this nefarious activity was apparent.</p>
<p>Another hybrid example was a game called Animal, which played twenty questions with a user. John Walker modified it in 1975, so that it would copy the most up-to-date version of itself into all user-accessible directories whenever it
 
was run. Eventually, Animals could be found roaming in every directory in the system.<sup><a href="#f2_113" name="b2_113">113</a></sup> The copying behavior was unknown to the game's user, so it would be considered a Trojan horse. The copying could also be seen as self-replication, and although it didn't infect other code, it didn't use a network either - not really a worm, not really a virus, but certainly exhibiting viral behavior.</p>
<p>There are other combinations of malware too. For example, a <em>dropper</em> is malware which leaves behind, or <em>drops</em>, other malware.<sup><a href="#f2_12" name="b2_12">12</a></sup> A worm can propagate itself, depositing a Trojan horse on all computers it compromises; a virus can leave a back door in its wake.</p>
<p>A <em>blended threat</em> is a virus that exploits a technical vulnerability to propagate itself, in addition to exhibiting "traditional" characteristics. This has considerable overlap with the definition of a worm, especially since many worms exploit technical vulnerabilities. These technical vulnerabilities have historically required precautions and defenses distinct from those that anti-virus vendors provided, and this rift may account for the duplication in terms.<sup><a href="#f2_114" name="b2_114">114</a></sup> The Internet worm was a blended threat, according to this definition.</p>
<h4><a name="c21a"></a>2.1.10 Zombies</h4>
<p>Computers that have been compromised can be used by an attacker for a variety of tasks, unbeknownst to the legitimate owner; computers used in this way are called <em>zombies</em>. The most common tasks for zombies are sending spam and participating in coordinated, large-scale denial-of-service attacks.</p>
<p>Sending spam violates the acceptable use policy of many Internet service providers, not to mention violating laws in some jurisdictions. Sites known to send spam are also blacklisted, marking sites that engage in spam-related activity so that incoming email from them can be summarily rejected. It is therefore ill-advised for spammers to send spam directly, in such a way that it can be traced back to them and their machines. Zombies provide a windfall for spammers, because they are a free, throwaway resource: spam can be relayed through zombies, which obscures the spammer's trail, and a blacklisted zombie machine presents no hardship to the spammer. <sup><a href="#f2_13" name="b2_13">13</a></sup></p>
<p>As for denials of service, one type of denial-of-service attack involves either flooding a victim's network with traffic, or overwhelming a legitimate service on the victim's network with requests. Launching this kind of attack from a single machine would be pointless, since one machine's onslaught is unlikely to generate enough traffic to take out a large target site, and traffic from one machine can be easily blocked by the intended victim. On the other hand, a large number of zombies all targeting a site at the same time can cause grief. A coordinated, network-based denial-of-service attack that is mounted from a large number of machines is called a <em>distributed denial-of-service</em> attack, or <em>DDoS</em> attack.</p>
 
<p>Networks of zombies need not be amassed by the person that uses them; the
use of zombie networks can be bought for a price.<sup><a href="#f2_14" name="b2_14">14</a></sup> Another issue is how to con-
trol zombie networks. One method involves zombies listening for commands on Internet Relay Chat (IRC) channels, which provides a relatively anonymous, scalable means of control. When this is used, the zombie networks are referred to as <em>botnets</em>, named after automated IRC client programs called <em>bots</em>.<sup><a href="#f2_15" name="b2_15">15</a></sup></p>
<h3><a name="c22"></a>2.2 Naming</h3>
<p>When a new piece of malware is spreading, the top priority of anti-virus companies is to provide an effective defense, quickly. Coming up with a catchy name for the malware is a secondary concern.</p>
<p>Typically the primary, human-readable name of a piece of malware is decided by the anti-virus researcher<sup><a href="#f2_16" name="b2_16">16</a></sup> who first analyzes the malware.<sup><a href="#f2_115" name="b2_115">115</a></sup> Names are often based on unique characteristics that malware has, either some feature of its code or some effect that it has. For example, a virus' name may be derived from some distinctive string that is found inside it, like <tt>"Your PC is now Stoned!"</tt><sup><a href="#f2_17" name="b2_17">17</a></sup> Virus writers, knowing this, may leave such clues deliberately in the hopes that their creation is given a particular name. Anti-virus researchers, knowing <em>this</em>, will ignore obvious naming clues so as not to play into the virus writer's hand.<sup><a href="#f2_18" name="b2_18">18</a></sup></p>
<p>There is no central naming authority for malware, and the result is that a piece of malware will often have several different names. Needless to say, this is confusing for users of anti-virus software, trying to reconcile names heard in alerts and media reports with the names used by their own anti-virus software. To compound the problem, some sites use anti-virus software from multiple different vendors, each of whom may have different names for the same, piece of malware.<sup><a href="#f2_19" name="b2_19">19</a></sup> Common naming would benefit anti-virus researchers talking to one another too.<sup><a href="#f2_20" name="b2_20">20</a></sup></p>
<p>Unfortunately, there isn't likely to be any central naming authority in the near future, for two reasons.<sup><a href="#f2_21" name="b2_21">21</a></sup> First, the current speed of malware propagation precludes checking with a central authority in a timely manner.<sup><a href="#f2_22" name="b2_22">22</a></sup> Second, it isn't always clear what would need to be checked, since one distinct piece of malware may manifest itself in a practically infinite number of ways.</p>
<p>Recommendations for malware naming do exist, but in practice are not usually followed,<sup><a href="#f2_23" name="b2_23">23</a></sup> and anti-virus vendors maintain their own separately-named databases of malware that they have detected. It would, in theory, be possible to manually map malware names between vendors using the information in these databases, but this would be a tedious and error-prone task.</p>
<p>A tool called VGrep automates this process of mapping names.<sup><a href="#f2_116" name="b2_116">116</a></sup> First, a machine is populated with the malware of interest. Then, as shown in Figure 2.1, each anti-virus product examines each file on the machine, and outputs what (if any) malware it detects. VGrep gathers all this anti-virus output and collates
 
it for later searching. The real technical challenge is not collating the data, but simply getting usable, consistent output from a wide range of anti-virus products.</p>
<div align="center">
<img src="img/mja01/fig21.gif" alt="Figure 2.1. VGrep operation"/>
<p><strong>Figure 2.1. VGrep operation</strong></p>
</div>
<p>The naming problem and the need for tools like VGrep can be demonstrated using an example. Using VGrep and cross-referencing vendor's virus databases, the partial list of names below for the same worm can be found.<sup><a href="#f2_24" name="b2_24">24</a></sup></p>
<ul>
<li>Bagle.C</li>
<li>Email-worm.Win32.Bagle.c</li>
<li><a class="__cf_email__" href="/cdn-cgi/l/email-protection" data-cfemail="edbadedfc2af8c8a8188c38eada0a0">[email&#160;protected]</a><script data-cfhash='f9e31' type="text/rocketscript">/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */</script></li>
<li><a class="__cf_email__" href="/cdn-cgi/l/email-protection" data-cfemail="287f1b1a066a4d494f444d066b684545">[email&#160;protected]</a><script data-cfhash='f9e31' type="text/rocketscript">/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */</script></li>
<li>WORM_BAGLE.C</li>
<li>Worm.Bagle.A3</li>
</ul>
<p>These results highlight some of the key identifiers used for naming malware:<sup><a href="#f2_117" name="b2_117">117</a></sup></p>
<dl>
<dt><strong>Malware type.</strong></dt><dd>This is the type of the threat which, for this example, is a worm. Platform specifier. The environment in which the malware runs; this worm needs the Windows 32-bit operating system API C'W32" and "Win32").<sup><a href="#f2_25" name="b2_25">25</a></sup> More generally, the platform specifier could be any execution environment, such as an application's programming language (e.g., "VBS" for "Visual Basic Script"), or may even need to specify a combination of hardware and software platform.</dd>
<dt><strong>Family name.</strong></dt><dd>The family name is the "human-readable" name of the malware that is usually chosen by the anti-virus researcher performing the analysis. This example shows several different, but obviously related, names. The relationship is not always obvious: "Nachi" and "Welchia" are the same worm, for instance.</dd>
 
<dt><strong>Variant.</strong></dt><dd>Not unlike legitimate software, a piece of malware tends to be released multiple times with minor changes.<sup><a href="#f2_26" name="b2_26">26</a></sup> This change is referred to as the malware's <em>variant</em> or, following the biological analogy, the <em>strain</em> of the malware.
<p>Variants are usually assigned letters in increasing order of discovery, so this "C" variant is the third B[e]agle found. Particularly persistent families with many variants will have multiple letters, as "Z" gives way to "AA." Unfortunately, this is not unusual - some malware has dozens of variants.<sup><a href="#f2_27" name="b2_27">27</a></sup></p></dd>
<dt><strong>Modifiers.</strong></dt><dd>Modifiers supply additional information about the malware, such as its primary means of propagation. For example, "mm" stands for "mass mailing."</dd>
</dl>
<p>The results also highlight the fact that not all vendors supply all these identifiers for every piece of malware, that there is no common agreement on the specific identifiers used, and that there is no common syntax used for names.</p>
<p>Besides VGrep, there are online services where a suspectfilecan be uploaded and examined by multiple anti-virus products. Output from a service like this also illustrates the variety in malware naming:<sup><a href="#f2_28" name="b2_28">28</a></sup></p>
<table summary="">
<tr><td>Worm/Mydoom.BC</td><td>Win32:Mytob-D</td><td>I-Worm/Mydoom</td></tr>
<tr><td>Win32.Worm.Mytob.C</td><td>Worm.Mytob.C</td><td>Win32.HLLM.MyDoom.22</td></tr>
<tr><td><a class="__cf_email__" href="/cdn-cgi/l/email-protection" data-cfemail="9dcaaeafb2d0e4e9f2ffb3d9ddf0f0">[email&#160;protected]</a><script data-cfhash='f9e31' type="text/rocketscript">/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */</script></td><td>W32/Mytob.C-mm</td><td>Net-Worm.Win32.Mytob.c</td></tr>
<tr><td>Win32/Mytob.D</td><td>Mytob.D</td><td>&nbsp;</td></tr>
</table>
<p>Ultimately, however, the biggest concern is that the malware is detected and eliminated, not what it's called.</p>
<h3><a name="c23"></a>2.3 Authorship</h3>
<p>People whose computers are affected by malware typically have a variety of colorful terms to describe the person who created the malware. This book will use the comparatively bland terms <em>malware author</em> and <em>malware writer</em> to describe people who create malware; when appropriate, more specific terms like <em>virus writer</em> may be used too.</p>
<p>There's a distinction to be made between the malware author and the malware distributor. Writing malware doesn't imply distributing malware, and vice versa, and there have been cases where the two roles are known to have been played by different people.<sup><a href="#f2_29" name="b2_29">29</a></sup> Having said that, the malware author and distributor will be assumed to be the same person throughout this book, for simplicity.</p>
<p>Is a malware author a "hacker?" Yes and no. The term <em>hacker</em> has been distorted by the media and popular usage to refer to a person who breaks into
 
computers, especially when some kind of malicious intent is involved. Strictly speaking, a person who breaks into computers is a <em>cracker</em>, not a hacker,<sup><a href="#f2_118" name="b2_118">118</a></sup> and there may be a variety of motivations for doing so. In geek parlance, being called a hacker actually has a positive connotation, and means a person who is skilled at computer programming; hacking has nothing to do with computer intrusion or malware.</p>
<p>Hacking (in the popular sense of the word) also implies a manual component, whereas the study of malware is the study of large-scale, automated forms of attack. Because of this distinction and the general confusion over the term, this book will not use it in relation to malware.</p>
<h3><a name="c24"></a>2.4 Timeline</h3>
<p>Figure 2.2 puts some important events in context. With the exception of adware and spyware, which appeared in the late 1990s, all of the different types of malware were known about in the early 1970s. The prevalence of virus, worms, and other malware has been gradually building steam since the mid-1980s, leaving us with lots of threats - no matter how they're counted.</p>
<div align="center">
<img src="img/mja01/fig22.gif" alt="Figure 2.2. Timeline of events"/>
<p><strong>Figure 2.2. Timeline of events</strong></p>
</div>
 
<p><strong>Notes for Chapter 2</strong></p>
<p><a name="f2_1" href="#b2_1">1</a> This case doesn't appear to have gone to trial yet, so the person may yet be found not guilty. Regardless, the charges in the indictment [327] serve as an example of how a logic bomb can be used maliciously.</p>
<p><a name="f2_2" href="#b2_2">2</a> The term "computer virus" is preferable if there's any possibility of confusion with biological viruses.</p>
<p><a name="f2_3" href="#b2_3">3</a> Bassham and Polk [28] note that innovation is important for the longevity of computer viruses, especially if the result is something that hasn't yet been seen by anti-virus software. They also point out that non-destructive viruses have an increased chance of survival, by not drawing attention to themselves.</p>
<p><a name="f2_4" href="#b2_4">4</a> These three definitions are based on Harley et al. [137]; Radatti [258] talks about viruses passing through unaffected platforms, which he calls Typhoid Mary Syndrome.</p>
<p><a name="f2_5" href="#b2_5">5</a> Insofar as a worm can be said to stand.</p>
<p><a name="f2_6" href="#b2_6">6</a> This farsighted book also included ideas about an internet and laser printers [50].</p>
<p><a name="f2_7" href="#b2_7">7</a> The Xerox work is described in Shoch and Hupp [287], and both they and Dewdney [91] mention Creeper and Reaper. There were two versions of Creeper, of which the first would be better called a rabbit, the second a worm.</p>
<p><a name="f2_8" href="#b2_8">8</a> This version of the event is from [329]. An interesting historical twist: Morris, Jr.'s father was one of the people playing Darwin in the early 1960s at Bell Labs, and created 'The species which eventually wiped out all opposition...' [9, page 95].</p>
<p><a name="f2_9" href="#b2_9">9</a> Nazario [229] calls this second kind of rabbit a "jumping executable worm."</p>
<p><a name="f2_10" href="#b2_10">10</a> "Hybrid" is used in a generic sense here; Harley et al. [137] use the term "hybrid viruses" to describe viruses that execute concurrently with the infected code.</p>
<p><a name="f2_11" href="#b2_11">11</a> From Thompson [322]; he simply calls it a Trojan horse.</p>
<p><a name="f2_12" href="#b2_12">12</a> This differs from Harley et al. [137], who define a dropper to be a program that installs malware. However, this term is so often applied to malware that this narrower definition is used here.</p>
<p><a name="f2_13" href="#b2_13">13</a> There are many other spamming techniques besides this; Spammer-X [300, Chapter 3] has more information. Back-door functionality left behind by worms has been used for sending spam in this manner [188].</p>
<p><a name="f2_14" href="#b2_14">14</a> Acohido and Swartz [2] mention a $2000-$3000 rental fee for 20,000 zombies, but prices have been dropping [300].</p>
 
<p><a name="f2_15" href="#b2_15">15</a> Cooke et al. [79] looks at botnet evolution, and takes the more general view that botnets are just zombie armies, and need a controlling communication channel, but that channel doesn't have to be IRC. There are also a wide variety of additional uses for botnets beyond those listed here [319].</p>
<p><a name="f2_16" href="#b2_16">16</a> In the anti-virus industry, people who analyze malware for anti-virus companies are referred to as "researchers." This is different from the academic use of the term.</p>
<p><a name="f2_17" href="#b2_17">17</a> This was one suggested way to find the Stoned virus [290].</p>
<p><a name="f2_18" href="#b2_18">18</a> Lyman [189], but this is common knowledge in the anti-virus community.</p>
<p><a name="f2_19" href="#b2_19">19</a> Diversity is usually a good thing when it comes to defense, and large sites will often use different anti-virus software on desktop machines than they use on their gateway machines. In a panel discussion at the 2003 <em>Virus Bulletin</em> conference, one company revealed that they used eleven different anti-virus products.</p>
<p><a name="f2_20" href="#b2_20">20</a> While the vast majority of interested parties want common naming, their motivations for wanting this may be different, and they may treat different parts of the name as being significant [182].</p>
<p><a name="f2_21" href="#b2_21">21</a> Having said this, an effort has been announced recently to provide uniform names for malware. The "Common Malware Enumeration" will issue a unique identifier for malware causing major outbreaks, so users can refer to highly mneumonic names like "CME-42," which intuitively may have been issued before "CME-40" and "CME-41" [176].</p>
<p><a name="f2_22" href="#b2_22">22</a> Of course, this begs the question of why such a central authority wasn't established in the early days of malware prevalence, when there was less malware and the propagation speeds tended to be much, much slower.</p>
<p><a name="f2_23" href="#b2_23">23</a> CARO, the Computer Antivirus Research Organization, produced virus-naming guidelines in 1991 [53], which have since been updated [109].</p>
<p><a name="f2_24" href="#b2_24">24</a> Vendor names have been removed from the results.</p>
<p><a name="f2_25" href="#b2_25">25</a> "API" stands for "application programming interface."</p>
<p><a name="f2_26" href="#b2_26">26</a> Not all variants necessarily come from the same source. For example, the "B" variant of the Blaster worm was released by someone who had acquired a copy of the "A" variant and modified it [330].</p>
<p><a name="f2_27" href="#b2_27">27</a> A few, like Gaobot, have hundreds of variants, and require three letters to describe their variant!</p>
<p><a name="f2_28" href="#b2_28">28</a> This example is from [47], again with vendor information removed.</p>
<p><a name="f2_29" href="#b2_29">29</a> Dellinger's "Virus 2" spread courtesy of the virus writer's friends [87], and secondhand stories indicate that Stoned was spread by someone besides its author [119,137,290]. Malware writers are rarely caught or come forward, so discovering these details is unusual.</p>
 
<p><a name="f2_100" href="#b2_100">100</a> For example, Adleman [3] and Cohen [75].</p>
<p><a name="f2_101" href="#b2_101">101</a> The details of the case may be found in [328]; [326] has sentencing information.</p>
<p><a name="f2_102" href="#b2_102">102</a> Paraphrased liberally from Virgil's Aeneid, Book II [336].</p>
<p><a name="f2_103" href="#b2_103">103</a> Anderson [12].</p>
<p><a name="f2_104" href="#b2_104">104</a> A sidebar in Harley et al. [137, page 60] has an amusing collection of suggested plural forms that didn't make the cut.</p>
<p><a name="f2_105" href="#b2_105">105</a> Benford [33] and Gerrold [118], respectively. Benford talks about his real computer viruses in this collection of reprinted stories.</p>
<p><a name="f2_106" href="#b2_106">106</a> As told in Cohen [74].</p>
<p><a name="f2_107" href="#b2_107">107</a> Skrenta [289] and Dellinger [87].</p>
<p><a name="f2_108" href="#b2_108">108</a> The whole sordid tale is in Rosen [267].</p>
<p><a name="f2_109" href="#b2_109">109</a> The original Core War article is Dewdney [91]; Darwin is described in [9, 201].</p>
<p><a name="f2_110" href="#b2_110">110</a> Bontchev [46].</p>
<p><a name="f2_111" href="#b2_111">111</a> Vossen [338] and van het Groenewoud [331], respectively.</p>
<p><a name="f2_112" href="#b2_112">112</a> This definition of spyware and adware follows Gordon [124].</p>
<p><a name="f2_113" href="#b2_113">113</a> Walker wrote a letter to Dewdney [340], correcting Dewdney's explanation of Animal in his column [92] (this column also mentions Skrenta's virus).</p>
<p><a name="f2_114" href="#b2_114">114</a> Chien and Szor [70] explain blended threats and the historical context of the anti-virus industry with respect to them.</p>
<p><a name="f2_115" href="#b2_115">115</a> Bontchev [44] and Lyman [189] describe the process by which a name is assigned.</p>
<p><a name="f2_116" href="#b2_116">116</a> VGrep was originally by Ian Whalley; this discussion of its operation is based on its online documentation [333].</p>
<p><a name="f2_117" href="#b2_117">117</a> This description is based on the CARO identifiers and terminology [109].</p>
<p><a name="f2_118" href="#b2_118">118</a> The Jargon File lists the many nuances of "hacker," along with a hitchhiker's guide to the hacker subculture [260].</p>
 
 
<h2><a name="c3"></a>Chapter 3 Viruses</h2>
<p>A computer virus has three parts:<sup><a href="#f3_100" name="b3_100">100</a></sup></p>
<dl>
<dt><strong>Infection mechanism</strong></dt><dd>How a virus spreads, by modifying other code to contain a (possibly altered) copy of the virus. The exact means through which a virus spreads is referred to as its <em>infection vector</em>. This doesn't have to be unique - a virus that infects in multiple ways is called <em>multipartite</em>.</dd>
<dt><strong>Trigger</strong></dt><dd>The means of deciding whether to deliver the payload or not.</dd>
<dt><strong>Payload</strong></dt><dd>What the virus does, besides spread. The payload <em>may</em> involve damage, either intentional or accidental. Accidental damage may result from bugs in the virus, encountering an unknown type of system, or perhaps unanticipated multiple viral infections.</dd>
</dl>
<p>Except for the infection mechanism, the other two parts are optional, because infection is one of the key defining characteristics of a virus. In the absence of infection, only the trigger and payload remain, which is a logic bomb.</p>
<p>In pseudocode, a virus would have the structure below. The <tt>trigger</tt> function would return a boolean, whose value would indicate whether or not the trigger conditions were met. The payload could be anything, of course.</p>
<pre class="source">
	def virus():
	    infect()
            if trigger() is true:
               payload()
</pre>
<p>Infection is done by selecting some target code and infecting it, as shown below. The target code is locally accessible to the machine where the virus
 
runs, applying the definition of viruses from the last chapter. Locally accessible targets may include code in shared network directories, though, as these directories are made to appear locally accessible.</p>
<p>Generally, <em>k</em> targets may be infected each time the infection code below is run. The exact method used to select targets varies, and may be trivial, as in the case of the boot-sector infectors in Section 3.1.1. The tricky part of <tt>select_target</tt> is that the virus doesn't want to repeatedly re-infect the same code; that would be a waste of effort, and may reveal the presence of the virus. <tt>Select_target</tt> has to have some way to detect whether or not some potential target code is already infected, which is a double-edged sword. If the virus can detect itself, then so can anti-virus software. The <tt>infect_code</tt> routine performs the actual infection by placing some version of the virus' code in the target.</p>
<pre class="source">
	def infect():
            repeat <em>k</em> times:
                target = select_target()
                if no target:
                    return
                infect_code(target)
</pre>
<p>Viruses can be classified in a variety of ways. The next two sections classify them along orthogonal axes: the type of target the virus tries to infect, and the method the virus uses to conceal itself from detection by users and anti-virus software. Virus creation need not be difficult, either; the virus classification is followed by a look at do-it-yourself virus kits for the programming-challenged.</p>
<h3><a name="c31"></a>3.1 Classification by Target</h3>
<p>One way of classifying viruses is by what they try to infect. This section looks at three: boot-sector infectors, executable file infectors, and data file infectors (a.k.a. macro viruses).</p>
<h4><a name="c311"></a>3.1.1 Boot-Sector Infectors</h4>
<p>Although the exact details vary, the basic boot sequence on most machines goes through these steps:</p>
<ol>
<li>Power on.</li>
<li>ROM-based instructions run, performing a self-test, device detection, and initialization. The boot device is identified, and the boot block is read from it; typically the boot block consists of the initial block(s) on the device.<sup><a href="#f3_1" name="b3_1">1</a></sup> Once the boot block is read, control is transferred to the loaded code. This step is referred to as the <em>primary boot</em>.</li>
 
<li>The code loaded during the primary boot step loads a larger, more sophisticated program that understands the boot device's filesystem structure, and transfers control to it. This is the <em>secondary boot</em>.</li>
<li>The secondary boot code loads and runs the operating system kernel.</li>
</ol>
<div align="center">
<img src="img/mja01/fig31.gif" alt="Figure 3.1. Multiple boot sector infections"/>
<p><strong>Figure 3.1. Multiple boot sector infections</strong></p>
</div>
<p>A <em>boot-sector infector</em>, or BSI, is a virus that infects by copying itself to the boot block. It may copy the contents of the former boot block elsewhere on the disk first,<sup><a href="#f3_2" name="b3_2">2</a></sup> so that the virus can transfer control to it later to complete the booting process.</p>
<p>One potential problem with preserving the boot block contents is that block allocation on disk is filesystem-specific. Properly allocating space to save the boot block requires a lot of code, a luxury not available to BSIs. An alternate method is to always copy the original boot block to some fixed, "safe" location on disk. This alternate method can cause problems when a machine is infected multiple times by different viruses that happen to use that same safe location, as shown in Figure 3.1. This is an example of unintentional damage being done by a virus, and has actually occurred: Stoned and Michelangelo were BSIs that both picked the same disk block as their safe location.<sup><a href="#f3_101" name="b3_101">101</a></sup></p>
<p>In general, infecting the boot sector is strategically sound: the virus may be in a known location, but it establishes itself before any anti-virus software starts or operating system security is enabled. But BSIs are rare now. Machines are rebooted less often, and there is very little use of bootable media like floppy disks.<sup><a href="#f3_3" name="b3_3">3</a></sup> From a defensive point of view, most operating systems prevent writing to the disk's boot block without proper authorization, and many a BIOS<sup><a href="#f3_4" name="b3_4">4</a></sup> has boot block protection that can be enabled.</p>
 
<h4><a name="c312"></a>3.1.2 File Infectors</h4>
<p>Operating systems have a notion of files that are executable. In a broader sense, executable files may also include files that can be run by a command-line user "shell." A <em>file infector</em> is a virus that infects files which the operating system or shell consider to be executable; this could include batch files and shell scripts, but binary executables are the most common target.</p>
<p>There are two main issues for file infectors:</p>
<ol>
<li>Where is the virus placed?</li>
<li>How is the virus executed when the infected file is run?</li>
</ol>
<p>For BSIs, the answer to these questions was apparent. A BSI places itself in the boot block and gets executed through a machine's normal boot sequence. File infectors have a few more options at their disposal, though, and often the answers to these questions are interdependent. The remainder of this section is organized around the answer to the first question: where is the virus placed?</p>
<h5><a name="c3121"></a>3.1.2.1 Beginning of File</h5>
<p>Older, very simple executable file formats like the .COM MS-DOS format would treat the entirefileas a combination of code and data. When executed, the entire file would be loaded into memory, and execution would start by jumping to the beginning of the loaded file.<sup><a href="#f3_102" name="b3_102">102</a></sup></p>
<p>In this case, a virus that places itself at the start of the file gets control first when the infected file is run, as illustrated in Figure 3.2. This is called a <em>prepending</em> virus. Inserting itself at the start of a file involves some copying, which isn't difficult, but isn't the absolute easiest way to infect a file.</p>
<h5><a name="c3122"></a>3.1.2.2 End of File</h5>
<p>In contrast, appending code onto the end of a file is extremely easy. A virus that places itself at the end of a file is called an <em>appending</em> virus.</p>
<p>How does the virus get control? There are two basic possibilities:</p>
<ul>
<li>The original instruction(s) in the code can be saved, and replaced by a jump to the viral code. Later, the virus will transfer control back to the code it infected. The virus may try to run the original instructions directly in their saved location, or the virus may restore the infected code back to its original state and run it.</li>
<li>Many executable file formats specify the start location in a file header. The virus can change this start location to point to its own code, then jump to the original start location when done.</li>
</ul>
<p>Figure 3.3 shows an appending virus using the latter scheme.</p>
 
<div align="center">
<img src="img/mja01/fig32.gif" alt="Figure 3.2. Prepending virus"/>
<p><strong>Figure 3.2. Prepending virus</strong></p>
</div>
<div align="center">
<img src="img/mja01/fig33.gif" alt="Figure 3.3. Appending virus"/>
<p><strong>Figure 3.3. Appending virus</strong></p>
</div>
<h5><a name="c3123"></a>3.1.2.3 Overwritten into File</h5>
<p>An <em>overwriting</em> virus places itself <em>atop</em> part of the original code.<sup><a href="#f3_5" name="b3_5">5</a></sup> This avoids an obvious change in file size that would occur with a prepending or appending virus, and the virus' code can be placed in a location where it will get control.</p>
 
<p>Obviously, overwriting code blindly is almost certain to break the original code and lead to rapid discovery of the virus. There are several options, with varying degrees of complexity and risk.</p>
<ul>
<li>The virus can look for, and overwrite, sections of repeated values in the hopes of avoiding damage to the original code.<sup><a href="#f3_6" name="b3_6">6</a></sup> Such values would tend to appear in a program's data rather than in the code, so a mechanism for gaining control during execution would have to be used as well. Ideally, the virus could restore the repeated value once it has finished running.</li>
<li>The virus can overwrite an arbitrary part of a file if it can somehow preserve the original contents elsewhere, similar to the BSI approach. An innocent-looking data file of some kind, like a JPEG file, could be used to stash the original contents. A less-portable approach might take low-level details into account: many filesystems overallocate space for files, and an overwriting virus could quietly use this extra disk space without it showing up in normal filesystem operations.</li>
<li>Space may be overallocated inside afiletoo. Parts of an executable file may be padded so that they are aligned to a page boundary, so that the operating system kernel can efficiently map the executables into memory. The net result is unused space inside executable files where a virus may be located.<sup><a href="#f3_7" name="b3_7">7</a></sup></li>
<li>Conceivably, a virus could compress a part of the original code to make space for itself, and decompress the original code when the virus has completed execution. However, room would have to be made for both the virus and the decompression code.</li>
</ul>
<p>None of these options is likely to yield a large amount of space, so overwriting viruses must be small.</p>
<h5><a name="c3124"></a>3.1.2.4 Inserted into File</h5>
<p>Another possibility is that a virus can insert itself into the target code, moving the target code out of the way, and even interspersing small pieces of virus code with target code. This is no easy feat: branch targets in the code have to be changed, data locations must be updated, and linker relocation information needs modification. Needless to say, this file infection technique is rarely seen.<sup><a href="#f3_8" name="b3_8">8</a></sup></p>
<h5><a name="c3125"></a>3.1.2.5 Not in File</h5>
<p>A <em>companion</em> virus is one which installs itself in such a way that it is naturally executed before the original code. The virus never modifies the infected code, and gains control by taking advantage of the process by which the operating system or shell searches for executable files. Although this bears the hallmarks of a Trojan horse, a companion virus is a "real" virus by virtue of self-replication.</p>
 
<p>The easiest way to explain companion viruses is by example.<sup><a href="#f3_103" name="b3_103">103</a></sup></p>
<ul>
<li>The companion virus can place itself earlier in the search path, with the same name as the target file, so that the virus will be executed first when an attempt is made to execute the target file.</li>
<li>MS-DOS searches for an executable named <tt>foo</tt> by looking for <tt>foo.com</tt>, <tt>foo.exe</tt>, and <tt>foo.bat</tt>, in that order. If the target file is a .EXE file, then the companion virus can be a .COM file with the same name.</li>
<li>The target file can be renamed, and the companion virus can be given the target file's original name.</li>
<li>Windows associates file types (as determined by the filename's extension) with applications in the Registry. With strategic Registry changes, the association for .EXE files can be made to run the companion virus instead of the original executable. Effectively, all executable files are infected at once.</li>
<li>The ELF file format commonly used on recent Unix systems has an "interpreter" specified in each executable's file header - this invariably points to the system's run-time linker.<sup><a href="#f3_104" name="b3_104">104</a></sup> A companion virus can replace the run-time linker, again causing all executables to be infected at once.</li>
<li>Companion viruses are possible even in GUI-based environments. A target application's icon can be overlaid with the icon for the companion virus. When a user clicks on what they <em>think</em> is the application's icon, the companion virus runs instead.</li>
</ul>
<h4><a name="c313"></a>3.1.3 Macro Viruses</h4>
<p>Some applications allow data files, like word processor documents, to have "macros" embedded in them. Macros are short snippets of code written in a language which is typically interpreted by the application, a language which provides enough functionality to write a virus. Thus, <em>macro viruses</em> are better thought of as data file infectors, but since their predominant form has been macros, the name has stuck.</p>
<p>When a macro-containing document is loaded by the application, the macros can be caused to run automatically, which gives control to the macro virus. Some applications warn the user about the presence of macros in a document, but these warnings may be easily ignored.</p>
<p>A proof-of-concept of macro viruses was published in 1989,<sup><a href="#f3_105" name="b3_105">105</a></sup> in response to rumors of their existence. Macro viruses didn't hit the mainstream until 1995, when the Concept virus was distributed, targeting Microsoft Word documents across multiple platforms.<sup><a href="#f3_9" name="b3_9">9</a></sup></p>
<p>Concept's operation is shown in Figure 3.4. Word has a persistent, global set of macros which apply to all edited documents, and this is Concept's target:
 
once installed in the global macros, it can infect all documents edited in the future. A document infected by Concept includes two macros that have special properties in Word.</p>
<div align="center">
<img src="img/mja01/fig34.gif" alt="Figure 3.4. Concept in action"/>
<p><strong>Figure 3.4. Concept in action</strong></p>
</div>
<dl>
<dt><strong>AutoOpen</strong></dt><dd>Any code in the AutoOpen macro is run automatically when the file is opened. This is how an infected document gains control.</dd>
<dt><strong>FileSaveAs</strong></dt><dd>The code in the FileSaveAs macro is run when its namesake menu item (File... Save As...) is selected. In other words, this code can be used to infect any as-yet-uninfected document that is being saved by the user.</dd>
</dl>
<p>From a technical standpoint, macro languages are easier to use than lower-level programming languages, so macro viruses drastically lower the barrier to virus creation.</p>
<h3><a name="c32"></a>3.2 Classification by Concealment Strategy</h3>
<p>Another way of classifying viruses is by how they try to conceal themselves, both from users and from anti-virus software.</p>
<h4><a name="c321"></a>3.2.1 No Concealment</h4>
<p>Not hiding at all is one concealment strategy which is remarkably easy to implement in a computer virus. It goes without saying, however, that it's not
 
very effective - once the presence of a virus is known, it's trivial to detect and analyze.</p>
<div align="center">
<img src="img/mja01/fig35.gif" alt="Figure 3.5. Encrypted virus pseudocode"/>
<p><strong>Figure 3.5. Encrypted virus pseudocode</strong></p>
</div>
<h4><a name="c322"></a>3.2.2 Encryption</h4>
<p>With an <em>encrypted</em> virus, the idea is that the virus <em>body</em> (infection, trigger, and payload) is encrypted in some way to make it harder to detect. This "encryption" is not what cryptographers call encryption; virus encryption is better thought of as obfuscation. (Where it's necessary to distinguish between the two meanings of the word, I'll use the term "strong encryption" to mean encryption in the cryptographic sense.)</p>
<p>When the virus body is in encrypted form, it's not runnable until decrypted. What executes first in the virus, then, is a <em>decryptor loop</em>, which decrypts the virus body and transfers control to it. The general principle is that the decryptor loop is small compared to the virus body, and provides a smaller profile for anti-virus software to detect.</p>
<p>Figure 3.5 shows pseudocode for an encrypted virus. A decryptor loop can decrypt the virus body in place, or to another location; this choice may be dictated by external constraints, like the writability of the infected program's code. This example shows an in-place decryption.</p>
<p>How is virus encryption done? Here are six ways:<sup><a href="#f3_106" name="b3_106">106</a></sup></p>
<dl>
<dt><strong>Simple encryption.</strong></dt><dd>No key is used for simple encryption, just basic parameterless operations, like incrementing and decrementing, bitwise rotation, arithmetic negation, and logical NOT:<sup><a href="#f3_10" name="b3_10">10</a></sup>
 
<table summary="simple encryption">
<tr><th>Encryption</th><th>Decryption</th></tr>
<tr><td>inc body<sub>i</sub></td><td>dec body<sub>i</sub></td></tr>
<tr><td>rol body<sub>i</sub></td><td>ror body<sub>i</sub></td></tr>
<tr><td>neg body<sub>i</sub></td><td>neg body<sub>i</sub></td></tr>
</table></dd>
<dt><strong>Static encryption key.</strong></dt><dd>A static, constant key is used for encryption which doesn't change from one infection to the next. The operations used would include arithmetic operations like addition, and logical operations like XOR. Notice that the use of reversible operations is a common feature of simpler types of virus encryption. In pseudocode:
<table summary="static encryption key">
<tr><th>Encryption</th><th>Decryption</th></tr>
<tr><td>body<sub>i</sub> + 123</td><td>body<sub>i</sub> - 123</td></tr>
<tr><td>body<sub>i</sub> xor 42</td><td>body<sub>i</sub> xor 42</td></tr>
</table></dd>
<dt><strong>Variable encryption key.</strong></dt><dd>The key begins as a constant value, but changes as the decryption proceeds. For example:
<pre class="source">
	key = 123
	for i in 0...length(body):
            <img src="/img/cache/bfca81c922a87b93d002b24cc010a2a4.gif" alt="\text{body_i}" valign="middle"/> = <img src="/img/cache/bfca81c922a87b93d002b24cc010a2a4.gif" alt="\text{body_i}" valign="middle"/> xor key
	    key = key + <img src="/img/cache/bfca81c922a87b93d002b24cc010a2a4.gif" alt="\text{body_i}" valign="middle"/>
</pre></dd>
<dt><strong>Substitution cipher.</strong></dt><dd>A more general encryption could employ lookup tables which map byte value between their encrypted and decrypted forms. Here, encrypt and decrypt are 256-byte arrays, initialized so that if <tt>encrypt[j]=k</tt>, then <tt>decrypt[k]=j</tt>:
<table summary="Substitution cipher">
<tr><th>Encryption</th><th>Decryption</th></tr>
<tr><td>body<sub>i</sub>=encrypt[body<sub>i</sub>]</td><td>body<sub>i</sub>=decrypt[body<sub>i</sub>]</td></tr>
</table>
<p>This substitution cipher is a 1:1 mapping, but in actual fact, the virus body may not contain all 256 possible byte values. A homophonic substitution cipher allows a <em>1:n</em> mapping, increasing complexity by permitting multiple encrypted values to correspond to one decrypted value.</p></dd>
<dt><strong>Strong encryption.</strong></dt><dd>There is no reason why viruses cannot use strong encryption. Previously, code size might have been a factor, if the virus would have to carry strong decryption code with it, but this is no longer a problem:
 
most systems now contain strong encryption libraries which can be used by viruses <sup><a href="#f3_107" name="b3_107">107</a></sup></dd>
</dl>
<p>The major weakness in the encryption schemes above is that the encrypted virus body is the same from one infection to the next. That constancy makes a virus as easy to detect as one using no concealment at all! With <em>random encryption</em> keys,<sup><a href="#f3_108" name="b3_108">108</a></sup> this error is avoided: the key used for encryption changes randomly with each new infection. This idea can be applied to any of the encryption types described here. Obviously, the virus' decryptor loop must be updated for each infection to incorporate the new key.</p>
<h4><a name="c323"></a>3.2.3 Stealth</h4>
<p>A <em>stealth</em> virus is a virus that actively takes steps to conceal the infection itself, not just the virus body. Furthermore, a stealth virus tries to hide from everything, not just anti-virus software. Some examples of stealth techniques are below.<sup><a href="#f3_109" name="b3_109">109</a></sup></p>
<ul>
<li>An infected file's original timestamp can be restored after infection, so that the file doesn't look freshly-changed.</li>
<li>The virus can store (or be capable of regenerating) all pre-infection information about a file, including its timestamp, file size, and the file's contents. Then, system I/O calls can be intercepted, and the virus would play back the original information in response to any I/O operations on the infected file, making it appear uninfected. This technique is applicable to boot block I/O too.
<p>The exact method of intercepting I/O calls depends on the operating system. Under MS-DOS, for instance, I/O requests are made with interrupt calls, whose handlers are located via user-accessible interrupt vectors; the virus need only modify the interrupt vector to insert itself into the chain of interrupt handlers. On other systems, I/O is performed using shared libraries, so a virus can impose itself into key shared library routines to intercept I/O calls for most applications.</p></li>
<li>Some systems store the secondary boot loader as consecutive disk blocks, to make the primary boot loader's task simpler. On these systems, there are two views of the secondary boot loader, as a sequence of blocks, and as a file in the filesystem. A virus can insert itself into the secondary boot loader's blocks, relocating the original blocks elsewhere in the filesystem. The end result is that the usual, filesystem view shows no obvious changes, but the virus is hidden and gets run courtesy of the real primary boot loader.<sup><a href="#f3_110" name="b3_110">110</a></sup></li>
</ul>
<p>A variation is a <em>reverse stealth</em> virus, which makes everything look infected - the damage is done by anti-virus software frantically (and erroneously) trying to disinfect.<sup><a href="#f3_111" name="b3_111">111</a></sup></p>
 
<p>Stealth techniques overlap with techniques used by <em>rootkits</em>. Rootkits were originally toolkits for people who had broken into computers; they used these toolkits to hide their tracks and avoid detection.<sup><a href="#f3_112" name="b3_112">112</a></sup> Malware now uses rootkits too: for example, the Ryknos Trojan horse tried to hide itself using a rootkit intended for digital-rights management.<sup><a href="#f3_113" name="b3_113">113</a></sup></p>
<h4><a name="c324"></a>3.2.4 Oligomorphism</h4>
<p>Assuming an encrypted virus' key is randomly changed with each new infection, the only unchanging part of the virus is the code in the decryptor loop. Anti-virus software will exploit this fact for detection, so the next logical development is to change the decryptor loop's code with each infection.</p>
<p>An <em>oligomorphic</em> virus, or <em>semi-polymorphic</em> virus, is an encrypted virus which has a small, finite number of different decryptor loops at its disposal. The virus selects a new decryptor loop from this pool for each new infection. For example, Whale had 30 different decryptor variants, and Memorial had 96 decryptors.<sup><a href="#f3_114" name="b3_114">114</a></sup></p>
<p>In terms of detection, oligomorphism only makes a virus marginally harder to spot. Instead of looking for one decryptor loop for the virus, anti-virus software can simply have all of the virus' possible decryptor loops enumerated, and look for them all.</p>
<h4><a name="c325"></a>3.2.5 Polymorphism</h4>
<p>A <em>polymorphic</em> virus is superficially the same as an oligomorphic virus. Both are encrypted viruses, both change their decryptor loop on each infection.<sup><a href="#f3_115" name="b3_115">115</a></sup> However, a polymorphic virus has, for all practical purposes, an infinite number of decryptor loop variations. Tremor, for example, has almost six <em>billion</em> possible decryptor loops!<sup><a href="#f3_116" name="b3_116">116</a></sup> Polymorphic viruses clearly can't be detected by listing all the possible combinations.</p>
<p>There are two questions that arise with respect to polymorphic viruses. First, how can a virus detect that it has previously infected a file, if its presence is hidden sufficiently well? Second, how does the virus change its decryptor loop from infection to infection?</p>
<h5><a name="c3251"></a>3.2.5.1 Self-Detection</h5>
<p>At first glance, it might seem easy for a polymorphic virus to detect if it has previously infected some code - when the virus morphs for a new infection, it can also change whatever aspect of itself that it looks for. This doesn't work, though, because a virus must be able to recognize infection by <em>any</em> of its practically-infinite forms. This means that the infection detection mechanism must be independent of the exact code used by the virus:</p>
 
<div align="center">
<img src="img/mja01/fig36.gif" alt="Figure 3.6. Fun with NTFS alternate data streams"/>
<p><strong>Figure 3.6. Fun with NTFS alternate data streams</strong></p>
</div>
<dl>
<dt><strong>File timestamp.</strong></dt><dd>A virus could change the timestamp of an infected file, so that the sum of its time and date is some constant value <em>K</em> for all infections.<sup><a href="#f3_117" name="b3_117">117</a></sup> A lot of software only displays the last two digits of the year, so an infected file's year could be increased by 100 without attracting attention.<sup><a href="#f3_118" name="b3_118">118</a></sup></dd>
<dt><strong>File size.</strong></dt><dd>An infected file could have its size padded out to some meaningful size, such as a multiple of 1234.<sup><a href="#f3_11" name="b3_11">11</a></sup></dd>
<dt><strong>Data hiding.</strong></dt><dd>In complex executable file formats, like ELF, not all parts of the file's information may be used by a system. A virus can hide aflagin unused areas, or look for an unusual combination of attributes that it has set in the file. For example, Zperm looks for the character "Z" as the minor linker version in an executable's file header on Windows.<sup><a href="#f3_119" name="b3_119">119</a></sup></dd>
<dt><strong>Filesystem features.</strong></dt><dd>Some filesystems allow files to be tagged with arbitrary attributes, whose existence is not always made obvious. These can be used by a virus to store code, data, or flags which indicate that a file has been infected. Figure 3.6 shows such "alternate data streams" being used in an NTFS filesystem to attach a flag to a file; the presence of this flag doesn't show up in directory listings, the file size, or in the graphical filesystem browser.<sup><a href="#f3_12" name="b3_12">12</a></sup></dd>
 
<dt><strong>External storage.</strong></dt><dd>The indication that a file is infected need not be directly associated with the file itself. For example, a virus could use a hash function to map an infected file's name into an obfuscated string, and use that string to create a key in the Windows Registry. The virus could then use the existence of that key as an infection indicator. Even if the Registry key was discovered, it wouldn't immediately reveal the name of the infected file (especially if a strong cryptographic hash function was used).</dd>
</dl>
<p>Note that none of these mechanisms need to work perfectly, because a false positive only means that the virus won't infect some code that it might have otherwise. Also, since all these infection-detection methods work for polymorphic viruses, they also work for the more specific case of non-polymorphic viruses too. Viruses which retain some constancy can just look for one or two bytes of their own code,<sup><a href="#f3_120" name="b3_120">120</a></sup> rather than resorting to more elaborate methods.</p>
<p>It was once suggested that systems could be <em>inoculated</em> against specific viruses by faking the virus' self-detection indicator on an uninfected system.<sup><a href="#f3_121" name="b3_121">121</a></sup> Unfortunately, there are too many viruses now to make this feasible.</p>
<h5><a name="c3252"></a>3.2.5.2 Changing the Decryptor Loop</h5>
<p>The code in a polymorphic virus is transformed for each fresh infection using a <em>mutation engine</em>.<sup><a href="#f3_122" name="b3_122">122</a></sup> The mutation engine has a grab-bag of code transformation tricks which take as input one sequence of code and output another, equivalent, sequence of code. Choosing which technique to apply and where to apply it can be selected by the engine using a pseudo-random number generator.<sup><a href="#f3_123" name="b3_123">123</a></sup> The result is an engine which is extensible and which can permute code in a large number of ways. Some sample transformations are shown below.<sup><a href="#f3_124" name="b3_124">124</a></sup></p>
<dl>
<dt><strong>Instruction equivalence.</strong></dt><dd>Especially on CISC architectures like the Intel x86, there are often many single instructions which have the same effect. All these instructions would set register r1 to zero:
<pre class="source">	
	clear r1
	xor r1,r1
	and 0,r1
	move 0,r1
</pre></dd>
<dt><strong>Instruction sequence equivalence.</strong></dt><dd>Instruction equivalence can be generalized to sequences of instructions. While single-instruction equivalence is at the mercy of the CPU's instruction set, instruction sequence equivalence is more portable, and applies to both high-level and low-level languages:
<pre class="source">
	x = 1    &lt;=>	y = 21
			x = y - 20
</pre></dd>
 
<dt><strong>Instruction reordering.</strong></dt><dd>Instructions may have their order changed, so long as constraints imposed by inter-instruction dependencies are observed.
<pre class="source">
	r1 = 12			r2 = r3 + r2
	r2 = r3 + r2     &lt;=>	r1 = 12
	r4 = r1 + r2		r4 = r1 + r2
</pre>
<p>Here, the calculation of r4 depends on the values of r1 and r2, but the assignments to r1 and r2 are independent of one another and may be done in any order.</p>
<p>Instruction reordering is well-studied, because it is an application of the instruction scheduling done by optimizing compilers to increase instruction-level parallelism.</p></dd>
<dt><strong>Register renaming.</strong></dt><dd>A minor, but significant, change can be introduced simply by changing the registers that instructions use. While this makes no difference from a high-level perspective, such as a human reading the code, renaming changes the bit patterns that encode the instructions; this complicates matters for anti-virus software looking for the virus' instructions. For example:
<pre class="source">	
	r1 = 12			r3 = 12
	r2 = 34		&lt;=>	r1 = 34
	r3 = r1 + r2		r2 = r3 + r1
</pre>
<p>The concept of register renaming naturally extends to variable renaming in higher-level languages, such as those a macro virus might employ.</p></dd>
<dt><strong>Reordering data.</strong></dt><dd>Changing the locations of data in memory will have a similar effect in terms of altering instruction encoding as register renaming. This would not necessarily have a corresponding transformation in a high-level language, as the variable names themselves would not be changed, just their order.</dd>
<dt><strong>Making spaghetti.</strong></dt><dd>Although some programmers are naturally gifted when it comes to producing "spaghetti code," others are not as fortunate. Happily, code can be automatically transformed so that formerly-consecutive instructions are scattered, and linked together by unconditional jumps:
 
<pre class="source">
start:				L1:
	r1 = 12				r2 = 34
	r2 = 34		=>		goto L2
	r3 = r1 + r2		start:
					r1 = 12
					goto L1
				L2:
					r3 = r1 + r2
</pre>
<p>The instructions executed, and their execution order, is the same in both pieces of code.</p></dd>
<dt><strong>Inserting junk code.</strong></dt><dd>"Junk" computations can be inserted which are inert with respect to the original code - in other words, running the junk code doesn't affect what the original code does. Two examples of adding junk code are below:
<pre class="source">
	r1 = 12			r1 = 12			r5 = 42
	inc r1		&lt;=	r2 = 34		=>	r1 = 12
	inc r1			r3 = r1 + r2	     X:
	r1 = r1 - 2					r2 = 34
	r2 = 34						dec r5
	r3 = r1 + r2					bne X
							r3 = r1 + r2
</pre>
<p>The code on the left shows the difference between inserting junk code and using instruction sequence equivalence: with junk code, the original code isn't changed. The one on the right inserts a loop as junk code.</p></dd>
<dt><strong>Run-time code generation.</strong></dt><dd>One way to transform the code is to not have all of it present until it runs. Either fresh code can be generated at run time, or existing code can be modified.
<pre class="source">
r1 = 12			r1 = 12
r2 = 34		=>	r2 = 34
r3 = r1 + r2		generate <em>r3 = r1 + r2</em>
			call generated_code
</pre></dd>
<dt><strong>Interpretive dance.</strong></dt><dd>The way code is executed can be changed, from being directly executed to being interpreted by some application-specific virtual machine.<sup><a href="#f3_125" name="b3_125">125</a></sup> A "classical" interpreter for such virtual machine code mimics the operation of a real CPU as it fetches, decodes, and executes instructions. In the example below, two of the real instructions are assigned different virtual machine opcodes. Another opcode forces the interpreter loop to exit.
 
demonstrating the mixing of interpreted and real code. In the interpreter, the variable <tt>ipc</tt> is the interpreter's program counter, and controls the instruction fetched and executed from the <tt>CODE</tt> array.
<pre class="source">
r1 = 12			ipc = 0
r2 = 34		=>	loop:
r3 = r1 + r2			switch CODE[ipc]:
					case 0:
						exit loop
					case 1:
						r2 = 34
					case 2:
						r1 = 12
				inc ipc
			r3 = rl + r2
			...
			CODE:
				2
				1
				0
</pre>
<p>This transformation can be repeated multiple times, giving multiple levels of interpreters.</p></dd>
<dt><strong>Concurrency.</strong></dt><dd>The original code can be separated into multiple threads of execution, which not only transforms the code, but can greatly complicate automatic analysis:<sup><a href="#f3_13" name="b3_13">13</a></sup>
<pre class="source">
r1 = 12			start thread <em>T</em>
r2 = 34		=>	r1 = 12
r3 = rl + r2		wait for signal
			r3 = r1 + r2
				...
			T:
				r2 = 34
				send signal
				exit thread <em>T</em>
</pre></dd>
<dt><strong>Inlining and outlining.</strong></dt><dd>Code <em>inlining</em> is a technique normally employed to avoid subroutine call overhead,<sup><a href="#f3_14" name="b3_14">14</a></sup> that replaces a subroutine call with the subroutine's code:
 
<pre class="source">
	...			...
	call S1			r1 = 12
	call S2			r2 = r3 + r2
	...		=>	r4 = r1 + r2
S1:
	r1 = 12			r1 = 12
	r2 = r3 + r2		r2 = 34
	r4 = r1 + r2		r3 = r1 + r2
	return			...
S2:
	r1 = 12
	r2 = 34
	r3 = r1 + r2
	return
</pre>
<p>Outlining is the reverse operation; it need not preserve any logical code grouping, however:</p>
<pre class="source">
	...			...
	r1 = 12			r1 = 12
	r2 = r3 + r2		r2 = r3 + r2
	r4 = r1 + r2		call S12
			=>	r3 = r1 + r2
	r1 = 12			...
	r2 = 34			S12:
	r3 = r1 + r2			r4 = r1 + r2
					r1 = 12
					r2 = 34
					return
</pre>
<p>Another option is to convert the code into <em>threaded code</em>, which has nothing to do with threads used for concurrent programming, despite the name. Threaded code is normally used as an alternative way to implement programming language interpreters.<sup><a href="#f3_126" name="b3_126">126</a></sup> Subroutines in threaded code don't return to the place from which they were invoked, but instead directlyjump to the next subroutine; the threaded code itself is simply an array of code addresses:</p>
 
<pre class="source">
	...			...
	r1 = 12			next = &amp;CODE
	r2 = r3 + r2		goto [next]
	r4 = r1 + r2		CODE:
			=>		&amp;I1
	r1 = 12				&amp;I2
	r2 = 34				&amp;X
	r3 = r1 + r2		X:
	...				r1 = 12
					r2 = 34
					r3 = r1 + r2
					...
				I1:
					r1 = 12
					inc next
					goto [next]
				I2:
					r2 = r3 + r2
					r4 = r1 + r2
					inc next
					goto [next]
</pre></dd>
<dt><strong>Subroutine interleaving.</strong></dt><dd>Inlining and outlining transformations maintain the original code, but rebundle it in different ways. Code can also be transformed by combining independent subroutines together, as in the following example.
<pre class="source">
	...			...
	call S1			call S12
	call S2			...
	...		=>	S12:
S1:					r5 = 12
	r1 = 12				<em>r1 = 12</em>
	r2 = r3 + r2			r6 = r3 + r2
	r4 = r1 + r2			<em>r2 = 34</em>
	return				r4 = r5 + r6
S2:					<em>r3 = r1 + r2</em>
	<em>r1 = 12</em>		return
	<em>r2 = 34</em>
	<em>r3 = r1 + r2</em>
	return
</pre>
 
<p>The code from S1 has had some registers renamed to avoid collisions with registers used by S2. The overall effect in the interleaved subroutine is the same as the original code in terms of the values computed.</p></dd>
</dl>
<p>A number of these transformations are also used in the (legitimate) field of code obfuscation; code obfuscation research is used to try and prevent reverse engineering. There are also many, many elaborate code transformations performed by optimizing compilers. Not all compiler techniques and code obfuscation techniques have yet been used by virus writers.</p>
<p>Instead of supplying transformations for the mutation engine to pick from, a virus writer may create a mutation engine that will <em>automatically</em> produce a distinct, equivalent decryptor loop. In compilers, automatically searching for a code sequence is referred to as <em>superoptimization</em>, and the search may be implemented in a variety of ways: brute-force, automated theorem proving, or any technique for searching a large search space.<sup><a href="#f3_127" name="b3_127">127</a></sup> Zellome, for example, uses a genetic algorithm in its mutation engine.<sup><a href="#f3_128" name="b3_128">128</a></sup> Enormous computational demands are required by such a search, although a clever algorithm may avoid generating too much illegal code and thus improve search time.<sup><a href="#f3_15" name="b3_15">15</a></sup> For now, this mutation method is a curiosity only.</p>
<h4><a name="c326"></a>3.2.6 Metamorphism</h4>
<div class="epigraph">
'Viruses aim to keep their size as small as possible and it is impractical to make the main virus body polymorphic'
<p>- Tarkan Yetiser<sup><a href="#f3_129" name="b3_129">129</a></sup></p>
</div>
<br clear="all"/>
<p><em>Metamorphic</em> viruses are viruses that are polymorphic in the virus body.<sup><a href="#f3_130" name="b3_130">130</a></sup> They aren't encrypted, and thus need no decryptor loop, but avoid detection by changing: a new version of the virus body is produced for each new infection.</p>
<p>The code-modifying techniques used by polymorphic viruses all apply to metamorphic viruses. Both employ a mutation engine, except a polymorphic virus need not change its engine on each infection, because it can reside in the encrypted part of the virus. In contrast, a metamorphic virus' mutation engine has to morph itself anew for each infection.</p>
<p>Some metamorphic viruses are very elaborate. Simile's mutation engine, about 12,000 lines of assembly code, translates Simile from machine code to a machine-independent intermediate code. Operating on the intermediate code, the mutation engine undoes old obfuscations, applies new transformations, and generates fresh machine code.<sup><a href="#f3_131" name="b3_131">131</a></sup> Metamorphic mutation engines whose input and output are machine code must be able to disassemble and reassemble machine code.<sup><a href="#f3_16" name="b3_16">16</a></sup></p>
<p>Metamorphism is relatively straightforward to implement in viruses that spread in source code form, such as macro viruses. A virus may rely on system tools for metamorphism, too. Apparition, for instance, is written in Pascal<sup><a href="#f3_17" name="b3_17">17</a></sup>
 
and carries its own source code; if a compiler is found on an infected system, the virus inserts junk code into its source and recompiles itself.</p>
<p>While polymorphic and metamorphic viruses are decidedly nontrivial to detect by anti-virus software, they are also hard for a virus writer to implement correctly - the numbers of these viruses are small in comparison to other types.</p>
<h4><a name="c327"></a>3.2.7 Strong Encryption</h4>
<p>The encryption methods discussed so far result in viruses that, once captured, are susceptible to analysis. The major problem is not the encryption method, because that can always be strengthened; the major problem is that viruses carry their decryption keys with them.<sup><a href="#f3_132" name="b3_132">132</a></sup></p>
<p>This might seem a necessary weakness, because if a virus doesn't have its key, it can't decrypt and run its code. There are, however, two other possibilities.</p>
<ol>
<li>The key comes from outside an infected system:
<ul>
<li>A virus can retrieve the key from a web site, but that would mean that the virus would then have to carry the web site's address with it, which could be blocked as a countermeasure. To avoid knowing a specific web site's name, a virus could use a web search engine to get the key instead.
<p>Generally, any electronic data stream that a virus can monitor would be usable for key delivery, especially ones with high volumes of traffic that are unlikely to be blocked: email messages, Usenet postings, instant messaging, IRC, file-sharing networks.</p></li>
<li>A <em>binary virus</em> is one where the virus is in two parts, and doesn't become virulent until both pieces are present on a system.<sup><a href="#f3_133" name="b3_133">133</a></sup> There have only been a few binary viruses, such as Dichotomy and RMNS.<sup><a href="#f3_18" name="b3_18">18</a></sup>
<p>One manifestation ofbinary viruses would be where virus <img src="/img/cache/47e205a9f01f6951d4dc6de16c404a8d.gif" alt="V_1" valign="middle"/> has strongly-encrypted code, and virus <img src="/img/cache/81ed5ef3779e6b081b22740d7399b22f.gif" alt="V_2" valign="middle"/> has its key. But this scheme is unlikely to work well in practice. If <img src="/img/cache/47e205a9f01f6951d4dc6de16c404a8d.gif" alt="V_1" valign="middle"/> and <img src="/img/cache/81ed5ef3779e6b081b22740d7399b22f.gif" alt="V_2" valign="middle"/> travel together, then both will bear the same risk of capture and analysis, defeating the purpose of separating the encryption key. If <img src="/img/cache/47e205a9f01f6951d4dc6de16c404a8d.gif" alt="V_1" valign="middle"/> and <img src="/img/cache/81ed5ef3779e6b081b22740d7399b22f.gif" alt="V_2" valign="middle"/> spread separately (e.g., <img src="/img/cache/81ed5ef3779e6b081b22740d7399b22f.gif" alt="V_2" valign="middle"/> is released a month after <img src="/img/cache/47e205a9f01f6951d4dc6de16c404a8d.gif" alt="V_1" valign="middle"/>, and uses a different infection vector) then their spread would be independent.</p>
<p>Now, say that <img src="/img/cache/20868fa29dfc38ac154b8ef762766b41.gif" alt="P_1" valign="middle"/> is the probability of <img src="/img/cache/47e205a9f01f6951d4dc6de16c404a8d.gif" alt="V_1" valign="middle"/> reaching a given machine, and <img src="/img/cache/2d70da379b3ffb56bd104b348ba21c55.gif" alt="P_2" valign="middle"/> is that probability for <img src="/img/cache/81ed5ef3779e6b081b22740d7399b22f.gif" alt="V_2" valign="middle"/>. With an independent spread, the probability of them bothfindingthe same machine and becoming virulent is <img src="/img/cache/40ff00e1e62c8b502db3e09f60462762.gif" alt="P_1 \times P_2" valign="middle"/>, i.e., smaller.<sup><a href="#f3_19" name="b3_19">19</a></sup></p></li>
</ul></li>
<li>The key comes from <em>inside</em> an infected system. Using <em>environmental key generation</em>, the decryption key is constructed of elements already present in the target's environment, like:
 
<ul>
<li>the machine's domain name;</li>
<li>the time or date;</li>
<li>some data in the system (e.g., file contents);</li>
<li>the current user name;</li>
<li>the interface's language setting (e.g., Chinese, Hebrew).</li>
</ul>
<p>This makes it very easy to target viruses to particular individuals or groups. A target doesn't even know that they possess the key!</p>
<p>Combined with strong encryption, environmental key generation would render a virus unanalyzable even if captured. To fully analyze an encrypted virus, it has to be decrypted, and while the elements comprising the key may be discovered, the exact value of the key will not.<sup><a href="#f3_20" name="b3_20">20</a></sup> In this case, the only real hope of decryption lies in a poor choice of key. A poorly-chosen key with a relatively small range of possible values (e.g., the language setting) would be susceptible to a brute-force attack.</p>
<p>How can the virus know that its decryption was successful? It doesn't. While the virus could carry a checksum with it to verify that the decryption worked,<sup><a href="#f3_21" name="b3_21">21</a></sup> that might give away information to an analyst. An alternative method is to catch exceptions that invalid code may cause, then try to run the decrypted "code" and see if it works.</p></li>
</ol>
<h3><a name="c33"></a>3.3 Virus Kits</h3>
<p>Humans love their tools, and it's not surprising that a variety of tools exists for writing viruses. A <em>virus kit</em> is a program which automatically produces all or part of a virus' code.<sup><a href="#f3_134" name="b3_134">134</a></sup> They have different interfaces, from command-line tools to menu-based tools to full-blown graphical user interfaces. Figures 3.7 and 3.8 show two versions of a GUI-based virus kit.<sup><a href="#f3_22" name="b3_22">22</a></sup></p>
<p>Programming libraries are available, too, such as add-on mutation engines which will turn any virus into a polymorphic virus. In an Orwellian twist, though, success is failure. The more popular a virus kit or library, the greater the chance that anti-virus researchers have devoted time to detecting all of its progeny.</p>
 
<div align="center">
<img src="img/mja01/fig37.gif" alt="Figure 3.7. Virus kit"/>
<p><strong>Figure 3.7. Virus kit</strong></p>
</div>
<div align="center">
<img src="img/mja01/fig38.gif" alt="Figure 3.8. Virus kit, the next generation"/>
<p><strong>Figure 3.8. Virus kit, the next generation</strong></p>
</div>
 
<p><strong>Notes for Chapter 3</strong></p>
<p><a name="f3_1" href="#b3_1">1</a> Even though there may be several initial blocks/sectors involved, I'll refer to this as the boot block (singular) for convenience.</p>
<p><a name="f3_2" href="#b3_2">2</a> Disks are used for concreteness, but really this could be any bootable media.</p>
<p><a name="f3_3" href="#b3_3">3</a> Although media is used which can potentially be bootable, like CD-ROMs, they are not often booted from.</p>
<p><a name="f3_4" href="#b3_4">4</a> "BIOS" stands for "Basic Input/Output System;" this refers to in-ROM code on PCs.</p>
<p><a name="f3_5" href="#b3_5">5</a> This section was originally based on Harley et al. [ 137]. Some sources would classify viruses using some of these techniques as "cavity infectors" [77], but as cavity infection involves overwriting, this distinction seems unnecessary.</p>
<p><a name="f3_6" href="#b3_6">6</a> The ZeroHunt virus looked for sequences of bytes with the value 0, for instance [198].</p>
<p><a name="f3_7" href="#b3_7">7</a> This technique was employed for viruses back in 1987 [95], and is still in use [27, 58].</p>
<p><a name="f3_8" href="#b3_8">8</a> Having said that, Zmist does it [106].</p>
<p><a name="f3_9" href="#b3_9">9</a> Ironically, it was shipped out by Microsoft on some CD-ROMs [17]. The Concept source code is still easily obtainable, and an analysis can be found in many sources [122, 137, 187].</p>
<p><a name="f3_10" href="#b3_10">10</a> For the pedantic, there's an implied key of 1 for these operations.</p>
<p><a name="f3_11" href="#b3_11">11</a> Executable files infected by the CTX virus, for example, will have their size adjusted to be a multiple of 101 bytes [195].</p>
<p><a name="f3_12" href="#b3_12">12</a> The Stream virus uses NTFS' alternate data streams, but not to detect infection. Stream is an overwriting virus that saves the original code as a separate data stream called "STR" that is associated with the infected file [313].</p>
<p><a name="f3_13" href="#b3_13">13</a> This example is only for illustration; threads do not typically share register contents.</p>
<p><a name="f3_14" href="#b3_14">14</a> The term "subroutine" will be used generically to describe either a procedure, function, or method.</p>
<p><a name="f3_15" href="#b3_15">15</a> Joshi et al. [155] note their speedup compared to a brute-force algorithm. Agapow [4] examines clustering of functional code in the space of all possible programs, arguing that mutation from one piece of functional code to another is possible.</p>
<p><a name="f3_16" href="#b3_16">16</a> The Mental Driller [320]; Lakhotia et al. [178] also discuss the mutation engine, and argue that metamorphic viruses are ultimately constrained in their complexity, because of their need to disassemble and de-obfuscate their own code.</p>
<p><a name="f3_17" href="#b3_17">17</a> Borland's Object Pascal for Windows, to be precise [162].</p>
<p><a name="f3_18" href="#b3_18">18</a> Kaspersky [159, 160]. Interestingly, the 1961 Darwin players tried an experiment with such multi-part programs, and declared the experiment a 'flop' [201].</p>
 
<p><a name="f3_19" href="#b3_19">19</a> Recall that probabilities fall in the range [0,1], so their product can't be greater than either one.</p>
<p><a name="f3_20" href="#b3_20">20</a> Even if the exact key isn't discovered, general information about the virus' intent may be revealed by the elements used for the key.</p>
<p><a name="f3_21" href="#b3_21">21</a> Bontchev[46]. The <em>random decryption algorithm</em> (RDA) works along those lines: the virus doesn't carry its key, but doesn't get its key from the environment, either. An RDA virus decrypts itself by brute force, trying different decryption keys until it locates a known value in the decrypted code [208].</p>
<p><a name="f3_22" href="#b3_22">22</a> Okay, it depends on how "virus" is defined - this is really a worm generator, but it has one of the best GUIs. These are both by [K]alamar.</p>
<p><a name="f3_100" href="#b3_100">100</a> These parts are from Harley et al. [137]. The phrase "infection mechanism" is also used extensively in biology.</p>
<p><a name="f3_101" href="#b3_101">101</a> As reported in [14].</p>
<p><a name="f3_102" href="#b3_102">102</a> Levine[183].</p>
<p><a name="f3_103" href="#b3_103">103</a> The first is from Bontchev [38]; <em>everyone</em> mentions the second [38, 137, 187]; the third and final ones are from Harley et al. [137]. The fourth is mentioned in [77].</p>
<p><a name="f3_104" href="#b3_104">104</a> Levine [183].</p>
<p><a name="f3_105" href="#b3_105">105</a> Highland [141].</p>
<p><a name="f3_106" href="#b3_106">106</a> The first three are from [13], the fourth from [248].</p>
<p><a name="f3_107" href="#b3_107">107</a> As pointed out by one of my students.</p>
<p><a name="f3_108" href="#b3_108">108</a> Wells [13].</p>
<p><a name="f3_109" href="#b3_109">109</a> The first two are from Harley et al. [137].</p>
<p><a name="f3_110" href="#b3_110">110</a> Bontchev [38].</p>
<p><a name="f3_111" href="#b3_111">111</a> Bontchev [46].</p>
<p><a name="f3_112" href="#b3_112">112</a> Hoglund and Butler [144].</p>
<p><a name="f3_113" href="#b3_113">113</a> Florio [112] analyzes Ryknos; the infamous rootkit in question was outed by Russinovich [271].</p>
<p><a name="f3_114" href="#b3_114">114</a> [161] and [309], respectively.</p>
<p><a name="f3_115" href="#b3_115">115</a> Definition based on [217, 351].</p>
<p><a name="f3_116" href="#b3_116">116</a> Fischer [108].</p>
<p><a name="f3_117" href="#b3_117">117</a> Ludwig[187].</p>
<p><a name="f3_118" href="#b3_118">118</a> Ludwig again, and Ferbrache [103].</p>
<p><a name="f3_119" href="#b3_119">119</a> Szor[311].</p>
<p><a name="f3_120" href="#b3_120">120</a> Ferbrache [103].</p>
<p><a name="f3_121" href="#b3_121">121</a> Ferbrache [103].</p>
 
<p><a name="f3_122" href="#b3_122">122</a> Nachenberg [217].</p>
<p><a name="f3_123" href="#b3_123">123</a> Yetiser[351].</p>
<p><a name="f3_124" href="#b3_124">124</a> These are from Cohen [75] (upon whom this organization was originally based) and Collberg et al. [76]; additional sources are noted below.</p>
<p><a name="f3_125" href="#b3_125">125</a> Klint [166].</p>
<p><a name="f3_126" href="#b3_126">126</a> Bell [32]. There are other variations, like indirect threaded code [90].</p>
<p><a name="f3_127" href="#b3_127">127</a> The seminal superoptimization paper was Massalin [196], who used a brute-force search; Joshi et al. [155] use automated theorem proving, and Michalewicz and Fogel [206] cover a wide variety of heuristic search methods.</p>
<p><a name="f3_128" href="#b3_128">128</a> Ferric and Shannon [105].</p>
<p><a name="f3_129" href="#b3_129">129</a> Yetiser[351].</p>
<p><a name="f3_130" href="#b3_130">130</a> This section is based on Szor and Ferric [314].</p>
<p><a name="f3_131" href="#b3_131">131</a> Perriotetal. [249].</p>
<p><a name="f3_132" href="#b3_132">132</a> Unless stated otherwise, this section is based on Filiol [107] and Riordan and Schneier [265].</p>
<p><a name="f3_133" href="#b3_133">133</a> Skulason [291] first described the idea, for the more general case of a multi-part virus; the term "binary virus" is from Bontchev [46].</p>
<p><a name="f3_134" href="#b3_134">134</a> This section is based on Tarala [316].</p>
 
<h2><a name="c4"></a>Chapter 4 Anti-virus techniques</h2>
<div class="epigraph">
'... it is trivial to write a program that identifies all infected programs with 100% accuracy.'
<p>- Eugene Spafford<sup><a href="#f4_1" name="b4_1">1</a></sup></p>
</div>
<br clear="all"/>
<p>Anti-virus software does up to three major tasks:<sup><a href="#f4_100" name="b4_100">100</a></sup></p>
<dl>
<dt><strong>Detection</strong></dt><dd>Detecting whether or not some code is a virus or not which, in the purest form of detection, results in a Boolean value: yes, this code is infected, or no, this code is not infected. Ultimately, detection is a losing game. Precisely detecting viruses by their appearance or behavior is provably undecidable<sup><a href="#f4_101" name="b4_101">101</a></sup> - a virus writer can always construct a virus which is undetectable by some anti-virus software. (Then the anti-virus software can be updated to detect the new virus, at which point the virus writer can build another new virus, and so on.)
<p>Should a virus always be detected, even if it can't run? Yes. Even if a virus is dormant on one system, it is still useful to detect it so that the virus doesn't affect another system. Anti-virus software is regularly applied to incoming email, for instance, where the email recipient's machine is different from the machine running the mail server and anti-virus software. The other case is where a virus won't run on <em>any</em> system. Finding an intended virus may point to some underlying security flaw, and thus it can be useful to detect those viruses too.</p></dd>
<dt><strong>Identification</strong></dt><dd>Once a virus is detected, which virus is it? The identification process may be distinct from detection, or identification may occur as a side effect of the detection method being used.</dd>
<dt><strong>Disinfection</strong></dt><dd>Disinfection is the process of removing detected viruses; this is sometimes called <em>cleaning</em>. Normally a virus would need to be precisely identified in order to perform disinfection.</dd>
</dl>
 
<div align="center">
<img src="img/mja01/fig41.gif" alt="Figure 4.1. Virus detection outcomes"/>
<p><strong>Figure 4.1. Virus detection outcomes</strong></p>
</div>
<p>Detection and disinfection can be performed using <em>generic</em> methods that try to work with known and unknown viruses, or using <em>virus-specific</em> methods which only work with known viruses. (Virus-specific methods may catch unknown variants of known viruses, however.)</p>
<p>The majority of this chapter is devoted to detection. It is arguably the most important of the three tasks above, because identification and disinfection both require detection as a prerequisite. In addition, early detection (i.e., before an infection has occurred) completely alleviates the need for the other tasks.</p>
<p>There are five possible outcomes for detection. Figure 4.1 shows four of them. Perfect virus detection would always have the outcomes circled on the diagonal, where a virus is detected if one is really present, and no virus is detected if none is there. Detection isn't perfect, though. A <em>false positive</em> is when the anti-virus software reports a virus even though a virus isn't really there, which can waste time and resources on wild goose chases. A <em>false negative</em>, or a <em>miss</em>, is when anti-virus software doesn't detect a virus that's present. Either type of false reading serves to undermine user confidence in the anti-virus software. The fifth outcome is <em>ghost positives</em>, where a virus is detected that is no longer there, but a previous attempt at disinfection was incomplete and left enough virus remnants to still be detected.<sup><a href="#f4_102" name="b4_102">102</a></sup></p>
<p>Detection methods can be classified as static or dynamic, depending on whether or not the virus' code is running when the detection occurs. This chapter first looks at detection methods using this classification, then disinfection and related issues, virus databases and virus description languages, and some miscellaneous short topics.</p>
 
<h3><a name="c41"></a>4.1 Detection: Static Methods</h3>
<p>Static anti-virus techniques attempt virus detection without actually running any code. This section examines three static techniques: scanners, heuristics, and integrity checkers.</p>
<h4><a name="c411"></a>4.1.1 Scanners</h4>
<p>The term "scanner" in the context of anti-virus software is another term which has been diluted through common usage, like "virus" itself. It is often applied generically to refer to anti-virus software, regardless of what technique the anti-virus software is using.</p>
<p>Scanners can be classified based on when they are invoked:<sup><a href="#f4_103" name="b4_103">103</a></sup></p>
<dl>
<dt><strong>On-demand</strong></dt><dd>On-demand scanners run when explicitly started by the user. Many anti-virus techniques draw upon a database of information about current threats, and forcing an on-demand scan is useful when a new virus database is installed. An on-demand scan may also be desirable when an infection is suspected, or when a questionable file is downloaded.</dd>
<dt><strong>On-access</strong></dt><dd>An on-access scanner runs continuously, scanning every file when it's accessed. As might be expected, the extra I/O overhead and resources consumed by the scanner impose a performance penalty.
<p>Some on-access scanners permit tuning, so that scans are only performed for read accesses or write accesses; normally scanning would be done for both. A machine where all files arrive via the network may only want scanning on write accesses, for example, because that would provide complete anti-virus coverage while minimizing the performance hit.<sup><a href="#f4_2" name="b4_2">2</a></sup></p></dd>
</dl>
<p>In this section, a more restricted view is taken of scanners. Each virus is represented by one or more patterns, or <em>signatures</em>, sequences of bytes which (hopefully) uniquely characterize the virus. Signatures are sometimes called scan strings, and need not be constant strings. Some anti-virus software may support "don't care" symbols called <em>wildcards</em> that match an arbitrary byte, a part of a byte, or zero or more bytes.<sup><a href="#f4_104" name="b4_104">104</a></sup></p>
<p>The process of searching for viruses by looking through a file for signatures is called <em>scanning</em>, and the code that does the search is called a <em>scanner</em>. More generally, the search is done through a stream of bytes, which would include the contents of a boot block, a whole file, part of a file being written or read, or network packets.</p>
<p>With hundreds of thousands of signatures to look for, searching for them one at a time is infeasible. The biggest technical challenge in scanning is finding algorithms which are able to look for multiple patterns efficiently, and which scale well. The next sections examine three such algorithms, which illustrate
 
the general principles behind multiple-pattern search, and which have been used in both anti-virus software and the intrusion-detection systems of Chapter 8.</p>
<div align="center">
<img src="img/mja01/fig42.gif" alt="Figure 4.2. Aho-Corasick finite automaton and failure function"/>
<p><strong>Figure 4.2. Aho-Corasick finite automaton and failure function</strong></p>
</div>
<h5><a name="c4111"></a>4.1.1.1 Algorithm: Aho-Corasick</h5>
<p>The Aho-Corasick algorithm dates back to 1975 and was originally intended for bibliographic search.<sup><a href="#f4_105" name="b4_105">105</a></sup> The algorithm is best illustrated with an example. A scanner would be looking for signatures which could be composed of any byte values, but for simplicity, English words will be used in the example instead of signatures: hi, hips, hip, hit, chip.</p>
<p>Aho-Corasick needs two things for searching, both of which are shown in Figure 4.2:</p>
<ol>
<li>A <em>finite automaton</em> is used to keep track of the state of the search. Conceptually, this is represented as a graph, where the circles represent search states and the edges are possible transitions that can be made from one state to another; the label on an edge indicates the character that causes that transition to be made. (The "other" label is a catch-all which matches any character for which there is no explicit transition.) A doubly-circled state is a final state, where output (i.e., a signature match) occurs, and the associated output is printed above its final state. The start state is denoted by an edge which doesn't originate at a state. The states are numbered for reference purposes.</li>
<li>A <em>failure function</em> tells the search algorithm which state to go to if no suitable transition is found in the finite automaton. Intuitively, this is the earliest place that the search can possibly resume matching.</li>
</ol>
 
<div align="center">
<img src="img/mja01/fig43.gif" alt="Figure 4.3. Aho-Corasick in operation"/>
<p><strong>Figure 4.3. Aho-Corasick in operation</strong></p>
</div>
<p>The computation of the finite automaton and failure function will be shown later, but for now, here is the search code that uses them:</p>
<pre class="source">
	state = START_STATE
	while not end of input:
	    ch = next input character
	    while no edge state <img src="/img/cache/f10f77efadde9acec4fa206323cbb95e.gif" alt="\overset{ch}{\rightarrow}t" valign="middle"/> exists:
                state = failure(state)
            state = <em>t</em>
            if state is final:
                output matches
</pre>
<p>(The notation state<img src="/img/cache/f10f77efadde9acec4fa206323cbb95e.gif" alt="\overset{ch}{\rightarrow}t" valign="middle"/> means an edge labeled ch from state <tt>state</tt> to some state <em>t</em>.)</p>
<p>Figure 4.3 gives the result of running the search code on the input string "microchips," showing the finite automaton's state numbers underneath. From the start state 0, the first two input characters just cause a transition back to state 0. The third character, c, causes a transition into state 2, but there is no transition from state 2 for the following r, so the failure function is used to locate a state from which to resume the search: state 0 again. Skipping ahead, the transition from state 4 on i leads to state 7, a final state where the signature "hi" is matched. Two signatures are matched next, in state 9. There are no transitions at all from state 9, so the failure function is used again, causing the search to resume at state 5, where there is a transition on s to final state 8. The Aho-Corasick algorithm thus searches in parallel for multiple signatures, even detecting overlapping ones.</p>
<p>How are the finite automaton and failure function constructed? There are three steps:</p>
<ol>
<li>Build a trie from the signatures.<sup><a href="#f4_3" name="b4_3">3</a></sup> A <em>trie</em> is a tree structure used for searching, where the tree's edges are labeled. A signature has a unique path in the trie from the root to some leaf; signatures with common prefixes share trie paths as long as possible, then diverge.
 
<div align="center">
<img src="img/mja01/fig44.gif" alt="Figure 4.4. Trie building"/>
<p><strong>Figure 4.4. Trie building</strong></p>
</div>
<p>Figure 4.4 shows the trie being built incrementally for the running example. The trie's root is the start state of the finite automaton, and a self-edge is added to it. A signature is added by starting at the root, tracing along existing paths until a necessary edge is absent, then adding the remaining edges and states. The end of a path becomes a final state.</p></li>
<li>Label the states in the trie. The trie states are assigned numbers such that states closer to the root have lower numbers. This corresponds to a breadth-first ordering of the states. (If the trie states are laid out as in previous figures, then numbering is a simple matter of stepping through the columns of states.) The breadth-first ordering and labels appear in Figure 4.5.</li>
<li>Compute the failure function and finish the automaton. The failure function is undefined for the start state, but must be computed for all other states.
 
<div align="center">
<img src="img/mja01/fig45.gif" alt="Figure 4.5. Trie labeling"/>
<p><strong>Figure 4.5. Trie labeling</strong></p>
</div>
<p>Any state directly connected to the start state (in other words, at a depth of 1 in the trie) can only resume searching at the start state. For other states, the partially-computed failure function is used to trace back through the automaton to find the earliest place the search can resume. Processing states in breadth-first order ensures that needed failure function values are always present.</p>
<p>The computation algorithm is below. Notice that it not onlyfillsin the failure function, but also updates the finite automaton. (The notation <img src="/img/cache/1841d20db48b682aecf52641dd3e8ba4.gif" alt="r\overset{a}{\rightarrow}s" valign="middle"/> means an edge from some state <em>r</em> with some label <em>a</em> to state s, and state<img src="/img/cache/63b47ab1bfe92af7a385adc65afd6940.gif" alt="\overset{a}{\rightarrow}t" valign="middle"/> is an edge labeled <em>a</em> from state state to some state <em>t</em>.)</p>
<pre class="source">
	foreach state s where depth(s) = 1:
	    failure(s) = START_STATE

	foreach state s where depth(s) > 1, in breadth order:
	find the edge <img src="/img/cache/1841d20db48b682aecf52641dd3e8ba4.gif" alt="r\overset{a}{\rightarrow}s" valign="middle"/>
	state = failure(<em>r</em>)
	while no edge state<img src="/img/cache/63b47ab1bfe92af7a385adc65afd6940.gif" alt="\overset{a}{\rightarrow}t" valign="middle"/> exists:
	    state = failure(state)
	failure(s) = <em>t</em>
	output(s) <img src="/img/cache/1a4d3aa5781ebd50a8104d20b287ac85.gif" alt="\cup" valign="middle"/>= output(<em>t</em>)
</pre>
<p>Returning to the example, the algorithm starts by initializing <em>failure(1) = 0</em> and <em>failure(2) = 0</em>. Then, tracing through the rest of the algorithm:</p>
 
<table summary="">
<tr><th>s</th><th><img src="/img/cache/1841d20db48b682aecf52641dd3e8ba4.gif" alt="r\overset{a}{\rightarrow}s" valign="middle"/></th><th><img src="/img/cache/807b1a568e1f48cb7c7419bfc566dec9.gif" alt="state\overset{a}{\rightarrow}t" valign="middle"/></th><th><em>failure(s)</em></th></tr>
<tr><td>3</td><td><img src="/img/cache/7cf577fd14e5f980f55a69a027d63c5f.gif" alt="1\overset{i}{\rightarrow}3" valign="middle"/></td><td><img src="/img/cache/3d6bff198bc4869e92a7af128f03e96c.gif" alt="0\overset{i}{\rightarrow}0" valign="middle"/></td><td>0</td></tr>
<tr><td>4</td><td><img src="/img/cache/1b4e08d8cf8a58a1c0b2d60f3505015a.gif" alt="2\overset{h}{\rightarrow}4" valign="middle"/></td><td><img src="/img/cache/7a43a5992edbb17329c70a70c841c44b.gif" alt="0\overset{h}{\rightarrow}0" valign="middle"/></td><td>1</td></tr>
<tr><td>5</td><td><img src="/img/cache/575cc44e0194ae7f1c8eb2444cf2d3dd.gif" alt="3\overset{p}{\rightarrow}5" valign="middle"/></td><td><img src="/img/cache/3e5e93f21a89655839d2dd1631d6c3dd.gif" alt="0\overset{p}{\rightarrow}0" valign="middle"/></td><td>0</td></tr>
<tr><td>6</td><td><img src="/img/cache/51ea4f268eaadb8f0fd135ea22da0d1f.gif" alt="3\overset{t}{\rightarrow}6" valign="middle"/></td><td><img src="/img/cache/186a2f878f8bc8fb79df317ffded0f9b.gif" alt="0\overset{t}{\rightarrow}0" valign="middle"/></td><td>0</td></tr>
<tr><td>7</td><td><img src="/img/cache/2ecd2dd582524ee7e3c56533c33cacfa.gif" alt="4\overset{i}{\rightarrow}7" valign="middle"/></td><td><img src="/img/cache/62c93c48a4601fe55b5eb4db39e1d8e1.gif" alt="1\overset{i}{\rightarrow}0" valign="middle"/></td><td>3</td></tr>
<tr><td>8</td><td><img src="/img/cache/47c83b7e45327bfc6235bf9ad9f61c3a.gif" alt="5\overset{s}{\rightarrow}8" valign="middle"/></td><td><img src="/img/cache/365b5b9031e42504ab9cd8b7d99073ac.gif" alt="0\overset{s}{\rightarrow}0" valign="middle"/></td><td>0</td></tr>
<tr><td>9</td><td><img src="/img/cache/04bab7986da49988087954986d3d1c4d.gif" alt="7\overset{p}{\rightarrow}9" valign="middle"/></td><td><img src="/img/cache/58cdc9a81929721aee981ebd09ac8f3e.gif" alt="3\overset{p}{\rightarrow}0" valign="middle"/></td><td>5</td></tr>
</table>
<p>Computing state 7's failure function value causes its output to change in the finite automaton, and makes it a final state. State 9's output is changed too. The final result is identical to Figure 4.2.</p></li>
</ol>
<p>An alternative form of Aho-Corasick combines the finite automaton with the failure function. The result is a new finite automaton for searching that only makes one transition for every input character read, ensuring linear worst-case performance. In practice, Aho-Corasick implementations must solve the challenging problem of how best to represent the finite automaton in a time- and space-efficient manner.<sup><a href="#f4_106" name="b4_106">106</a></sup></p>
<h5><a name="c4112"></a>4.1.1.2 Algorithm: Veldman</h5>
<p>The Aho-Corasick algorithm is not the only way to search for signatures. One insight leads to a new family of search algorithms: it may be good enough to perform a linear search on a reduced set of signatures. The search doesn't have to be done in parallel.</p>
<p>This insight underlies Veldman's signature search algorithm.<sup><a href="#f4_107" name="b4_107">107</a></sup> The set of signatures being looked for at any one time is filtered down to a manageable level, then a sequential search is done. The key is limiting the sequential search as much as possible.</p>
<p>Four adjacent, non-wildcard bytes are chosen from each signature. These four-byte pattern substrings are then used to construct two hash tables which are used for filtering during the search. Ideally, each pattern substring is chosen so that many signatures are represented by the substring. For example. Figure 4.6 shows that three pattern substrings are sufficient to express five signatures: blar?g, foo, greep, green, agreed. Two-byte pattern substrings are supported as a special case for signatures which are short or contain frequent wildcards, and the substrings don't have to be selected from the beginning of a signature.</p>
<p>After the pattern substrings are chosen, the hash tables are built. The first hash table is used for the first two bytes of a substring, the second hash table
 
for the last two bytes of a substring, if present. At search time, the hash tables are indexed by adjacent pairs of input bytes. A single bit in the hash table entry indicates whether or not the pair of input bytes might be part of a pattern substring (and possibly part of a signature). A signature table is constructed along with the hash tables, too - this is an array of lists, where each list contains all the signatures that might match a pattern substring. Thefinalhash table entry for a pattern substring is set to point to the appropriate signature list. Figure 4.7 illustrates the hash tables and signature table for the example above.</p>
<div align="center">
<img src="img/mja01/fig46.gif" alt="Figure 4.6. Pattern substring selection for Veldman's algorithm"/>
<p><strong>Figure 4.6. Pattern substring selection for Veldman's algorithm</strong></p>
</div>
<p>The search algorithm is given below. The <tt>match</tt> subroutine walks through a list of signatures and attempts to match each signature against the input. Matching also compensates for the inexact filtering done by the hash tables: for example, a byte sequence like "grar" or "blee" would pass through the hash tables, but would be winnowed out by <tt>match</tt>.</p>
<pre class="source">
	foreach byte sequence <img src="/img/cache/0c7774a6133a76a9256660392e1b061e.gif" alt="b_1b_2b_3b_4" valign="middle"/> in input:
	    if HT1[<img src="/img/cache/341d750b78dd108b12e3eac5cd2b7b62.gif" alt="b_1b_2" valign="middle"/>] is "<img src="/img/cache/b75a3fd9300479b267b98a50962b9eb8.gif" alt="\surd" valign="middle"/>":
	        if two-byte pattern:
		    signatures = HT1[<img src="/img/cache/341d750b78dd108b12e3eac5cd2b7b62.gif" alt="b_1b_2" valign="middle"/>]->st
		    match(signatures)
		else:
		    if HT2[<img src="/img/cache/93fae20b2eba7c8ed7593a70ce76e90f.gif" alt="b_3b_4" valign="middle"/>] is "<img src="/img/cache/b75a3fd9300479b267b98a50962b9eb8.gif" alt="\surd" valign="middle"/>":
			signatures = HT2[<img src="/img/cache/93fae20b2eba7c8ed7593a70ce76e90f.gif" alt="b_3b_4" valign="middle"/>]->st
			match(signatures)
</pre>
<p>Veldman's algorithm easily supports wildcards of arbitrary complexity in signatures, something the stock Aho-Corasick algorithm doesn't handle.<sup><a href="#f4_108" name="b4_108">108</a></sup> However, the sequential search overhead of Veldman's algorithm must be carefully monitored, and both Veldman and Aho-Corasick look at every byte in the input. Is it possible to do better?</p>
<h5><a name="c4113"></a>4.1.1.3 Algorithm: Wu-Manber</h5>
<p>The Wu-Manber algorithm relies on the same insight as Veldman's algorithm, limiting the set of signatures that must be linearly searched.<sup><a href="#f4_109" name="b4_109">109</a></sup> The difference is that Wu-Manber is able to skip input bytes that can't possibly correspond to a match, resulting in improved performance. The same example signatures will be used to demonstrate the algorithm: blar?g, foo, greep, green, agreed.</p>
 
<div align="center">
<img src="img/mja01/fig47.gif" alt="Figure 4.7. Data structures for Veldman's algorithm"/>
<p><strong>Figure 4.7. Data structures for Veldman's algorithm</strong></p>
</div>
<p>The Wu-Manber search code is below:</p>
<pre class="source">
	i = MINLEN
	while i &lt; n:
	    shift = SHIFT[<img src="/img/cache/d40eb156199100355b2c078e7b884244.gif" alt="b_{i-1}b_i" valign="middle"/>]
	    if shift = 0:
		signatures = HASH[<img src="/img/cache/d40eb156199100355b2c078e7b884244.gif" alt="b_{i-1}b_i" valign="middle"/>]
		match(signatures)
		shift = 1
	    i = i + shift
</pre>
<p>The bytes of the input are denoted <img src="/img/cache/bda0411e4b6129d514dcbfa5810fb14a.gif" alt="b_1" valign="middle"/> to <img src="/img/cache/e9fe295c38cf48a487562df323d6569f.gif" alt="b_n" valign="middle"/>, and <tt>MINLEN</tt> is the minimum length of any pattern substring; its calculation will be explained below. Two hash tables are used, as shown in Figure 4.8. <tt>SHIFT</tt> holds the number of input bytes that may safely be skipped, and <tt>HASH</tt> stores the sets of signatures to attempt matching against. The hash functions used to index into the hash tables have not been shown, and in practice, different hash functions may be used for the different hash tables. The <tt>match</tt> subroutine attempts to match the input text starting at <img src="/img/cache/384d288dddc226ad621a02e07a4cca8b.gif" alt="b_{i-MINLEN+1}" valign="middle"/> against a list of signatures.</p>
<p>A trace of the algorithm for the running example is in Figure 4.9. <tt>MINLEN</tt> is three, and for this short input, only four hash table lookups in <tt>SHIFT</tt> occur, with one (successful) matching attempt finding "foo" starting at <img src="/img/cache/fa64f0ef69e83299f2c860805cc8b3f6.gif" alt="b_6" valign="middle"/>.</p>
<p>This leaves the question of how the hash tables are constructed. It is a four-step process:</p>
 
<div align="center">
<img src="img/mja01/fig48.gif" alt="Figure 4.8. Wu-Manber hash tables"/>
<p><strong>Figure 4.8. Wu-Manber hash tables</strong></p>
</div>
<div align="center">
<img src="img/mja01/fig49.gif" alt="Figure 4.9. Wu-Manber searching"/>
<p><strong>Figure 4.9. Wu-Manber searching</strong></p>
</div>
<ol>
<li>Calculating <tt>MINLEN</tt>. This is the minimum number of adjacent, non-wildcard bytes in any signature. For the example, <tt>MINLEN</tt> is 3 because of the signature "foo:"
<table summary="signature length">
<tr><th>Signature</th><th>Length</th></tr>
<tr><td>blar?g</td><td>4</td></tr>
<tr><td>foo</td><td>3</td></tr>
<tr><td>greep</td><td>5</td></tr>
<tr><td>green</td><td>5</td></tr>
<tr><td>agreed</td><td>6</td></tr>
</table></li>
 
<li>Initializing the <tt>SHIFT</tt> table. Now, take one pattern substring for each signature containing <tt>MINLEN</tt> bytes: bla, foo, gre, agr. The Wu-Manber search code above examines adjacent pairs of input bytes, so consider every two-byte pair in the pattern substrings:
<pre>
	ag fo la re
	bl gr oo
</pre>
<p>If the pair of input bytes <em>isn't</em> one of these, then the search can safely skip <tt>MINLEN-1</tt> input bytes. Because the <tt>SHIFT</tt> table holds the number of bytes to skip for any input byte pair, initialize each entry in it to <tt>MINLEN-1</tt>.</p></li>
<li>Filling in the <tt>SHIFT</tt> table. For each two-byte pattern substring pair <img src="/img/cache/3e44107170a520582ade522fa73c1d15.gif" alt="xy" valign="middle"/>, <img src="/img/cache/7790be61ad39ece9f7bde81009feb584.gif" alt="q_{xy}" valign="middle"/> is the rightmost ending position of <img src="/img/cache/3e44107170a520582ade522fa73c1d15.gif" alt="xy" valign="middle"/> in <em>any</em> pattern substring. The <tt>SHIFT</tt> table is filled in by setting <img src="/img/cache/b1b71c15c341b6563ed112392c1847de.gif" alt="\text{SHIFT}[xy]=\text{MINLEN}-q_{xy}" valign="middle"/>. For example:
<table summary="">
<tr><th>xy</th><th>Signature(s)</th><th><img src="/img/cache/7790be61ad39ece9f7bde81009feb584.gif" alt="q_{xy}" valign="middle"/></th></tr>
<tr><td>bl</td><td>bla</td><td>2</td></tr>
<tr><td>ag</td><td>bla</td><td>3</td></tr>
<tr><td>gr</td><td>agr,gre</td><td>3</td></tr>
</table>
<p>The bytes in the pattern substrings are numbered from 1, explaining why the ending position of "bl" in <img src="/img/cache/708570d5180aeb1ce1d03339ddc2ec32.gif" alt="b_1l_2a_3" valign="middle"/> is 2, for instance.</p></li>
<li>Filling in the <tt>HASH</tt> table. If <img src="/img/cache/0a8549ae72cdd9dff61b75287bd79864.gif" alt="\text{MINLEN}-q_{xy}" valign="middle"/> is zero for some <img src="/img/cache/3e44107170a520582ade522fa73c1d15.gif" alt="xy" valign="middle"/> above, then the search has found the rightmost end of a pattern substring. A match can be tried; <img src="/img/cache/e43036476e6a745edb0d3909b04bb527.gif" alt="\text{HASH}[xy]" valign="middle"/> is set to the list of signatures whose pattern substring ends in <img src="/img/cache/3e44107170a520582ade522fa73c1d15.gif" alt="xy" valign="middle"/>.</li>
</ol>
<p>The full Wu-Manber algorithm is much more general; only a simplified form of it has been presented here. It was designed to scale well and handle tens of thousands of signatures, even though its worst case is horrendous, requiring a sequential search through all signatures for every input byte. Tests have shown that it lives up to these design goals, outperforming advanced forms of Aho-Corasick except when the number of possible input values is very small.<sup><a href="#f4_4" name="b4_4">4</a></sup></p>
<h5><a name="c4114"></a>4.1.1.4 Testing</h5>
<p>How can a user determine if their anti-virus scanner is working? Testing using live viruses may <em>seem</em> to be a good idea, and an endless supply of them is available on the Internet and in a typical mailbox.<sup><a href="#f4_110" name="b4_110">110</a></sup> Malware of any sort is potentially dangerous, though, and shouldn't be handled without special precautions, especially by users without any special training.</p>
 
<div align="center">
<pre><a class="__cf_email__" href="/cdn-cgi/l/email-protection" data-cfemail="c991fc86e899ec898899">[email&#160;protected]</a><script data-cfhash='f9e31' type="text/rocketscript">/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */</script>[4\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H* </pre>
<p><strong>Figure 4.10. The EICAR test file</strong></p>
</div>
<p>Testing can be done using non-viral code which the anti-virus software will recognize to be a test file. The EICAR test file is intended to fill the need for such a non-viral file. It is a legitimate MS-DOS program and, when run, prints the message:</p>
<pre>EICAR-STANDARD-ANTIVIRUS-TEST-FILE!</pre>
<p>All modem anti-virus software should detect this test file. The contents of the file were designed to be printable ASCII, and can be entered with any text editor. The only caveat is that the file's contents, in Figure 4.10, must be the first 68 bytes in the file. (The disassembly of this code is not particularly enlightening, and is omitted.) Some trailing whitespace is permitted, so long as the file doesn't exceed 128 bytes in length; nothing else may be in the file.</p>
<p>The drawback to the EICAR test file is that it <em>is</em> non-viral, and it hardly constitutes an exhaustive test of anti-virus software. Anti-virus software is unlikely to rely solely on a scanner anyway, and the EICAR test file does nothing to exercise other anti-virus techniques.</p>
<h5><a name="c4115"></a>4.1.1.5 Improving Performance</h5>
<p>Scanning an entire file for viruses is slow; it is referred to using the derogative term <em>grunt scanning</em>. There are four general approaches to improving scanner performance:</p>
<dl>
<dt><strong>Reduce amount scanned.</strong></dt><dd>Scanning an entire file is not only slow, but increases the likelihood of false positives, as a signature may be erroneously found in the wrong place.<sup><a href="#f4_111" name="b4_111">111</a></sup> Instead, scanning can be targeted to specific locations based on assumptions about viral behavior.
<ul>
<li>Assuming that viruses add themselves to the beginning or the end of an executable file, searches can be limited to those areas. This is called <em>top and tail</em> scanning.</li>
<li>More complicated executable formats allow an executable's entry point to be specified. Scanning can be restricted to the program's entry point and instructions reachable from that entry point.</li>
<li>If the exact positions of all virus signatures are known, then scanning can be specifically directed to those areas. The assumption here is that all viruses <em>are</em> known, along with their behavior in terms of file location. This is in contrast to the more generic assumptions about virus locations
 
made above. In conjunction with the entry point scanning above, this is referred to as <em>fixed point scanning</em>.</li>
<li>Many viruses are small. The amount scanned in any location can be set according to the size of common viruses. For example, if most viruses are less than 8K in size, then the scanner may only examine 8K areas at the beginning and end of the executable.<sup><a href="#f4_112" name="b4_112">112</a></sup></li>
</ul>
<p>Use of scanning-reduction techniques implies that the scanner will no longer see the complete input. The input to a scanning algorithm doesn't have to be a faithful representation of a file's contents, however. The algorithms work equally well on an abridged view of the input.</p>
<p>Of the performance-enhancing approaches, reducing the amount scanned is the only approach that directly affects the potential correctness of the result.</p></dd>
<dt><strong>Reduce amount of scans.</strong></dt><dd>Regardless of how much of a file is or isn't scanned, avoiding a scan completely is better.<sup><a href="#f4_5" name="b4_5">5</a></sup> This can be accomplished several ways:
<ul>
<li>Scanning can only be done for certain file types; only executable files may be scanned, for instance, and not data files. Viruses and other threats have been markedly versatile in choosing places to reside, making this scanning-avoidance option no longer viable.</li>
<li>Anti-virus software can compute and store state information for files that have been successfully scanned, and only re-scan files if they have changed.<sup><a href="#f4_113" name="b4_113">113</a></sup> While the technique is sound, a number of issues arise:<sup><a href="#f4_114" name="b4_114">114</a></sup>
<ul>
<li>What information about a file is stored? A file's state information must be sufficient to determine if the file has been changed or not. File state may include the file length and the date/time of the last file modification; these are easy to compare for changes, but also easy for a virus writer to fake.
<p>A stronger means of change detection would compute a checksum of the file, and store the checksum in the file's state information too. Note that the checksum is only used for avoiding scans, and isn't used for virus detection in this case, like integrity checkers (Section 4.1.3) do.</p></li>
<li>Where is state information stored? The possible locations include:
<ol>
<li>In memory. An in-memory cache offilestate information would not persist across machine reboots, or any other situation where the anti-virus software would be restarted. The size of a memory cache would necessarily be bounded to prevent too much memory from being consumed, and a cache replacement algorithm would be needed to select cache entries to evict when
 
the cache fills up. Removing file state from the cache doesn't change anti-virus accuracy, just performance - in the worst case, re-scanning would be required.</li>
<li>On disk, in a database. File state information can be stored in a database on disk. Persistence and size aren't problems, but the file state database becomes a target for attack. Also, if the database is keyed to filenames, then a file which is renamed or copied is a file which gets rescanned, because its new identity isn't present in the database.</li>
<li>On disk, tagged onto files. Extended filesystem attributes can be used to attach file state information onto the file itself. These attributes are carried along when a file is renamed or copied.</li>
</ol></li>
<li>What constitutes a change? Obviously, any differences between the stored file state and its current state would indicate a change. The comparison should be ordered so that cheaper operations, like fetching a file's length, are done before more expensive operations like checksumming.
<p>Updates to the virus database, while not a change in file state <em>per se</em>, should appear as a change so that the file is re-scanned.<sup><a href="#f4_115" name="b4_115">115</a></sup> This is trivial to implement with an in-memory file state cache: a cache flush resets all stored file state information at once. For on-disk information, this can be implemented by adding the version of the virus database used for scanning into the file state information.</p>
<p>An alternative approach is to use <em>session keys</em>. A session key is a unique key which is changed each time the anti-virus software is run, and files have the current session key attached to them when they are scanned. The scanner checks for a file's session key before scanning it; a re-scan is done if the session key doesn't match or is absent.</p></li>
<li>How are checksums computed efficiently?<sup><a href="#f4_116" name="b4_116">116</a></sup> Computing the checksum of an entire file can take longer than scanning it. This presents the same problem as grunt scanning had to begin with! Much the same solution is used: only checksum key areas of a file. The "key areas" of a file depend on the file type, though, which implies that checksumming code must be able to understand all the different types of file.
<p>A more clever way to find the key areas of a file is to leverage the existing anti-virus software. The scanner is implicitly identifying key areas by virtue of where it looks for a signature. The anti-virus checksumming code can let the scanner proceed, recording the disk
 
blocks accessed in the file - these are the key areas that should be checksummed.</p></li>
<li>How is tampering avoided? On-disk information of any kind is subject to attack. File state information can be encrypted to make it slightly harder to forge. If session keys are used, the session key can be used as the encryption key to encrypt something that can be verified for correct decryption: a constant value, the filename, or the file state information.<sup><a href="#f4_6" name="b4_6">6</a></sup></li>
</ul></li>
</ul></dd>
<dt><strong>Lower resource requirements.</strong></dt><dd>Engineering tradeoffs may be made to improve on-access performance, such as lowering CPU and memory demands by using a smaller, less precise set of signatures. This doesn't have to impact overall accuracy, because additional verification can catch false positives, as Section 4.4 explains.
<p>Signature selection is a difficult issue, and involves tradeoffs in precision as well as resource requirements. Short signatures can result in false positives and misidentification;<sup><a href="#f4_117" name="b4_117">117</a></sup> long signatures are more precise, but bloat the virus database. There is the additional danger of being <em>too</em> precise. Long signatures may be so specific as to not detect minor virus variants - ideally, signatures are chosen with possible variations in mind whenever possible, like changes to data strings. Compiler-generated code is not terribly distinctive for short signatures, either, and signatures may be better chosen from the data area for viruses written in high-level languages.<sup><a href="#f4_118" name="b4_118">118</a></sup></p></dd>
<dt><strong>Change the algorithm.</strong></dt><dd>There is an overwhelming amount of research done on efficient string-searching algorithms, and improving the basic searching algorithm is always a possibility.
<p>One avenue that may be explored is the use of algorithms tailored to specific file types. There are many, many kinds of compressed, archived, encoded, and weakly encrypted files which may harbor viruses. Too many, in fact: typically, anti-virus scanners are preceded by a file type-specific decoder, which provides the scanner with a plaintext, logical view of the input. Scanning algorithms exist for directly searching specific file types, like compressed files, which would avoid the need for separate decoding.<sup><a href="#f4_119" name="b4_119">119</a></sup> This would only make good engineering sense for file types which are frequently-encountered and tend to have large file sizes.</p></dd>
<dt><strong>Change the algorithm implementation.</strong></dt><dd>Tuning an algorithm's implementation is a touchy process, and the results may depend on the compiler, CPU, and memory as much as they depend on the code itself. For algorithms that are implemented using frequent lookups in tables whose data doesn't change, converting the algorithm and its data into directly-executable code has yielded performance dividends in the past.<sup><a href="#f4_120" name="b4_120">120</a></sup> Effectively, the tables are
 
turned into code. Changing the underlying algorithm itself, rather than its implementation, is likely to have a bigger impact, though.<sup><a href="#f4_121" name="b4_121">121</a></sup></dd>
</dl>
<p>These general approaches are not specific to scanners, and may be adapted to improve the performance of other anti-virus techniques.</p>
<h4><a name="c412"></a>4.1.2 Static Heuristics</h4>
<p>Anti-virus software can employ <em>static heuristics</em> in an attempt to duplicate expert anti-virus analysis. Static heuristics can find known or unknown viruses by looking for pieces of code that are generally "virus-like," instead of scanning for specific virus signatures.<sup><a href="#f4_122" name="b4_122">122</a></sup> This is a static analysis technique, meaning that the code being analyzed is not running, and there is no guarantee that any suspicious code found would ever be executed.<sup><a href="#f4_123" name="b4_123">123</a></sup></p>
<p>Static heuristic analysis is done is two steps:<sup><a href="#f4_124" name="b4_124">124</a></sup></p>
<ol>
<li>Data: the Gathering. Data can be collected using any number of static heuristics. Whether or not any one heuristic correctly classifies the input is not critical, because the results of many heuristics will be combined and analyzed later.
<p>A scanner can be used to locate short signatures which are generally indicative of suspicious code, called <em>boosters</em>.<sup><a href="#f4_125" name="b4_125">125</a></sup> The presence of a booster increases the likelihood that the code being analyzed is viral. For example:</p>
<ul>
<li>Junk code.</li>
<li>Decryption loops.</li>
<li>Self-modifying code.</li>
<li>Use of undocumented API calls.</li>
<li>Manipulation of interrupt vectors.</li>
<li>Use of unusual instructions, especially ones that wouldn't be generated by a compiler.</li>
<li>Strings containing obscenities, or obvious cues like the word "virus."</li>
</ul>
<p>It is equally important to look for things that are present in "normal" code, things that viruses don't usually do. For example, viruses don't often create pop-up dialogue boxes for the user.<sup><a href="#f4_126" name="b4_126">126</a></sup> This would be considered a <em>negative heuristic</em>, or a <em>stopper</em>.</p>
<p>Other heuristics can be computed which aren't based on scanning:</p>
<ul>
<li>The difference between an executable's entry point and its end of file can be computed.<sup><a href="#f4_127" name="b4_127">127</a></sup> Too small a value, when compared to the same value for typical uninfected executables, may point to an appender.</li>
 
<li>Spectral analysis of the code may be done, computing a histogram of the bytes or instructions used in the code. Encrypted code will have a different spectral signature from unencrypted code.<sup><a href="#f4_128" name="b4_128">128</a></sup></li>
</ul></li>
<li>Analysis. As hinted at by the terms "booster" and "stopper," analysis of static heuristic data may be as simple as weighting each heuristic's value and summing the results. If the sum passes some threshold, then the input is deemed to be infected.
<p>More elaborate methods of data analysis might use neural networks, expert systems, or data mining techniques.<sup><a href="#f4_129" name="b4_129">129</a></sup></p></li>
</ol>
<p>Signatures of suspicious code will most likely be chosen by expert anti-virus researchers. This process can be automated, however, at least for some restricted domains: IBM researchers automatically found static heuristic signatures for BSIs. They took two corpuses of boot blocks, one exclusively containing BSIs, one with no infections. A computer found trigrams - sequences of three bytes - which appeared frequently in the BSI corpus but not in the other corpus. Finally, they computed a 4-cover such that each BSI had at least four of the found BSI trigrams. After this process, they were left with a set of only fifty trigrams to look for. The presence or absence of these trigrams was used to classify a boot block as infected or not.<sup><a href="#f4_130" name="b4_130">130</a></sup></p>
<p>Static heuristics may be viewed as a way to reduce the resource requirements of anti-virus scanners. Full virus signatures in a virus database can be distilled down to a set of short, generic, static heuristic signatures. (The distillation may even be done automatically, using the IBM technique just described.) An anti-virus scanner can look for these short signatures, loading in their associated set of full virus signatures only if a match is found. This alleviates the need to keep full signatures in memory.<sup><a href="#f4_131" name="b4_131">131</a></sup></p>
<h4><a name="c413"></a>4.1.3 Integrity Checkers</h4>
<p>With the exception of companion viruses, viruses operate by changing files. An <em>integrity checker</em> exploits this behavior to find viruses, by watching for unauthorized changes to files.<sup><a href="#f4_132" name="b4_132">132</a></sup></p>
<p>Integrity checkers must start with a perfectly clean, 100% virus-free system; it is impossible to understate this. The integrity checker initially computes and stores a checksum for each file in the system it's watching. Later, a file's checksum is recomputed and compared against the original, stored checksum. If the checksums are different, then a change to the file occured.</p>
<p>There are three types of integrity checker:</p>
<ol>
<li>Offline. Checksums are only verified periodically, e.g., once a week.</li>
<li>Self-checking. Executable files are modified to check themselves when run. Ironically, modifying executables to self-check their integrity involves
 
virus-like mechanisms. Self-checking can be done in a less-obtrusive way by adding the self-checking code into shared libraries.
<p>In general, anti-virus software will perform integrity self-checking,<sup><a href="#f4_133" name="b4_133">133</a></sup> regardless of the anti-virus technique it uses. The allure of attacking anti-virus software is too great to ignore.</p></li>
<li>Integrity shells. An executable file's checksum is verified immediately prior to execution. This can be incorporated into the operating system kernel for binary executable files; the ideal positioning is less clear for other types of "executable" files, like batch files, shell scripts, and scripting language programs.</li>
</ol>
<p>As Section 4.3 explains, integrity checkers have a long list of drawbacks, and are not suitable as the only means of anti-virus protection for a system.</p>
<h3><a name="c42"></a>4.2 Detection: Dynamic Methods</h3>
<p>Dynamic anti-virus techniques decide whether or not code is infected by running the code and observing its behavior.</p>
<h4><a name="c421"></a>4.2.1 Behavior Monitors/Blockers</h4>
<div class="signature">
'Interestingly, viruses are detected now (and always have been) by behavioral recognition. Unfortunately, the customers are the ones who have been forced to perform this function.'
<p>- Paul Schmehl<sup><a href="#f4_134" name="b4_134">134</a></sup></p>
</div>
<p>A <em>behavior blocker</em> is anti-virus software which monitors a running program's behavior in real time, watching for suspicious activity. If such activity is seen, the behavior blocker can prevent the suspect operations from succeeding, can terminate the program, or can ask the user for the appropriate action to perform. Behavior blockers are sometimes called <em>behavior monitors</em>, but the latter term implies (rightly or wrongly) that no action is taken, and the burglars are only watched while they steal the silver.</p>
<p>What does a behavior blocker look for? Roughly speaking, a behavior blocker watches for a program to stray from what the blocker considers to be "normal" behavior. Normal behavior can be modeled in three ways, by describing:<sup><a href="#f4_135" name="b4_135">135</a></sup></p>
<ol>
<li>The actions that are permitted. This is called <em>positive detection</em>,</li>
<li>The actions that are not permitted, called <em>negative detection</em>,</li>
<li>Some combination of the two, in much the same way that static heuristics included boosters and stoppers.</li>
</ol>
<p>An analogy can be drawn with natural immune systems, because behavior blockers are trying to discern <em>self</em> from <em>nonself</em>, or normal from anomalous
 
behavior. This is the same thing that immune systems need to do to distinguish normal cells from foreign invaders.<sup><a href="#f4_136" name="b4_136">136</a></sup> Care must be taken, however, because anomalous behavior does not automatically imply viral behavior.</p>
<div align="center">
<img src="img/mja01/fig4b.gif" alt="Figure 4.11. Static vs. dynamic"/>
<p><strong>Figure 4.11. Static vs. dynamic</strong></p>
</div>
<p>The actions examined by behavior blockers do not need to include every instruction executed; they need only include actions of interest for virus detection. For example, most virus activity eventually needs to call some system functionality, like I/O operations - only these actions have to be considered. No matter how obfuscated the I/O calls are statically, the calls will appear clearly when the code runs.<sup><a href="#f4_137" name="b4_137">137</a></sup> This is a major benefit enjoyed by dynamic types of analysis like behavior blocking.</p>
<p>If each action that code performs is thought of as a symbol in a string, then behavior blockers can be seen to be looking for dynamic signatures instead of the static signatures used by static anti-virus techniques. (The same search algorithms can be used for dynamic signatures, but the "input string" is dynamically generated.) The difference is shown in Figure 4.11. Other ideas carry over from static techniques, too. Behavior blockers can look for short dynamic signatures which are generally indicative of virus-like behavior. Looking at I/O actions, for instance, an appending virus might exhibit a dynamic signature like:</p>
<ol>
<li>Opening an executable, with both read and write permission.</li>
<li>Reading the portion of the file header containing the executable's start address.</li>
<li>Writing the same portion of thefileheader. (The start address can be checked separately for changes consistent with expected viral behavior.)</li>
 
<li>Seeking to the end of the file.</li>
<li>Appending to the file.</li>
</ol>
<div align="center">
<img src="img/mja01/fig4c.gif" alt="Figure 4.12. From execution trace to dynamic signatures"/>
<p><strong>Figure 4.12. From execution trace to dynamic signatures</strong></p>
</div>
<p>Variations on this dynamic signature are obviously possible, and those variants can be enumerated and watched for too. Generic, dynamic signatures like these can be produced by human anti-virus experts.</p>
<p>Dynamic signatures specific to a given piece of code may be found automatically that characterize permitted actions for the code. The code is run and profiled before it becomes infected, watching the actions the code performs. To produce dynamic signatures of length <em>K</em>, the stream of actions is examined through a window of size <em>K</em>, saving all unique combinations of actions (Figure 4.12 is an example for <em>K = 3</em>); those are the code's dynamic signatures for normal behavior, which are recorded in a database. When the same code is run later, the same process is repeated, but this time the actions within the window are looked up in the database, to ensure that they were previously seen. Too many new action sequences indicate abnormal behavior. In practice, using system calls (without parameters) as actions, and a value of <em>K = 10</em>, this scheme was seen to work well for several Unix system programs.<sup><a href="#f4_138" name="b4_138">138</a></sup></p>
<p>False positives from behavior blockers can be mitigated by taking context information into account. A notion of "ownership" is especially useful in this regard, because it gives applications a lot of leeway in terms of the behaviors they can exhibit when working with their files.<sup><a href="#f4_139" name="b4_139">139</a></sup> Web browsers maintain a cache of previously-downloaded data, for example. Web browsers also clear out their caches periodically, without warning, and a mass deletion of files looks
 
more than a little bit like something that a virus would do. A behavior blocker that tracked the cache files' creation would know that they "belong" to the web browser, and so the file deletion is probably legitimate.</p>
<p>This file deletion example serves to illustrate a common criticism leveled at behavior blockers: the code whose behavior is being monitored is actually running. Any bad effects like file deletion that the behavior blocker doesn't prevent are allowed to proceed unchecked. A general, system-wide "undo" facility can alleviate some of these concerns by increasing the time window which the behavior blocker has to detect viral behavior without ill effect.<sup><a href="#f4_140" name="b4_140">140</a></sup> Not all operations can be undone, such as anything transmitting information outside the machine. A short-term undo ability for some asynchronous operations, like sending email, can be implemented by introducing a transmission delay in sending email to a remote machine.<sup><a href="#f4_141" name="b4_141">141</a></sup></p>
<p>Finally, there is the question ofhow long a running program's behavior should be monitored. The duration of monitoring is a concern because monitoring adds run-time overhead. Assuming most viruses will reveal themselves early when an infected program runs, programs only need to be monitored when they start. However, this assumption is not always valid. In any case, behavior blockers can be enabled and disabled for a running program as needed.</p>
<h4><a name="c422"></a>4.2.2 Emulation</h4>
<p>Behavior blocking allowed code to run on the real machine. In contrast, anti-virus techniques using <em>emulation</em> let the code being analyzed run in an emulated environment. The hope is that, under emulation, a virus will reveal itself. Because any virus found wouldn't be running on the real computer, no harm is done.</p>
<p>Emulation can be applied two ways, although the boundary between them is admittedly fuzzy:</p>
<dl>
<dt><strong>Dynamic heuristics</strong></dt><dd><em>Dynamic heuristics</em> are exactly the same as static heuristics. The only difference is in how the data is gathered: dynamic heuristic analysis gathers its data from the emulator about the code being analyzed. The analysis is done the same way as it is for static heuristics.
<p>Dynamic heuristics can look for the same features as behavior blockers too, like system calls. The emulator is a safe virtual environment in which to monitor running code, however, and emulation doesn't run the code to completion. Dynamic heuristics can be used effectively to spot the dynamic signatures of metamorphic viruses.<sup><a href="#f4_142" name="b4_142">142</a></sup></p></dd>
<dt><strong>Generic decryption</strong></dt><dd>For polymorphic viruses, the decryptor loop can be very hard for anti-virus software to spot. <em>Generic decryption</em> skirts this issue by relying on the virus' own decryptor loop to decrypt the virus body. Once
 
decrypted, the virus body can be detected using normal scanning methods.<sup><a href="#f4_143" name="b4_143">143</a></sup> This makes exact identification possible for known polymorphic viruses.
<p>Generic decryption uses heuristics to determine when a virus has decrypted itself.<sup><a href="#f4_144" name="b4_144">144</a></sup> For example, the virus may try to execute an instruction which resides in a previously-modified (i.e., decrypted) memory location. Another indicator is the apparent size of the decryption, although this amount will vary with the architecture. On Intel x86 platforms, 24 bytes or more of modified/decrypted memory is a promising sign of decryption. A series of boosters followed by some stoppers is yet another indication that decryption is complete.</p>
<p>Besides heuristics, an emulator can scan memory for signatures periodically during emulation, and upon completion of the emulation.<sup><a href="#f4_145" name="b4_145">145</a></sup></p></dd>
</dl>
<p>The rest of this section discusses the parts of an emulator, reasons to re-run the emulator, and ways to optimize emulation.</p>
<h5><a name="c4221"></a>4.2.2.1 Emulator Anatomy</h5>
<p>One way to execute code in a controlled way is to single-step through the code. Code could arguably be "emulated" this way.<sup><a href="#f4_146" name="b4_146">146</a></sup> However, single-stepping can be easily detected by a virus, and there is always the danger of a virus running in a non-virtual environment escaping. A more elaborate emulation mechanism is needed.</p>
<p>Conceptually, an emulator has five parts:<sup><a href="#f4_147" name="b4_147">147</a></sup></p>
<ol>
<li>CPU emulation.</li>
<li>Memory emulation. The full scope of the memory emulator's task is daunting: 32 bits of address means that potentially 4G of address space must be emulated. Fortunately, the emulator does not run enough of the code's instructions for that much emulated memory to be chewed up.
<p>For generic decryption, as mentioned above, the memory emulator will need to keep track of how much memory has been modified, and where it is. This is not only useful for deciding if the decryptor loop has finished. Later scanner operation can be limited to the areas of memory which the suspected virus has modified.</p></li>
<li>Hardware and operating system emulation. Real operating system code isn't used in an emulator, but rather a stripped-down mock-up of it. Why? There are four reasons:<sup><a href="#f4_148" name="b4_148">148</a></sup>
<ul>
<li>Copyright and licensing issues with the real operating system code.</li>
<li>Size - the real operating system consumes a lot of memory and disk space.</li>
 
<li>Startup time. The overhead is too great to boot an operating system in the emulator (or restore a snapshot) for <em>every</em> program being emulated.</li>
<li>The emulator needs monitoring capability which isn't present in a real operating system.</li>
</ul>
<p>Many operating system calls in an emulator will return faked, fixed values.</p>
<p>For hardware emulation, the parts typically used by viruses must be emulated, such as timers that a virus might use to generate random numbers. The low-level disk interface would have once been important to emulate, but any code now talking to that interface directly is probably up to no good.</p></li>
<li>Emulation controller. When does emulation stop? No attempt is made to run code being analyzed to completion (with the exception of running code in an anti-virus lab). There are two reasons for this. First, time spent emulating is time the user isn't getting any response from the program being analyzed. Second, some code never finishes; application programs run until the user tells them to quit, and network servers are meant to run indefinitely. This is related to the famous Halting Problem in computer science, which says that it is not possible in general for one program to decide if another program will ever stop running.
<p>In practice, the emulation controller will use rules and heuristics to decide when to stop emulation. Some example rules are:</p>
<ul>
<li>The number of instructions emulated. The exact values are architecture-dependent, and the maximum thresholds will increase with increases in computer power. On an Intel x86, less than 1000 instructions usually need to be emulated; emulation times start becoming prohibitive at about the 30,000 instruction mark.<sup><a href="#f4_149" name="b4_149">149</a></sup></li>
<li>The amount of time spent emulating. One anti-virus' default setting is 45 seconds.</li>
<li>The proportion of instructions that modify memory. Too low a proportion can mean non-viral code, or a virus which isn't encrypted.<sup><a href="#f4_150" name="b4_150">150</a></sup></li>
</ul>
<p>Heuristically, the emulation controller could watch for stoppers, things that viruses normally wouldn't do. For instance, most viruses won't perform output prior to decrypting.</p></li>
<li>Extra analyses. The emulator may gather additional data during emulation which can be used for additional, post-emulation analyses. For example, a histogram can be maintained of executed instructions which are typical of virus decryption. This can be used to find well-obscured polymorphic viruses. A histogram can also be used to detect metamorphic viruses by comparing the emulation histogram to histograms of known metamorphic viruses.<sup><a href="#f4_151" name="b4_151">151</a></sup></li>
</ol>
 
<div align="center">
<img src="img/mja01/fig4d.gif" alt="Figure 4.13. Herding goats"/>
<p><strong>Figure 4.13. Herding goats</strong></p>
</div>
<h5><a name="c4222"></a>4.2.2.2 If at First You Don't Succeed</h5>
<p>The emulation controller may re-invoke the emulator for a variety of reasons:</p>
<ul>
<li>Virus code may have results which are specific to a certain CPU and its properties. For example, self-modifying virus code may rely on how a particular CPU fetches its instructions, or instructions may be used which only work on a specific CPU. The emulator may need to be re-run with different CPU parameters.<sup><a href="#f4_152" name="b4_152">152</a></sup></li>
<li>If a virus is observed to install interrupt handlers, the emulator can be run on those handlers to test their behavior.<sup><a href="#f4_153" name="b4_153">153</a></sup></li>
<li>Some viruses do not take control at the usual entry point of an infected program, and instead have multiple entry points. The emulator can be run on each possible virus entry point.<sup><a href="#f4_154" name="b4_154">154</a></sup></li>
<li>The ability of a suspected virus to self-replicate can be confirmed using goat files.<sup><a href="#f4_155" name="b4_155">155</a></sup>
<p>A <em>goat</em> file is a "sacrificial" file that can be used as a decoy, where any modifications to the goat file indicate illicit activity. A goat file with known properties can also be used to deduce information about a virus.</p>
<p>The goat file in Figure 4.13 is an executable which in this case simply exits without performing any I/O. The goat file is fed to a suspected virus inside the emulator. If the goat file is modified, then the emulator is re-run, feeding the original goat file to the modified goat file. An attempt to modify
 
the original goat file must now indicate a virus, because self-replication has been demonstrated.</p></li>
<li>One problem with emulation is that viral behavior may be (deliberately) sporadic, only manifesting itself under certain conditions, like a time-based trigger.
<p>The code for these viral behaviors is usually run or not run based on the result of a conditional branch in the virus' code. The emulator can watch for untaken branches that could signal this, and queue up the untaken branches along with an approximate machine state for each: register contents, program counter and stack pointer values, and some contents of the top of the stack. After the main emulation is done, the emulator can be re-run on the queued branches to try and flush out hidden behavior.<sup><a href="#f4_156" name="b4_156">156</a></sup></p></li>
<li>A related use of re-running the emulator is watching for unused memory areas in the virus which may be instructions. (The instructions could be executed through a mechanism which the emulator didn't discover.) The emulation controller can heuristically set a "virus region" of memory, and watch for parts of it that aren't executed during the main emulation. Later, a machine state can be guessed at, such as setting all the register contents to zero, and the emulator can try to run the unused memory areas.<sup><a href="#f4_157" name="b4_157">157</a></sup></li>
</ul>
<h5><a name="c4223"></a>4.2.2.3 Emulator Optimizations</h5>
<p>"Optimization" is a broad term. Emulator optimizations can address emulator size and complexity, as well as being used to improve emulator performance.</p>
<ul>
<li>Instead of emulating real filesystems,the emulator can <em>use</em> real filesystems.<sup><a href="#f4_158" name="b4_158">158</a></sup> Disk reads can be passed through to the real disk, and any disk writes can be stored in the emulator and not written through to the disk. Naturally, subsequent reads of changed information would return the copy stored in the emulator. This optimization reduces emulator size, complexity, and startup time.</li>
<li>Data files may be emulated as though they contained code, because a virus may conceivably hide there. Code that makes extensive use of uninitialized registers is often an indication of a legitimate data file. This heuristic can be used to stop the emulator early.<sup><a href="#f4_159" name="b4_159">159</a></sup></li>
<li>A cache can be kept of previous emulator states, where a cached state record may include:<sup><a href="#f4_160" name="b4_160">160</a></sup>
<ul>
<li>the register contents;</li>
<li>the program counter's value;</li>
 
<li>instructions around the memory location where the program counter points;</li>
<li>the stack pointer;</li>
<li>stack contents around where the stack pointer points;</li>
<li>the size of the emulated file;</li>
<li>the number of memory writes done by the emulated code;</li>
<li>the number of memory bytes changed by the emulated code;</li>
<li>a checksum of the data written to memory.</li>
</ul>
<p>The emulator is run for some relatively small number of instructions: 400-1000 on an Intel x86. Normally the emulator would be paused here anyway, because if no decryption activity had been detected by this time, any virus would be assumed to be unencrypted, and the emulation controller could begin normal virus scanning.</p>
<p>A state record is constructed at this point, and the state cache is searched. A cache hit signifies that the code has been emulated previously and declared virus-free, so emulation may stop here. Otherwise, emulation resumes and continues to its normal termination. If the code is still deemed to be clean, the constructed state record is added to the state record cache for later. The net effect is a speed improvement, because emulation can be stopped early for previously-emulated code.</p></li>
</ul>
<h3><a name="c43"></a>4.3 Comparison of Anti-Virus Detection Techniques</h3>
<p>This chapter has presented a wide range of anti-virus techniques, each with relative strengths and weaknesses. No one technique is best for detecting every type of virus, and a combination of techniques is the most secure design.</p>
<p><strong>Scanning</strong></p>
<ul>
<li>Pro: Gives precise identification of any viruses that are found. This characteristic makes scanning useful by itself, as well as in conjunction with other anti-virus techniques.</li>
<li>Con: Requires an up-to-date database of virus signatures for scanning to be effective. Even assuming that users update their virus databases right away, which isn't the case, there is a delay between the time when a new threat is discovered and when an anti-virus company has a signature update ready. This leaves open a window of opportunity in which systems can be compromised. Also, scanning only finds known viruses, and some minor variants of them.</li>
</ul>
 
<p><strong>Static heuristics</strong></p>
<ul>
<li>Pro: Static heuristic analysis detects both known and unknown viruses.</li>
<li>Con: False positives are a major problem, and a detected virus is neither identified, nor disinfectible except by using generic methods.</li>
</ul>
<p><strong>Integrity checkers</strong></p>
<ul>
<li>Pro: Integrity checkers boast high operating speeds and low resource requirements. They detect known and unknown viruses.<sup><a href="#f4_161" name="b4_161">161</a></sup></li>
<li>Con: Detection only occurs <em>after</em> a virus has infected the computer, and the source of the infection can't necessarily be pinpointed. An integrity checker can't detect viruses in newly-created files, or ones modified legitimately, such as through a software update. Ultimately, the user will be called upon to assess whether a change to a file was made legitimately or not. Finally, found viruses can't be identified or disinfected.</li>
</ul>
<p><strong>Behavior blockers</strong></p>
<ul>
<li>Pro: Known and unknown viruses are detected.<sup><a href="#f4_162" name="b4_162">162</a></sup></li>
<li>Con: While a behavior blocker knows which executable is the problem, unlike an integrity checker, it again cannot identify or disinfect the virus. Run-time overhead and false positives are a concern, as is the fact that the virus is already running on the system prior to being detected.</li>
</ul>
<p><strong>Emulation</strong></p>
<ul>
<li>Pro: Any viruses found are running in a safe environment. Known and unknown viruses are detected, even new polymorphic viruses.<sup><a href="#f4_163" name="b4_163">163</a></sup></li>
<li>Con: Emulation is slow. The emulator may stop before the virus reveals itself, and even so, precise emulation is very hard to get correct. The usual concerns about identification and disinfection apply to emulation, too.</li>
</ul>
<p>In general, dynamic methods impose a run-time overhead for monitoring that is not incurred by static methods. The tradeoff is that dynamic methods, by watching code run, effectively peel away a layer of obfuscation from viral code.</p>
<h3><a name="c44"></a>4.4 Verification, Quarantine, and Disinfection</h3>
<p>Once a virus is detected, few people will want to have it remain on their computer. The tasks for anti-virus software that lie beyond detection are verification, quarantine, and disinfection. Compared to detection, these three tasks
 
are performed rarely, and can be much slower and more resource-intensive if necessary.<sup><a href="#f4_164" name="b4_164">164</a></sup></p>
<h4><a name="c441"></a>4.4.1 Verification</h4>
<p>Virus detection usually doesn't provide the last word as to whether or not code is infected. Anti-virus software will often perform a secondary verification after the initial detection of a virus occurs.</p>
<p>Verification is performed for two reasons. First, it is used to reduce false positives that might happen by coincidence, or by the use of short or overly general signatures. Second, verification is used to positively identify the virus. Identification is normally necessary for disinfection, and to prevent being led astray; virus writers will sometimes deliberately make their virus look like another one. In the absence of verification, anti-virus software can misidentify the virus and do unintentional damage to the system when cleaning up after the wrong virus.</p>
<p>Verification may begin by transforming the virus so as to make more information available. One way to accomplish this, when an encrypted virus is suspected, is for the anti-virus software to try decrypting the virus body to reveal a larger signature. This process is called <em>X-raying</em><sup><a href="#f4_165" name="b4_165">165</a></sup> For emulation-based anti-virus software, X-raying is a natural side effect of operation.</p>
<p>X-raying may be automated in easier ways than emulation, if some simplifying assumptions are allowed. A virus using simple encryption or a static encryption key (with or without random encryption keys) does not hide the frequency with which encrypted bytes occur; these encryption algorithms preserve the frequency of values that was present in the unencrypted version. Cryptanalysts were taking advantage of frequency analysis to crack codes as early as the 9th century CE,<sup><a href="#f4_166" name="b4_166">166</a></sup> and the same principle applies to virus decryption.<sup><a href="#f4_167" name="b4_167">167</a></sup> Normal, uninfected executables (i.e., the plaintext) tend to have frequently-repeated values, like zeroes. Under the assumptions above, if the most frequently-occurring plaintext value is known, then the most frequently-occurring values in an encrypted version of code (ciphertext) should correspond to it. For example, say that 99 is the most frequent value in plaintext, and 27 is most frequent in the ciphertext. For XOR-based encryption, the key must be 120 (99 xor 27).</p>
<p>Back to verification, once all information is made available, verification may be done in a number of ways:<sup><a href="#f4_168" name="b4_168">168</a></sup></p>
<ul>
<li>Comparing the found virus to a known copy of the virus. Shipping viruses with anti-virus software would be rather unwise, making this option only suitable for use in anti-virus labs.</li>
<li>Using a virus-specific signature, for detection methods that aren't signature-based to begin with. If the initial detection was signature-based, then a longer signature can be used for verification.</li>
 
<li>Checksumming all or part of the suspected virus, and comparing the computed checksum to the known checksum of that virus.</li>
<li>Calling special-purpose code to do the verification, which can be written in a general-purpose or domain-specific programming language.</li>
</ul>
<p>Except for special-purpose code, these are not viable solutions for metamorphic viruses, because they rely on the (unencrypted) virus body being the same for each infection.</p>
<h4><a name="c442"></a>4.4.2 Quarantine</h4>
<p>When a virus is detected in a file, anti-virus software may need to <em>quarantine</em> the infected file, isolating it from the rest of the system.<sup><a href="#f4_169" name="b4_169">169</a></sup> Quarantine is only a temporary measure, and may only be done until the user decides how to handle the file (e.g., giving approval to disinfect it). In other cases, the anti-virus software may have generically detected a virus, but have no idea how to clean it. Here, quarantine may be done until an anti-virus update is available that can deal with the virus that was discovered.</p>
<p>Quarantine can simply be a matter of copying the infected file into a distinct "quarantine" directory, removing the original infected file, and disabling all permission to access the infected file. The problem is that the file permissions may be easily changed by a user, and files may be copied out of a quarantine directory in a virulent form. A good solution limits further spread by accident, or casual copying, but shouldn't be elaborate, as accessing the infected file for disinfection will still be necessary.</p>
<p>One solution is to encrypt quarantined files by some trivial means, like an XOR with a constant. The virus is thereby rendered inert, because an executable file encrypted this way will no longer be runnable, and copying the file does no harm. Also, an encrypted, quarantined file is readily accessible for disinfection.</p>
<p>Another solution is to render the files in the quarantine directory invisible - what can't be seen can't be copied. Anti-virus software can accomplish this feat using file-hiding techniques like stealth viruses and rootkits use. However, this may not be the best idea, as viruses may then try to hide in the quarantine directory, letting the anti-virus software cloak their presence. There could also be issues with false positives produced by virus-like behavior from anti-virus software.<sup><a href="#f4_170" name="b4_170">170</a></sup></p>
<h4><a name="c443"></a>4.4.3 Disinfection</h4>
<p>Disinfection does <em>not</em> mean that an infected system has been restored to its original state, even if the disinfection was successful.<sup><a href="#f4_171" name="b4_171">171</a></sup> In some cases, like overwriting viruses that don't preserve the original contents, disinfection is just not possible.</p>
<p>As with everything else anti-virus, there are different ways to do disinfection:</p>
 
<ul>
<li>Restore infected files from backups. Because everyone meticulously keeps backups of their files, the affected files can be restored to their backed-up state. Some files are meant to change, like data files, and consequently restoring these files may result in data loss. There are also viruses called <em>data diddlers</em>, which are viruses whose payload slowly changes files.<sup><a href="#f4_172" name="b4_172">172</a></sup> By the time a data diddler has been detected, it can have made many subtle changes, and those changed files - not the original ones - would have been caught on the backups.</li>
<li>Virus-specific. Anti-virus software can encode in its database the information necessary to disinfect each known virus. Many viruses share characteristics, like relocating an executable's start address, so in many cases disinfection is a matter of invoking generic disinfection subroutines with the correct parameters.<sup><a href="#f4_173" name="b4_173">173</a></sup>
<p>Virus-specific information needed for disinfection can be derived automatically by anti-virus researchers, at least for relatively simple viruses. Goat files with different properties can be deliberately infected, and the resulting corpus of infected files can be compared to the originals. This comparison can reveal where a virus puts itself in an infected file, how the virus gets control, and where any relocated bytes from the original file may be found.<sup><a href="#f4_174" name="b4_174">174</a></sup> This can be likened to a chosen-plaintext attack in cryptography.<sup><a href="#f4_175" name="b4_175">175</a></sup></p></li>
<li>Virus-behavior-specific. Rather than customize disinfection to individual viruses, disinfection can be attempted based on assumptions about viral behavior. For prepending viruses, or appenders that gain control by modifying the program header, disinfection is a matter of: restoring the original program header; moving the original file contents back to their original location.
<p>Anti-virus software can store some information in advance for each executable file on an uninfected system which can be used later for disinfection.<sup><a href="#f4_176" name="b4_176">176</a></sup> The necessary information to store is the program header, thefilelength, and a checksum of the executable file's contents sans header. This disinfection technique integrates well with integrity checkers, since integrity checkers store roughly the same information anyway.</p>
<p>For an infected file, the saved program header can be immediately restored. The tricky part is determining where the originalfilecontents reside, because a prepending virus may have shifted them from their original location in the file. The disinfector knows the checksum of the original file contents, however - it can iterate over the infected file, checksumming the same number of bytes as were used for the original checksum (the uninfected file length minus the header length). If the new checksum matches the stored checksum, then the original file contents have been located and can be
 
restored. This is shown in Figure 4.14. The number of checksum iterations needed in the worst case is equivalent to the added length of the virus, the difference between the lengths of the infected and uninfected files.</p>
<div align="center">
<img src="img/mja01/fig4e.gif" alt="Figure 4.14. Disinfection using checksums"/>
<p><strong>Figure 4.14. Disinfection using checksums</strong></p>
</div>
<p>This method naturally enjoys several built-in safety checks which guard against situations where this disinfection method is inapplicable. The computed virus length can be checked for too-small, or even negative, values. Failure to match the stored checksum in the prescribed number of iterations also flags inapplicability.</p></li>
<li>Using the virus' code:
<ul>
<li>Stealth viruses happily supply the uninfected contents of a file. Anti-virus software can exploit this to disinfect a stealth virus by simply asking the virus for the file's contents.<sup><a href="#f4_177" name="b4_177">177</a></sup></li>
<li><em>Generic disinfection</em> methods assume that the virus will eventually restore and jump to the code it infected. A generic disinfector executes the virus under controlled conditions, watching for the original code to be restored by the virus on the disinfector's behalf.<sup><a href="#f4_178" name="b4_178">178</a></sup>
<ul>
<li>One anti-virus system stepped through the viral code in a real, not emulated, environment. The system ran harmless-looking instructions, skipping potentially harmful ones until the virus jumped back to the original code. This turned out to be a dangerous approach, and virus writers eventually found ways to trick the disinfector.<sup><a href="#f4_179" name="b4_179">179</a></sup></li>
<li>The infected code can be emulated until the virus jumps to the original code. The obvious way to do this is to have the emulator's controller heuristically watch for the jump.
 
<p>A minor variant allows anti-virus disinfection code to run <em>inside</em> the emulator along with the infected code. The disinfection code can then be in native code and yet be portable (subject to the emulator's own portability). As needed, the virus' code can be called by the disinfection code, and the emulator can sport an interface by which the in-emulator disinfection code can export a clean version of the file.</p></li>
</ul></li>
</ul></li>
</ul>
<p>Cruder disinfection can be done by zeroing out the virus, or simply deleting the infectedfile.<sup><a href="#f4_180" name="b4_180">180</a></sup> This will eradicate the virus, but won't restore the system at all.<sup><a href="#f4_7" name="b4_7">7</a></sup></p>
<h3><a name="c45"></a>4.5 Virus Databases and Virus Description Languages</h3>
<p>Up to now, the existence of a virus database for anti-virus software has been assumed but not discussed. Conceptually, a virus database is a database containing records, one for every known vims. When a virus is detected using a known-virus detection method, one side effect is to produce a virus identifier. This virus identifier may not be the virus' name, or even be human-readable, but can be used to index into the virus database and find the record corresponding to the found virus.<sup><a href="#f4_181" name="b4_181">181</a></sup></p>
<p>A virus record will contain all the information that the anti-virus software requires to handle the virus. This may include:</p>
<ul>
<li>A printable name for the virus, to display for the user.</li>
<li>Verification data for the virus. Again, a copy of the entire virus would not be present; the last section discussed other ways to perform verification.</li>
<li>Disinfection instructions for the virus.</li>
</ul>
<p>Any virus signatures stored in the database must be carefully handled. Why? Figure 4.15 illustrates a potential problem with virus databases, when more than one anti-virus program is present on a system. If virus signatures are stored in an unencrypted form, then one anti-virus program may declare another vendor's virus database to be infected, because it can find a wealth of virus signatures in the database file! The safest strategy is to encrypt stored virus signatures, and never to decrypt them. Instead, the input data being checked for a signature can be similarly encrypted, and the signature check can compare the encrypted forms.<sup><a href="#f4_182" name="b4_182">182</a></sup></p>
<p>As new viruses are discovered, an anti-virus vendor will update their virus database, and all their users will require an updated copy of the virus database in order to be properly protected against the latest threats. This raises a number of questions:</p>
 
<div align="center">
<img src="img/mja01/fig4f.gif" alt="Figure 4.15. Problem with unencrypted virus databases"/>
<p><strong>Figure 4.15. Problem with unencrypted virus databases</strong></p>
</div>
<ul>
<li>How is a user informed of updates? The typical model is that users periodically poll the anti-virus vendor for updates. The polling is done automatically by the anti-virus software, although a user can manually force an update to occur. Another model is referred to as a <em>push model</em>, where the anti-virus vendor "pushes out" updates to users as soon as they are available. Many vendors use the polling model, but will email alerts about new threats to users upon request, permitting them to make an informed choice about updating.</li>
<li>Should updates be manual or automatic? Automatic updates have the potential to provide current known-virus protection for users as soon as possible. Currency aside, some machines are not aggressively maintained by their users. Automatic updates are not always the best choice, however. Anti-virus software, like any software, can have bugs. It is rare, but possible, for a database update to cause substantial headaches for users because of this. In one case, a buggy update caused the networks of some Japanese railway, subway, and media organizations to be inaccessible for hours.<sup><a href="#f4_183" name="b4_183">183</a></sup></li>
<li>How often should updates be done? Frequency of updates is in part a reflection of the rate at which new threats appear. Once upon a time, monthly updates would have been sufficient; now, weekly and daily updates may not be often enough.</li>
<li>How should updates be distributed? Electronic distribution of updates, especially via the Internet, is the only viable means to disseminate frequent updates. This means that anti-virus vendors must have infrastructures for
 
distributing updates that are able to withstand heavy load - a highly-publicized threat may cause many users to update at the same time.
<p>The update process is an attractive target for attackers. It is something that is done often by users, and compromising updates would create a huge pool of vulnerable machines. The compromise may occur in a number of ways:</p>
<ul>
<li>The vendor's machines that distribute the update may be attacked.</li>
<li>An update may be compromised at the vendor <em>before</em> reaching the distribution machines. Anti-virus vendors are amply protected internally from malware, but an inside threat is always possible.</li>
<li>A user machine may be spoofed, so that it connects to an attacker's machine instead of the vendor's machines.</li>
<li>A "man-in-the-middle" attack may be mounted, where an attacker is able to intercept communications between the user and vendor. An attacker may modify the real update, or inject their own update into the communications channel.</li>
</ul>
<p>There is also the practical matter of what form the update will take. Transmitting a fresh copy of the entire virus database is not feasible due to the bandwidth demands it would place on the vendor's update infrastructure, not to mention the comparatively limited bandwidth that many users have.</p>
<p>The virus database will have a relatively small number of changes between updates, so instead of sending the entire database, a vendor can just send the changes to the database. These changes are sometimes called <em>deltas</em><sup><a href="#f4_184" name="b4_184">184</a></sup> Furthermore, these deltas can be compressed to try and make them smaller still. Downloaded deltas should be verified to protect against attacks and transmission errors.</p></li>
</ul>
<p>The update mechanism can also be used to update the anti-virus engine itself, not just the virus database.<sup><a href="#f4_185" name="b4_185">185</a></sup> This may be necessary tofixbugs, or add functionality required to detect new viruses. Known-virus scanners will need their data structures updated with the latest signatures as well.</p>
<p>Clearly, the information in the virus database and other updates from an anti-virus vendors must come from someplace. Anti-virus vendors often have an in-house <em>virus description language</em>, a domain-specific language designed to describe viruses, and how to detect, verify, and disinfect each one.<sup><a href="#f4_186" name="b4_186">186</a></sup> Two examples are given in Figure 4.16. Anti-virus researchers create descriptions such as these, and a compiler for the virus description language translates them into the virus database format.</p>
<p>Domain-specific languages tend to be very good at describing things in their domain, but not very good for general use. Virus description languages can have escape mechanisms to call code written in a general-purpose language,
 
code which is compiled and either interpreted or run natively.<sup><a href="#f4_187" name="b4_187">187</a></sup> This allows special-purpose code to be written for detection, verification, or disinfection.</p>
<pre>
	                  <strong>VERV description</strong>
	VIRUS	example			; short alias for virus
	NAME	An example virus	; full virus name
	LOAD	S-EXE 0000 0500		; load bytes 0-500 from .EXE entry point
	DEXOR1	0100 0500 0035 0000	; XOR bytes 100-500 with key at byte 35
	ZERO	0035 0001		; set key at byte 35 to zero
	CODE	0000 0500 4a4f484e	; is checksum of bytes 0-500 = 4a4f484e?
	                  <strong>CVDL description</strong>
	; looks for two words in virus' data
	:example,"painfully", AND "contrived",#
</pre>
<div align="center">
<p><strong>Figure 4.16. Example virus descriptions</strong></p>
</div>
<p>Special-purpose code can be used to direct the entire virus detection, instead of only being invoked when needed. For example, for viruses which have multiple entry points, special-purpose code can tell a scanner what locations it should scan.<sup><a href="#f4_188" name="b4_188">188</a></sup></p>
<h3><a name="c46"></a>4.6 Short Subjects</h3>
<p>To conclude this chapter, a veritable potpourri of short topics: anti-stealth techniques, macro virus detection, and the role of compiler optimization in anti-virus detection.</p>
<h4><a name="c461"></a>4.6.1 Anti-Stealth Techniques</h4>
<p>One assumption made up to this point is that anti-virus software sees an accurate picture of the data being checked for viruses. But what if a virus is using stealth to hide?</p>
<p><em>Anti-stealth</em> techniques are countermeasures used against stealth viruses. There are two options:</p>
<ol>
<li>Detect and disable the stealth mechanism. For example, calls to the operating system can be examined to make sure they're going to the "right" place. Section 5.5 looks at this in more depth.</li>
<li>Bypass the usual mechanisms to call the operating system in favor of unsubvertible ones. For Unix, this would mean that anti-virus software only uses direct system calls (assuming, of course, that the operating system kernel is secure); for MS-DOS systems, this could mean making direct BIOS calls to get disk data.</li>
</ol>
 
<h4><a name="c462"></a>4.6.2 Macro Virus Detection</h4>
<p>Macro viruses present some interesting problems for anti-virus software.<sup><a href="#f4_189" name="b4_189">189</a></sup> Macros are in source form, and are easy to change and allow a lot of freedom with formatting. Macro language interpreters can be extremely robust in terms of bullishly continuing execution in the face of errors; a missing or damaged macro won't necessarily keep a macro virus from operating. Some specific problems with macro viruses:</p>
<ul>
<li>Accidental or deliberate changes to a macro virus, even to its formatting, may create a new macro virus. This may even happen automatically: Microsoft Word converts documents from one version of Word to another, and this conversion has created new macro viruses in the process.</li>
<li>Bugs in macro virus propagation, or incomplete disinfection of a macro virus, can create new macro virus variants. Anti-virus software can accidentally create viruses if it's not careful!</li>
<li>A macro virus can accidentally "snatch" macros from an environment it infects, becoming a new virus. In one case, a Word macro virus even swiped two macros from Microsoft's software that protects against macro viruses.<sup><a href="#f4_190" name="b4_190">190</a></sup></li>
</ul>
<p>Macro viruses, despite these problems, have one redeeming feature.<sup><a href="#f4_191" name="b4_191">191</a></sup> Macros operate in a restricted domain, so anti-virus detection can determine what constitutes "normal" behavior with a very high degree of confidence. This limits the number of false positives that might otherwise be incurred by detection.</p>
<p>All of the same ideas have been trotted out for macro viruses as have been used for other types of virus, including signature scanning, static heuristics, behavior blocking, and emulation.<sup><a href="#f4_192" name="b4_192">192</a></sup> Due to variability in formatting, methods looking for static signatures are facilitated by removing whitespace and comments, or translating it into some equivalent canonical form first.<sup><a href="#f4_8" name="b4_8">8</a></sup> A similar need for canonicalization arises from macro languages which aren't case sensitive, where foo, FOO, and Foo would all refer to the same variable.<sup><a href="#f4_193" name="b4_193">193</a></sup></p>
<p>More systemic approaches to macro virus detection periodically examine documents on a system, and build a database of the documents and their properties.<sup><a href="#f4_194" name="b4_194">194</a></sup> In particular, macros in documents can be tracked; the sudden appearance of macros in a document, a change to known macros in a document, or a number of documents with the same changes to their macros are all signals that a macro virus may be active.</p>
<p>Macro viruses have not been parasitic, meaning they have not inserted viral code into legitimate code, but have acted more like companion viruses.<sup><a href="#f4_195" name="b4_195">195</a></sup> (Nothing prevents macro viruses from being parasitic; it's just slightly more effort to implement.) Disinfection strategies for macro viruses have consequently tended towards deletion-based approaches:</p>
 
<ul>
<li>Delete <em>all</em> macros in the infected document, including any unfortunate, legitimate user macros.</li>
<li>Delete macros known to be associated with the virus found. This requires a known-macro-virus database.</li>
<li>For macro viruses detected using heuristics, remove the macros found to contain the offending behavior.<sup><a href="#f4_196" name="b4_196">196</a></sup></li>
<li>Emulator-based detection can track the macros seen to be used by the macro virus and delete them.<sup><a href="#f4_197" name="b4_197">197</a></sup></li>
</ul>
<p>Applications supporting macros treat macros in a much more guarded fashion than they once did, and macro viruses are a much less prominent threat than they have been as a result.<sup><a href="#f4_198" name="b4_198">198</a></sup></p>
<h4><a name="c463"></a>4.6.3 Compiler Optimization</h4>
<p>Compiler techniques have natural overlaps with anti-virus detection. For example, some scanning algorithms are applied to match patterns in trees, for code generation;<sup><a href="#f4_199" name="b4_199">199</a></sup> scanning and parsing are needed for macro virus detection; work on efficient interpretation is applicable to emulation, and interpreting special-purpose code in the anti-virus engine.</p>
<p>One suggestion which rears its head occasionally is the possibility of using compiler optimizations for detection of viruses. Given that a number of compiler optimization techniques perform some sophisticated analyses, it isn't surprising to consider applying them to the problem of virus detection:</p>
<ul>
<li><em>Constant propagation</em> replaces variables which are defined as constants with the constants themselves. This increases the information available about code being analyzed, and facilitates other optimizations. With the code below, constant propagation yields the name of the file being opened:
<pre class="source">
file = "c:\autoexec.bat"		file = "c:\autoexec.bat"
...				=>	...
f = open(file)				f = open("c:\autoexec.bat")
</pre>
<p>Constant propagation has been proposed to assist in the static analysis of macro viruses.<sup><a href="#f4_200" name="b4_200">200</a></sup></p></li>
<li><em>Dead code</em> is code which is executed, but the results are never used. In the code below, for example, thefirstassignment to r1 is dead, because its value is not used before r1 is redefined:
<pre class="source">
r1 = 123
r1 = r2 + 7
</pre>
 
<p>Polymorphic viruses tend to exhibit a lot of dead code - more than 25% - especially when compared to non-viral code, so dead code analysis can make a useful heuristic to help with polymorphic virus detection.<sup><a href="#f4_201" name="b4_201">201</a></sup></p></li>
</ul>
<p>However, some problems loom. Compiler optimization algorithms are not known for efficiency, with the exception of algorithms designed specifically for use in dynamic, orjust-in-time, compilers. Such algorithms tend to trade speed increases for decreases in accuracy, though. It is often possible to concoct programs which exercise the worst case performance of optimization algorithms, or programs which make the task of precise analysis undecidable. Virus writers will undoubtedly take advantage of this if anti-virus' use of compiler optimization becomes widespread.</p>
 
<p><strong>Notes for Chapter 4</strong></p>
<p><a name="f4_1" href="#b4_1">1</a> And the rest of the quote: 'Unfortunately, this program must identify <em>every</em> (or nearly so) program as infected, whether it is or not!' [299, page 258]</p>
<p><a name="f4_2" href="#b4_2">2</a> Until the anti-virus signatures are updated or files are accessed from a non-network source, at which point a full on-demand scan would be indicated.</p>
<p><a name="f4_3" href="#b4_3">3</a> Obligatory Knuth citation: [168]. He says that the pronunciation of "trie" is "try."</p>
<p><a name="f4_4" href="#b4_4">4</a> Navarro and Raffinot [227]. "Very small" means 4-8 values, whereas scanning inputs will have 256 possible values for each input byte.</p>
<p><a name="f4_5" href="#b4_5">5</a> Unless the scan would take less effort than deciding whether or not to scan in the first place!</p>
<p><a name="f4_6" href="#b4_6">6</a> Although if this is done incorrectly, it opens the door for a brute-force attack on the session key.</p>
<p><a name="f4_7" href="#b4_7">7</a> With the exception of simple companion viruses.</p>
<p><a name="f4_8" href="#b4_8">8</a> This is obvious to compiler writers, who've been handling whitespace (and lack thereof) since compiling Fortran in the 1950s, but seemingly not so for patent examiners: Kuo [175].</p>
<p><a name="f4_100" href="#b4_100">100</a> Harley et al. [137] was used for this introductory section.</p>
<p><a name="f4_101" href="#b4_101">101</a> Cohen [74]. Harrison et al. [138] make some interesting follow-on points regarding Cohen's proof and Turing-compatibility.</p>
<p><a name="f4_102" href="#b4_102">102</a> Muttik [214].</p>
<p><a name="f4_103" href="#b4_103">103</a> Harley etal. [137].</p>
<p><a name="f4_104" href="#b4_104">104</a> Mall&eacute;n-Fullerton [192] considers the case of wildcards that match one byte; Bontchev [46] takes a more general view.</p>
<p><a name="f4_105" href="#b4_105">105</a> Not surprisingly, Aho and Corasick [5]. The version of the algorithm given here is a slight reformulation of the first version of the algorithm that Aho and Corasick give in their paper.</p>
<p><a name="f4_106" href="#b4_106">106</a> Tuck et al. [324] discuss many of these implementation choices for Aho-Corasick.</p>
<p><a name="f4_107" href="#b4_107">107</a> The version here is an much-abstracted form of Veldman's algorithm. The unadulterated version is in Bontchev [46].</p>
<p><a name="f4_108" href="#b4_108">108</a> Kumar and Spafford [174] adapted Aho-Corasick for wildcards.</p>
<p><a name="f4_109" href="#b4_109">109</a> The original algorithm is described in Wu and Manber [349], and is very general; the version here is a simplification along the lines of [227, 324].</p>
<p><a name="f4_110" href="#b4_110">110</a> This section is based on [96].</p>
<p><a name="f4_111" href="#b4_111">111</a> This item is based on Bontchev [46]. Top and tail scanning, entry point scanning, and size-based scanning assumptions are also in Nachenberg [217].</p>
<p><a name="f4_112" href="#b4_112">112</a> Nachenberg [217].</p>
 
<p><a name="f4_113" href="#b4_113">113</a> Carr[54].</p>
<p><a name="f4_114" href="#b4_114">114</a> Unless otherwise noted, this item is based on Flint and Hughes [111].</p>
<p><a name="f4_115" href="#b4_115">115</a> Carr[54].</p>
<p><a name="f4_116" href="#b4_116">116</a> This item is based on Nachenberg [215].</p>
<p><a name="f4_117" href="#b4_117">117</a> Mallen-Fullerton [192] talks about the signature length tradeoff.</p>
<p><a name="f4_118" href="#b4_118">118</a> Muttik[214].</p>
<p><a name="f4_119" href="#b4_119">119</a> For example, Navarro and Tarhio [228].</p>
<p><a name="f4_120" href="#b4_120">120</a> For example, Pennello [245].</p>
<p><a name="f4_121" href="#b4_121">121</a> Bentley [34].</p>
<p><a name="f4_122" href="#b4_122">122</a> Gryaznov [133], Symantec [307], and Zenkin [354].</p>
<p><a name="f4_123" href="#b4_123">123</a> Gryaznov [133].</p>
<p><a name="f4_124" href="#b4_124">124</a> Symantec [307], who apply this division to static and dynamic heuristics.</p>
<p><a name="f4_125" href="#b4_125">125</a> The "booster" and "stopper" terminology is from Nachenberg [221], who uses them in the context of emulation.</p>
<p><a name="f4_126" href="#b4_126">126</a> Gryaznov [133].</p>
<p><a name="f4_127" href="#b4_127">127</a> Nachenberg [221].</p>
<p><a name="f4_128" href="#b4_128">128</a> Ludwig [187]; detristan et al. [89] look at spectrum analysis in the context of intrusion detection systems. Muttik [214] talks about opcode frequency analysis too. Weber et al. [342] use instruction frequencies to try and spot hand-written assembly code, on the premise that more viruses are written in assembly code than high-level languages.</p>
<p><a name="f4_129" href="#b4_129">129</a> See [318, 307, 283], respectively.</p>
<p><a name="f4_130" href="#b4_130">130</a> Tesauroetal. [318].</p>
<p><a name="f4_131" href="#b4_131">131</a> Kephartetal. [163].</p>
<p><a name="f4_132" href="#b4_132">132</a> This section is based on Bontchev [38].</p>
<p><a name="f4_133" href="#b4_133">133</a> Bontchev [46].</p>
<p><a name="f4_134" href="#b4_134">134</a> Schmehl [278].</p>
<p><a name="f4_135" href="#b4_135">135</a> The first two are from Esponda et al. [101].</p>
<p><a name="f4_136" href="#b4_136">136</a> Like the Spanish Inquisition. No one ever expects them. Oh, right: Hofmeyretal. [143].</p>
<p><a name="f4_137" href="#b4_137">137</a> Nachenberg [216].</p>
<p><a name="f4_138" href="#b4_138">138</a> Hofmeyretal. [143].</p>
<p><a name="f4_139" href="#b4_139">139</a> Ford and Michalske [113], who also supply the browser story.</p>
<p><a name="f4_140" href="#b4_140">140</a> Ford and Thompson [114].</p>
<p><a name="f4_141" href="#b4_141">141</a> El Far et al. [98] look at a related idea: being able to recall unread messages from a remote machine soon after transmission.</p>
<p><a name="f4_142" href="#b4_142">142</a> Jordan [154] argues this for emulation with dynamic heuristics, but of the course the argument applies equally well to behavior blockers.</p>
 
<p><a name="f4_143" href="#b4_143">143</a> Nachenberg [217].</p>
<p><a name="f4_144" href="#b4_144">144</a> These first two heuristics are from Nachenberg [220], the third from [221].</p>
<p><a name="f4_145" href="#b4_145">145</a> Nachenberg [222].</p>
<p><a name="f4_146" href="#b4_146">146</a> Natvig [225] and Szor [308].</p>
<p><a name="f4_147" href="#b4_147">147</a> Based on Veldman [332], who had a four-part organization.</p>
<p><a name="f4_148" href="#b4_148">148</a> This item is based on Natvig [225].</p>
<p><a name="f4_149" href="#b4_149">149</a> Nachenberg [220].</p>
<p><a name="f4_150" href="#b4_150">150</a> Nachenberg [222].</p>
<p><a name="f4_151" href="#b4_151">151</a> Nachenberg [222].</p>
<p><a name="f4_152" href="#b4_152">152</a> Nachenberg [223].</p>
<p><a name="f4_153" href="#b4_153">153</a> Chambers [59].</p>
<p><a name="f4_154" href="#b4_154">154</a> Nachenberg [219].</p>
<p><a name="f4_155" href="#b4_155">155</a> Chambers [59] and Natvig [225].</p>
<p><a name="f4_156" href="#b4_156">156</a> Chambers [59] and Nachenberg [220].</p>
<p><a name="f4_157" href="#b4_157">157</a> Nachenberg [220].</p>
<p><a name="f4_158" href="#b4_158">158</a> Natvig [225].</p>
<p><a name="f4_159" href="#b4_159">159</a> Nachenberg [221].</p>
<p><a name="f4_160" href="#b4_160">160</a> This item is based on Nachenberg [223].</p>
<p><a name="f4_161" href="#b4_161">161</a> Pros and cons from [38, 354].</p>
<p><a name="f4_162" href="#b4_162">162</a> [Dis]advantages of behavior blockers are from Zenkin [354]. A mostly-overlapping set of disadvantages is in Nachenberg [216].</p>
<p><a name="f4_163" href="#b4_163">163</a> Veldman [332] mentions emulator advantages and disadvantages.</p>
<p><a name="f4_164" href="#b4_164">164</a> Chess [64] points this out for verification.</p>
<p><a name="f4_165" href="#b4_165">165</a> Nachenberg [217]; also Perriot and Ferrie [248], who argue the use of X-raying for virus detection.</p>
<p><a name="f4_166" href="#b4_166">166</a> Al-Kadi [7].</p>
<p><a name="f4_167" href="#b4_167">167</a> Itshaketal. [151].</p>
<p><a name="f4_168" href="#b4_168">168</a> All but the second are from Chess [64].</p>
<p><a name="f4_169" href="#b4_169">169</a> This section is based on Templeton [317].</p>
<p><a name="f4_170" href="#b4_170">170</a> This solution, and one of the attendant problems, was suggested by [306].</p>
<p><a name="f4_171" href="#b4_171">171</a> Harleyetal. [137].</p>
<p><a name="f4_172" href="#b4_172">172</a> Bontchev [46].</p>
<p><a name="f4_173" href="#b4_173">173</a> Nachenberg [218].</p>
<p><a name="f4_174" href="#b4_174">174</a> Chess et al. [66].</p>
<p><a name="f4_175" href="#b4_175">175</a> Schneier [279].</p>
<p><a name="f4_176" href="#b4_176">176</a> This method is from Mann [193].</p>
<p><a name="f4_177" href="#b4_177">177</a> Bontchev [46].</p>
 
<p><a name="f4_178" href="#b4_178">178</a> Szor [308].</p>
<p><a name="f4_179" href="#b4_179">179</a> This, and the "minor variant" below, are from Nachenberg [218].</p>
<p><a name="f4_180" href="#b4_180">180</a> Templeton[317].</p>
<p><a name="f4_181" href="#b4_181">181</a> From Kouznetsov et al., along with the virus record contents below [170].</p>
<p><a name="f4_182" href="#b4_182">182</a> Bontchev [46]. Carr [54] mentions a virus database which is compressed and encrypted.</p>
<p><a name="f4_183" href="#b4_183">183</a> Japan Times [153].</p>
<p><a name="f4_184" href="#b4_184">184</a> This, and the bandwidth problem, are from Kouznetsov and Ushakov [170].</p>
<p><a name="f4_185" href="#b4_185">185</a> Paketal. [238].</p>
<p><a name="f4_186" href="#b4_186">186</a> For examples, see [54, 64, 238, 251, 252, 259]. The examples in Figure 4.16 use the descriptions of VERY [64] and CVDL [251, 252, 259].</p>
<p><a name="f4_187" href="#b4_187">187</a> Nachenberg [219] and Pak et al. [238].</p>
<p><a name="f4_188" href="#b4_188">188</a> Nachenberg [219].</p>
<p><a name="f4_189" href="#b4_189">189</a> These problems are from Bontchev [43].</p>
<p><a name="f4_190" href="#b4_190">190</a> See [42, 200].</p>
<p><a name="f4_191" href="#b4_191">191</a> Zenkin [354].</p>
<p><a name="f4_192" href="#b4_192">192</a> See [61, 175] (signature scanning), [61,169] (static heuristics), [341, 354] (behavior blocking), and [69] (emulation).</p>
<p><a name="f4_193" href="#b4_193">193</a> Bontchev [43].</p>
<p><a name="f4_194" href="#b4_194">194</a> Chess etal. [65].</p>
<p><a name="f4_195" href="#b4_195">195</a> Bontchev [43], who also gives the first three disinfection methods below.</p>
<p><a name="f4_196" href="#b4_196">196</a> Chen et al. [61], who also proposed cleaning within macros by replacing detected macro virus instructions with non-viral instructions.</p>
<p><a name="f4_197" href="#b4_197">197</a> Chi [69].</p>
<p><a name="f4_198" href="#b4_198">198</a> Bontchev [45] opines on this at length.</p>
<p><a name="f4_199" href="#b4_199">199</a> Ahoetal. [6].</p>
<p><a name="f4_200" href="#b4_200">200</a> Ko [169].</p>
<p><a name="f4_201" href="#b4_201">201</a> Perriot [247], who also discusses lots of other optimizations and their application to polymorphic virus detection.</p>
 
 
<h2><a name="c5"></a>Chapter 5 Anti-anti-virus techniques</h2>
<p>All viruses self-replicate, but not all viruses act in an openly hostile way towards anti-virus software. Anti-anti-virus techniques are techniques used by viruses which do one of three things:</p>
<ol>
<li>Aggressively attack anti-virus software.</li>
<li>Try to make analysis difficult for anti-virus researchers.</li>
<li>Try to avoid being detected by anti-virus software, using knowledge of how anti-virus software works.</li>
</ol>
<p>The lack of clear definitions in this field comes into play again: arguably, any of the encryption methods described in Chapter 3 is an attempt to achieve the latter two goals.</p>
<p>To further confuse matters, "anti-anti-virus" is different from "anti-virus virus." Anti-virus virus has been used variously to describe: a virus that attacks other viruses; anti-virus software that propagates itself through viral means; software which drops viruses on a machine, then offers to sell "anti-virus" software to remove the viruses it put there.<sup><a href="#f5_100" name="b5_100">100</a></sup></p>
<p>Back to the relatively well-defined anti-anti-virus, this includes seven techniques: retroviruses, entry point obfuscation, anti-emulation, armoring, tunneling, integrity checker attacks, and avoidance.</p>
<h3><a name="c51"></a>5.1 Retroviruses</h3>
<p>A virus that actively tries to disable anti-virus software running on an infected machine is referred to as a <em>retrovirus</em>.<sup><a href="#f5_1" name="b5_1">1</a></sup> This is a generic term for a virus employing this type of active defense, and doesn't imply that any particular technique is used.</p>
 
<p>Having said that, a common retrovirus technique is for a virus to carry a list with it of process names used by anti-virus products. When it infects a machine, a retrovirus will enumerate the currently-running processes, and kill off any processes which match one of the names in the list. A partial list is shown below:<sup><a href="#f5_2" name="b5_2">2</a></sup></p>
<ul>
<li>Avgw.exe</li>
<li>F-Prot.exe</li>
<li>Navw32.exe</li>
<li>Regedit.exe</li>
<li>Scan32.exe</li>
<li>Zonealarm.exe</li>
</ul>
<p>It's not unusual to see lists like this appear in malware analyses. This particular list not only includes anti-virus process names, but also other security products like firewalls, and system utilities like the Windows Registry editor.</p>
<p>A more aggressive retrovirus can target the antivirus software on disk as well as in memory, so that antivirus protection is disabled even after the infected system is rebooted. For example, Ganda kills processes that appear to be anti-virus software, using the above list-based method; it also examines the programs run at system startup, looking for anti-virus software using the same list of names. If Ganda finds anti-virus software during this examination, it locates the executable image on disk and replaces the first instruction with a "return" instruction. This causes the anti-virus software to exit immediately after starting.<sup><a href="#f5_101" name="b5_101">101</a></sup></p>
<p>The above methods have one major drawback: by killing off the anti-virus software, they leave a telltale sign. An alert user might notice the absence of the anti-virus icon.<sup><a href="#f5_3" name="b5_3">3</a></sup> For the purposes of retroviruses, it's sufficient to render anti-virus software incapable of full operation, disabling it rather than killing it off completely.</p>
<p>How can this be done? One approach would be to try and starve anti-virus software of CPU time. A retrovirus with appropriate permission could reduce the priority of anti-virus software to the minimum value possible, to (ideally) keep it from running.<sup><a href="#f5_102" name="b5_102">102</a></sup> Most operating system schedulers have a mechanism to boost the priority of CPU-starved processes,<sup><a href="#f5_4" name="b5_4">4</a></sup> however, so attacking anti-virus software by reducing process priority is unlikely to be very effective. Another way to disable anti-virus software is to adjust the way a computer looks up hostname information on the network, to prevent anti-virus software from being able to connect to the anti-virus company's servers and update its database.</p>
 
<h3><a name="c52"></a>5.2 Entry Point Obfuscation</h3>
<p>Modifying an executable's start address, or the code at the original start address, constitutes extremely suspicious behavior for anti-virus heuristics. A virus can try to get control elsewhere instead; this is called <em>entry point obfuscation</em> or EPO.</p>
<p>Picking a random location in an executable to gain control isn't a brilliant survival strategy, because a infrequently-executed error handler may be chosen as easily as a frequently-executed loop. A more controlled selection of a location is better. Simile and Ganda both use EPO, and look for calls to the ExitProcess API function; these calls are overwritten to point to the viral code instead.<sup><a href="#f5_103" name="b5_103">103</a></sup> Because ExitProcess is called when a program wants to quit, these viruses get control upon the infected code's exit.</p>
<p>Locations for EPO may also be chosen by looking for known code sequences in executables.<sup><a href="#f5_104" name="b5_104">104</a></sup> Compilers for high-level languages emit repetitive code, and a virus can search the executable for such repetitive instruction sequences to overwrite with a jump to the virus' code. As the sequence being replaced is known, the virus can always restore and run the original instructions later.</p>
<h3><a name="c53"></a>5.3 Anti-Emulation</h3>
<p>Techniques to avoid anti-virus emulators can be divided into three categories, based on whether they try to outlast, outsmart, or overextend the emulator. The fix for the latter two categories is just to improve the emulator, although this tends to come at the cost of increased emulator complexity.</p>
<h4><a name="c531"></a>5.3.1 Outlast</h4>
<p>Except in an anti-virus lab, the amount of time an emulator has to spend running a program is strictly limited by the user's patience.<sup><a href="#f5_105" name="b5_105">105</a></sup> How can a virus evade detection long enough for the emulator to give up?</p>
<ul>
<li>Code can be added to the virus which does nothing, wasting time until the emulator quits - then the real viral code can run.<sup><a href="#f5_106" name="b5_106">106</a></sup> The emulator may look for obvious junk code, so the code would need to be disguised as a valid operation, like computing the first <em>n</em> digits of <img src="/img/cache/4f08e3dba63dc6d40b22952c7a9dac6d.gif" alt="\pi" valign="middle"/>.</li>
<li>A virus need not replicate every time it's run. It can act benign nine times out of every ten, for example, in a statistical ploy to appear harmless 90% of the time. If the anti-virus software is using the performance-improving tricks in Section 4.2.2.3, then the virus might get lucky and have an infected program be marked as clean when emulated; a later execution of that infected program would give the virus a free hand.</li>
<li>Emulators operate under the assumption that viral code will intercept execution at or near the start of an infected program. Entry point obfuscation,
 
besides an anti-heuristic measure, can also be considered an anti-emulation technique, because it can delay execution of viral code.</li>
</ul>
<h4><a name="c532"></a>5.3.2 Outsmart</h4>
<p>An alternative to waiting until emulator scrutiny is over is to restructure the viral code so that it doesn't look suspicious when it's emulated. The decryptor code could be spread all over, instead of appearing as one tight loop; multiple decryption passes could be used to decrypt the virus body.<sup><a href="#f5_107" name="b5_107">107</a></sup> Most techniques for avoiding dynamic heuristics would be candidates here.</p>
<h4><a name="c533"></a>5.3.3 Overextend</h4>
<p>A virus can push the boundaries of an emulator in an effort to either crash the emulator - not likely for a mature anti-virus emulator - or detect that the virus is being run under emulation, so that the virus can take appropriate (in)action. Here are some ways to try and overextend an emulator:</p>
<ul>
<li>Some CPUs, especially CISC ones, have undocumented instructions.<sup><a href="#f5_108" name="b5_108">108</a></sup> A virus can use these instructions in the hopes that an emulator will not support them, and thus give itself away.</li>
<li>The same idea can be applied to bugs that a CPU may exhibit, or differences between different processor implementations. The emulator may need to track results that are processor-dependent to correctly emulate such a virus.</li>
<li>The emulator's memory system can be exercised by trying to access unusual locations that, on a real machine, might cause a memory fault or access some memory-mapped I/O.<sup><a href="#f5_109" name="b5_109">109</a></sup> A cruder attack may simply try to exhaust an emulator's memory by accessing lots of locations. Memory system attacks are not particularly effective, however.</li>
<li>Assuming emulators return fixed values for calls to many operating system and other API functions, a virus can check for differences between two calls of the same function where a change <em>should</em> occur. For example, a virus could ask for the current time twice, assuming an emulated environment will return the same value both times.</li>
<li>An emulator may be taxed by importing obscure, but standard, libraries in case the emulator doesn't handle all of them.<sup><a href="#f5_110" name="b5_110">110</a></sup></li>
<li>External resources are next to impossible to properly emulate. A virus could take advantage of this by looking for external things like web pages.<sup><a href="#f5_111" name="b5_111">111</a></sup></li>
<li>Finally, checks specific to certain emulators can be performed. An emulator may only support a well-known set of I/O devices, or may have an interface to the "outside world" which can be tested for.<sup><a href="#f5_5" name="b5_5">5</a></sup></li>
</ul>
 
<h3><a name="c54"></a>5.4 Armoring</h3>
<p>A virus is said to be <em>armored</em> if it uses techniques which try to make analysis hard for anti-virus researchers. In particular, <em>anti-debugging</em> methods can be used against dynamic analysis, and <em>anti-disassembly</em> methods can be used to slow static analysis. Interestingly, these techniques have been in use since at least the 1980s to guard against software piracy.<sup><a href="#f5_112" name="b5_112">112</a></sup></p>
<h4><a name="c541"></a>5.4.1 Anti-Debugging</h4>
<p>Making dynamic analysis a painful process for humans is the realm of anti-debugging. These techniques target peculiarities of how debuggers work.<sup><a href="#f5_6" name="b5_6">6</a></sup> This is a last gasp, though - if the viral code is already being analyzed in a debugger, then its survival time is dwindling. If the goal is to annoy the human analyst, then the best bet in survival terms is to follow a false trail when a debugger is detected, and avoid any viral behavior.<sup><a href="#f5_113" name="b5_113">113</a></sup></p>
<p>There are three weak points in a debugger that can be used to detect its presence: idiosyncrasies, breakpoints, and single-stepping.</p>
<dl>
<dt><strong>Debugger-specific idiosyncrasies.</strong></dt><dd>As with emulators, debuggers won't present a program being debugged with an environment identical to its normal environment, and a virus can look for quirks of known debuggers.<sup><a href="#f5_114" name="b5_114">114</a></sup></dd>
<dt><strong>Debugger breakpoints.</strong></dt><dd>Debuggers implement breakpoints by modifying the program being debugged, inserting special breakpoint instructions at points where the debugger wants to regain control. Typical breakpoint instructions cause the CPU to trap to an interrupt service routine.<sup><a href="#f5_115" name="b5_115">115</a></sup>
<p>A virus can look for signs of debugging by being introspective: it can examine its own code for breakpoint instructions. Since the virus may use external library code where debugger breakpoints can be set, breakpoint instructions can also be looked for at the entry points to library API functions.<sup><a href="#f5_116" name="b5_116">116</a></sup></p>
<p>More generally, a virus can look for any changes to itself. From the virus' point of view, a change is an error, and there are two distinct possibilities for dealing with errors: <em>error detection</em> and <em>error correction</em>. Error detection, like the use of checksums or CRCs, would tell the virus whether or not a change had occurred to it, and the virus could take action accordingly. On the other hand, error correction not only detects errors, but is able to repair a finite number of them. A robust virus would imply the use of error correction over error detection - this would guard against transmission errors and keep casual would-be virus writers from modifying the virus, and also be able to remove debugger breakpoint instructions.<sup><a href="#f5_117" name="b5_117">117</a></sup></p></dd>
<dt><strong>Single-stepping.</strong></dt><dd>Debuggers trace through code, instruction by instruction, using the single-stepping facilities available in many CPUs. After each instruction is executed, the CPU posts an interrupt which the debugger handles.
 
<div align="center">
<img src="img/mja01/fig51.gif" alt="Figure 5.1 Checking for single-stepping"/>
<p><strong>Figure 5.1 Checking for single-stepping</strong></p>
</div>
<p>There are several ways to check for single-stepping:</p>
<ul>
<li>Push a value on the stack, pop it off, then check to see if it's still there.<sup><a href="#f5_7" name="b5_7">7</a></sup> As Figure 5.1 shows, an interrupt would dump information onto the stack, destroying the value that had been placed on there. Strictly speaking, any interrupt would cause this to happen, not just a single-stepping interrupt, but it is a conservative test from the virus' point of view.</li>
<li>Handling interrupts is an expensive task. Sample the current time, and watch for the slowdown that would occur under single-stepping.<sup><a href="#f5_118" name="b5_118">118</a></sup></li>
<li>CPUs commonly have an instruction prefetch queue, where instructions are loaded prior to their execution for performance reasons. A virus can dynamically modify the <em>next</em> instruction immediately following the program counter; if the new instruction runs rather than the old one, then single-stepping may be enabled. Why? Because the instruction prefetch queue was flushed, which would occur on an interrupt.</li>
</ul>
<p>The latter two methods are possible anti-emulation methods as well, because they would look for slow or incomplete emulators.</p></dd>
</dl>
<p>A general approach to anti-debugging is to look for changes to the addresses of interrupt handlers, and render the virus nonfunctional if the handler address is unexpected. One way to accomplish this is to include the addresses of the
 
breakpoint and single-stepping interrupt handlers as part of the virus' decryption key.<sup><a href="#f5_119" name="b5_119">119</a></sup></p>
<div align="center">
<img src="img/mja01/fig52.gif" alt="Figure 5.2. False disassembly"/>
<p><strong>Figure 5.2. False disassembly</strong></p>
</div>
<p>And, if all else fails, ask. Windows has an API function called IsDebuggerPresent which returns the calling process' debugging status. Elkem.C is one piece of malware that uses this technique.<sup><a href="#f5_120" name="b5_120">120</a></sup> The means of asking need not be direct, either. A request under Linux to trace a process more than once fails, and if a debugger has traced the virus' process already, an attempt by the virus to trace itself will fail.<sup><a href="#f5_121" name="b5_121">121</a></sup></p>
<h4><a name="c542"></a>5.4.2 Anti-Disassembly</h4>
<p>Any of the code obfuscation techniques used by polymorphic and metamorphic viruses are anti-disassembly techniques, but only in a weak sense. There are two goals for strong anti-disassembly:</p>
<ol>
<li>Disassembly should not be easily automated; the valuable time of an expert human should be required to make sense of the code.</li>
<li>The full code should not be available until such time as the code actually runs.</li>
</ol>
<p>To make automation difficult, a virus' code can make use of problems which are computationally very hard to solve. It turns out that the simple trick of mixing code and data is one such problem: precise separation of the two is known to be unsolvable.<sup><a href="#f5_122" name="b5_122">122</a></sup> In general, a virus may be structured so that separating code and data is also impossible - this can be done by using instructions as data values and vice versa.</p>
<p>A careful mix of code and data may even throw off human analysis temporarily. The x86 assembly code in Figure 5.2 starts with a subroutine call that
 
never returns: when run, the called code pops the saved return address off the stack and returns, so the net effect of this code is the same as a single return instruction. However, some bytes have been placed after the call, causing a false disassembly to occur when main is disassembled.<sup><a href="#f5_8" name="b5_8">8</a></sup></p>
<div align="center">
<img src="img/mja01/fig53.gif" alt="Figure 5.3. Anti-disassembly using strong cryptographic hash functions"/>
<p><strong>Figure 5.3. Anti-disassembly using strong cryptographic hash functions</strong></p>
</div>
<p>The second anti-disassembly goal, not having the full code available until run time, can be met in several ways:</p>
<ul>
<li>Code can be dynamically generated when the virus runs, much like a just-in-time (JIT) compiler.</li>
<li>Existing code can modify itself as the virus runs.<sup><a href="#f5_9" name="b5_9">9</a></sup> Self-modifying code is a rarity now in typical, compiler-generated programs, and this behavior would act as a red flag for anti-virus heuristics.</li>
<li>A more complex dynamic code generation scheme could draw on the execution environment for its instructions, much like the environmental key generation described in Section 3.2.7. An environmental parameter, like a username, is combined with a constant "salt" <em>K</em> which is chosen by the virus writer, and fed into a strong cryptographic hash function. Resulting bytes from the hash function are extracted and used as instructions. The value of <em>K</em> is selected to yield a desired instruction sequence when this is done. Direct analysis of this scheme is very difficult, because the viral code is not available to be analyzed and, even if an educated guess can be made about it, the strong cryptographic hash ensures that the exact value of the environmental parameter cannot be determined even when <em>K</em> is known. This scheme is illustrated in Figure 5.3, where the information in the shaded boxes indicates the information available to an analyst.<sup><a href="#f5_123" name="b5_123">123</a></sup></li>
<li>Keep the code in encrypted form, and decrypt parts of it only when needed.<sup><a href="#f5_10" name="b5_10">10</a></sup> Figure 5.4 shows how this can be done by inserting a breakpoint into the code immediately before an encrypted instruction, and supplying interrupt handlers for breakpoint and single-stepping interrupts.<sup><a href="#f5_124" name="b5_124">124</a></sup>
 
<pre>
				instruction
				<em>breakpoint
				encrypted instruction</em>
				instruction
				instruction
def breakpoint_handler():			def singlestep_handler():
    decrypt next instruction			    re-encrypt last instruction
    enable single-stepping			    disable single-stepping
    return from interrupt			    return from interrupt
</pre>
<div align="center">
<p><strong>Figure 5.4. On-demand code decryption</strong></p>
</div>
<p>Another suggestion is to use separate threads of execution, one to decrypt code ahead of the virus' program counter, the other to re-encrypt behind the virus' program counter.<sup><a href="#f5_125" name="b5_125">125</a></sup> This would intentionally be a delicately-tuned system, so that any variance (like that introduced by a debugger or emulator) would cause a crash, making it an anti-debugging technique too.</p></li>
</ul>
<p>Anti-disassembly techniques are not solely for irritating human anti-virus researchers. They can also be seen as a defense against anti-virus software using static heuristics.</p>
<h3><a name="c55"></a>5.5 Tunneling</h3>
<p>Anti-virus software may monitor calls to the operating system's API to watch for suspicious activity. A <em>tunneling</em> virus is one that traces through the code for API functions the virus uses, to ensure that execution will end up at the "right" place, i.e., the virus isn't being monitored. If the virus does detect monitoring, tunneling allows the monitoring to be bypassed.<sup><a href="#f5_126" name="b5_126">126</a></sup> An interesting symmetry is that the defensive technique in this case is exactly the same as the offensive technique: tracing through the API code.</p>
<p>The code "tracing" necessary for tunneling can be implemented by viruses in several ways,<sup><a href="#f5_127" name="b5_127">127</a></sup> all of which resemble anti-virus techniques. A static analysis method would scan through the code, looking for control flow changes. Dynamic methods would single-step through the code being traced, or use fullblown emulation.</p>
<p>Tunneling can only be done when the code in question can be read, obviously. For operating systems without strong memory protection between user processes and the operating system, like MS-DOS, tunneling is an effective technique. Many operating systems do distinguish between user space and kernel space, though, a barrier which is crossed by a trap-based operating system API. In other words, the kernel's code cannot be read by user processes. Surprisingly, tunneling can still be useful, because most high-level programming
 
languages don't call the operating system directly, but call small library stubs that do the dirty work - these stubs can be tunneled into.</p>
<p>Anti-virus software can dodge this issue if it installs itself into the operating system kernel. (This is also a desirable goal for viruses, because a virus in the kernel would control the machine completely.)</p>
<h3><a name="c56"></a>5.6 Integrity Checker Attacks</h3>
<p>In terms of anti-anti-virus techniques, integrity checkers warrant some careful handling, because they are able to catch <em>any</em> file change at all, not just suspicious code.<sup><a href="#f5_128" name="b5_128">128</a></sup></p>
<p>Stealth viruses have a big advantage against integrity checkers. A stealth virus can hide file changes completely, so the checker never sees them. Companion viruses are effective against integrity checkers for the same reason, because no changes to the infected file are ever seen.</p>
<p>Stealth viruses can also infect when a file is read, so the act of computing a checksum by an integrity checker will itself infect a file. In that case, the viral code would be included in the checksum without any alarm being raised.</p>
<p>Similarly, a "slow" virus can infect only when a file was about to be legitimately changed anyway.<sup><a href="#f5_129" name="b5_129">129</a></sup> The infection doesn't need to be immediate, so long as any alert that the integrity checker pops up appears soon after the legitimate change; a user is likely to dismiss the alert as a false positive.</p>
<p>Finally, integrity checkers may have flaws that can be exploited. In one classic case, deleting the integrity checker's database of checksums caused the checker to faithfully recompute checksums for allfiles!<sup><a href="#f5_11" name="b5_11">11</a></sup></p>
<h3><a name="c57"></a>5.7 Avoidance</h3>
<p>Those who admit to remembering the <em>Karate Kid</em> movies will know that the best way to avoid a punch is not to be there. The same principle applies to anti-anti-virus techniques. A virus can hide in places where anti-virus software doesn't look. If anti-virus software only checks the hard drive, infect USB keys and floppies; if anti-virus software doesn't examine all file types, infect those file types; if files with special names aren't checked, infect files with those names.<sup><a href="#f5_130" name="b5_130">130</a></sup> Unusual types of file archive formats may temporarily escape unpacking and scrutiny, too.<sup><a href="#f5_131" name="b5_131">131</a></sup> In general, avoidance is not particularly effective as a strategy, though.</p>
 
<p><strong>Notes for Chapter 5</strong></p>
<p><a name="f5_1" href="#b5_1">1</a> Retroviruses have also been called "anti-antivirus viruses." No, really [77].</p>
<p><a name="f5_2" href="#b5_2">2</a> This is an excerpt from Avkiller, which is actually a Trojan horse, but the name is irresistible in this context [185].</p>
<p><a name="f5_3" href="#b5_3">3</a> Although the Windows taskbar hides icons of "inactive" applications by default, so a vanishing anti-virus icon may not be noticed.</p>
<p><a name="f5_4" href="#b5_4">4</a> Windows and Unix systems, for example, both have multilevel feedback queues that operate this way [202, 294].</p>
<p><a name="f5_5" href="#b5_5">5</a> For example, VMware can be detected in a number of ways [233, 353].</p>
<p><a name="f5_6" href="#b5_6">6</a> Assuming a software-based debugger.</p>
<p><a name="f5_7" href="#b5_7">7</a> This, and the prefetch technique, are from Natvig [226]. He notes that the prefetch method's success depends upon how the CPU manages the prefetch queue.</p>
<p><a name="f5_8" href="#b5_8">8</a> Alas, this trick doesn't work as well for CPUs whose instructions need to be word-aligned in memory, but code and data can still be mixed.</p>
<p><a name="f5_9" href="#b5_9">9</a> Generally, self-modifying code can wreakhavoc on static analysis tools [186].</p>
<p><a name="f5_10" href="#b5_10">10</a> grugq and scut [132] call this "running line code."</p>
<p><a name="f5_11" href="#b5_11">11</a> Proof of concept courtesy of the Peach virus [15].</p>
<p><a name="f5_100" href="#b5_100">100</a> See [149, 244], [77], and [242], respectively.</p>
<p><a name="f5_101" href="#b5_101">101</a> Molnar and Szappanos [210].</p>
<p><a name="f5_102" href="#b5_102">102</a> A student suggested this possibility, although no actual example of this technique has been found to date.</p>
<p><a name="f5_103" href="#b5_103">103</a> Analyses of Simile and Ganda can be found in Perriot et al. [249] and Molnar and Szappanos [210], respectively.</p>
<p><a name="f5_104" href="#b5_104">104</a> GriYo[131].</p>
<p><a name="f5_105" href="#b5_105">105</a> The issue of how long to emulate is mentioned in Nachenberg [217], also Szor [308].</p>
<p><a name="f5_106" href="#b5_106">106</a> See Nachenberg [217]. [314] mentions the problems of junk code and occasional replication.</p>
<p><a name="f5_107" href="#b5_107">107</a> These possibilities are from Veldman [332].</p>
<p><a name="f5_108" href="#b5_108">108</a> These first four are from Veldman [332].</p>
<p><a name="f5_109" href="#b5_109">109</a> See also Natvig [226].</p>
<p><a name="f5_110" href="#b5_110">110</a> Natvig [226] talks about library-related emulation problems.</p>
<p><a name="f5_111" href="#b5_111">111</a> Szor and Ferrie [314] point out the external resource problem.</p>
<p><a name="f5_112" href="#b5_112">112</a> See Krakowicz [172] for an early, pre-lowercase treatise on the subject.</p>
<p><a name="f5_113" href="#b5_113">113</a> Hasson [139] suggests this strategy when using anti-debugging for software protection.</p>
 
<p><a name="f5_114" href="#b5_114">114</a> Hasson [139] and CrackZ [81].</p>
<p><a name="f5_115" href="#b5_115">115</a> See Rosenberg [268] for more information on this and single-stepping.</p>
<p><a name="f5_116" href="#b5_116">116</a> Hasson [139].</p>
<p><a name="f5_117" href="#b5_117">117</a> Pless [254] talks about the error detection/correction distinction. The use of Hamming codes for error correction for the first two reasons is in Ferbrache [103]; RDA.Fighter uses them for anti-debugging [83].</p>
<p><a name="f5_118" href="#b5_118">118</a> CrackZ[81].</p>
<p><a name="f5_119" href="#b5_119">119</a> Stampf[302].</p>
<p><a name="f5_120" href="#b5_120">120</a> This suggestion was made by CrackZ [81 ]; the Elkem.C analysis is in [239].</p>
<p><a name="f5_121" href="#b5_121">121</a> Cesare [57].</p>
<p><a name="f5_122" href="#b5_122">122</a> Horspool and Marovac [146].</p>
<p><a name="f5_123" href="#b5_123">123</a> Aycocketal. [22].</p>
<p><a name="f5_124" href="#b5_124">124</a> Bontchev [46].</p>
<p><a name="f5_125" href="#b5_125">125</a> Stampf[302].</p>
<p><a name="f5_126" href="#b5_126">126</a> Bontchev [46]; Methyl [205].</p>
<p><a name="f5_127" href="#b5_127">127</a> Methyl [205].</p>
<p><a name="f5_128" href="#b5_128">128</a> This section is based on Bontchev [38].</p>
<p><a name="f5_129" href="#b5_129">129</a> Gryaznov [133].</p>
<p><a name="f5_130" href="#b5_130">130</a> The first two are from Bontchev [38], the last from Sowhat [297].</p>
<p><a name="f5_131" href="#b5_131">131</a> Hypp&ouml;nen [149] notes this, along with a laundry list of anti-anti-virus techniques.</p>
 
<h2><a name="c6"></a>Chapter 6 Weakness exploited</h2>
<p>Weaknesses are thin ice on the frozen lake of security, vulnerable points through which a system's security may be compromised. Thin ice doesn't always break, and not all weaknesses are exploitable. However, an examination of the devious and ingenious ways that security can be breached is enlightening.</p>
<p>Malware may exploit weaknesses to initially infiltrate a system, or to gain additional privileges on an already-compromised machine. The weaknesses may be exploited automatically by malware authors' creations, or manually by people directly targeting a system. In this chapter, the initiator of an exploit attempt will be generically called an "attacker."</p>
<p>Weaknesses fall into two broad categories, based on where the weakness lies. Technical weaknesses involve tricking the target computer, while human weaknesses involve tricking people.</p>
<h3><a name="c61"></a>6.1 Technical Weaknesses</h3>
<p>Weaknesses in hardware are possible, but weaknesses in software are disturbingly common. After some background material, a number of frequent weaknesses are discussed, such as various kinds of buffer overflow (stack smashing, frame pointer overwriting, returns into libraries, heap overflows, and memory allocator attacks), integer overflows, and format string vulnerabilities. This is unfortunately not an exhaustive list of all possible weaknesses. At the end of this section, how weaknesses are found, and defenses to these weaknesses are examined. Where possible, weaknesses and defenses are presented in a language- and architecture-independent way.</p>
 
<div align="center">
<img src="img/mja01/fig61.gif" alt="Figure 6.1. Conceptual memory layout"/>
<p><strong>Figure 6.1. Conceptual memory layout</strong></p>
</div>
<h4><a name="c611"></a>6.1.1 Background</h4>
<p>Conceptually, a process' address space is divided into four "segments" as shown in Figure 6.1:<sup><a href="#f6_1" name="b6_1">1</a></sup></p>
<ul>
<li>The program's code resides in the fixed-size code segment. This segment is usually read-only.</li>
<li>Program data whose sizes are known at compile-time are in the fixed-size data segment.</li>
<li>A "heap" segment follows the data segment and grows upwards; it also holds program data. The heap as used in this context has <em>nothing whatsoever</em> to do with a heap data structure, even though they share the name.</li>
<li>A stack starts at high memory and grows downwards. In practice, the direction of stack growth depends on the architecture. Downwards growth will be assumed here for concreteness.</li>
</ul>
<p>A variable in an imperative language, like C, C++, and Java, is allocated to a segment based on the variable's lifetime and the persistence of its data. A sample C program with different types of variable allocation is shown in Figure 6.2. Global variables have known sizes and persist throughout run-time, so they are placed into the data segment by a compiler. Space for dynamic allocation has to grow on demand; dynamic allocation is done from the heap segment. Finally, local variables don't persist beyond the return of a subroutine, and subroutine
 
calls within a program follow a stack discipline, so local variables are allocated space on the stack.</p>
<div align="center">
<img src="img/mja01/fig62.gif" alt="Figure 6.2. Sample segment allocation"/>
<p><strong>Figure 6.2. Sample segment allocation</strong></p>
</div>
<p>A subroutine gets a new copy of its local variables each time the subroutine is called. These are stored in the subroutine's <em>stack frame</em>, which can be thought of as a structure on the stack. When a subroutine is entered, space for the subroutine's stack frame is allocated on the stack; when a subroutine exits, its stack frame space is deallocated. The code to manage the stack frame is added automatically by a compiler.<sup><a href="#f6_2" name="b6_2">2</a></sup> Figure 6.3 shows how the stack frames change when code runs. Note that A is called a second time before the first call to A has returned, and consequently A has <em>two</em> stack frames on the stack at that point, one for each invocation.</p>
<p>More than local variables may be found in a stack frame. It serves as a repository for all manner of bookkeeping information, depending on the particular subroutine, including:</p>
<ul>
<li>Saved register values. Registers are a limited resource, and it is often the case that multiple subroutines will use the same registers. Calling conventions specify the protocol for saving, and thus preserving, register contents that are not supposed to be changed - this may be done by the calling subroutine (the <em>caller</em>), the called subroutine (the <em>callee</em>), or some combination of the two. If registers need to be saved, they will be saved into the stack frame.</li>
<li>Temporary space. There may not be enough registers to hold all necessary values that a subroutine needs, and some values may be placed in temporary space in the stack frame.
 
<div align="center">
<img src="img/mja01/fig63.gif" alt="Figure 6.3. Stack frame trace"/>
<p><strong>Figure 6.3. Stack frame trace</strong></p>
</div></li>
<li>Input arguments to the subroutine. Arguments passed to the subroutine, if any.</li>
<li>Output arguments from the subroutine. These are arguments that the subroutine passes to other subroutines that it calls.</li>
<li>Return address. When the subroutine returns, this is the address at which execution resumes.</li>
<li>Saved frame pointer. A register is usually reserved for use as a stack pointer, but the stack pointer may move about as arguments and other data are pushed onto the stack. A subroutin's <em>frame pointer</em> is a register that always points to a fixed position within the subroutine's stack frame, so that a subroutine can always locate its local variables with constant offsets. Because each newly-called subroutine will have its own stack frame, and thus its own frame pointer, the previous value of the frame pointer must be saved in the stack frame.</li>
</ul>
<p>The inclusion of the last four as part of the stack frame proper is philosophical; some architectures include them, some don't. They will be assumed to be separate here in order to illustrate software weaknesses. For similar reasons, similar assumptions: arguments are passed on the stack, the return address and
 
saved frame pointer are on the stack. Variations of the weaknesses described here can often be found for situations where these assumptions aren't true.</p>
<div align="center">
<img src="img/mja01/fig64.gif" alt="Figure 6.4. Before and after a subroutine call"/>
<p><strong>Figure 6.4. Before and after a subroutine call</strong></p>
</div>
<p>Figure 6.4 shows the stack before and after a subroutine call. Prior to the call, the caller will have placed any arguments being passed into its argument build area. The call instruction will push the return address onto the stack and transfer execution to the callee.<sup><a href="#f6_3" name="b6_3">3</a></sup> The callee's code will begin by saving the old frame pointer onto the stack and creating a new stack frame.</p>
<h4><a name="c612"></a>6.1.2 Buffer Overflows</h4>
<p>A <em>buffer overflow</em> is a weakness in code where the bounds of an array (often a buffer) can be exceeded. An attacker who is able to write into the buffer, directly or indirectly, will be able to write over other data in memory and cause the code to do something it wasn't supposed to. Generally, this means that an attacker could coerce a program into executing arbitrary code of the attacker's choice. Often the attacker's goal is to have this "arbitrary code" start a user shell, preferably with all the privileges of the subverted program - for this reason, the code the attacker tries to have run is generically referred to as <em>shellcode</em>.</p>
<p>One question immediately arises: why are these buffers' array bounds not checked? Some languages, like C, don't have automatic bounds checking. Sometimes, bounds-checking code <em>is</em> present, but has bugs. Other times, a buffer overflow is an indirect effect of another bug.</p>
 
<pre class="source">
	def main():
	   fill_buffer0

	def fill_buffer0 :
	   character buffer[100]
	   i = 0
	   ch = input()
	   while ch <img src="/img/cache/f9bb70af966a4abbd08b776e6c5971ad.gif" alt="\ne" valign="middle"/> NEWLINE:
	        <img src="/img/cache/4c694dc1de6dc48ccaf9f6b9614c0900.gif" alt="\text{buffer_i}" valign="middle"/> = ch
		ch = input()
		i = i + 1
</pre>
<div align="center">
<p><strong>Figure 6.5. Code awaiting a stack smash</strong></p>
</div>
<p>Buffer overflows are not new. The general principle was known at least as far back as 1972,<sup><a href="#f6_100" name="b6_100">100</a></sup> and a buffer overflow was exploited by the Internet worm in 1988.</p>
<h5><a name="c6121"></a>6.1.2.1 Stack Smashing</h5>
<p><em>Stack smashing</em> is a specific type of buffer overflow, where the buffer being overflowed is located in the stack.<sup><a href="#f6_101" name="b6_101">101</a></sup> In other words, the buffer is a local variable in the code, as in Figure 6.5. Here, no bounds checking is done on the input being read. As the stack-allocated buffer is filled from low to high memory, an attacker can continue writing, right over top of the return address on the stack. The attacker's input can be shellcode, followed by the address of the shellcode on the stack - when <tt>fill_buffer</tt> returns, it resumes execution where the attacker specified, and runs the shellcode. This is illustrated in Figure 6.6.</p>
<p>The main problem for the attacker isfindingout the address of the buffer in the stack. Fortunately for the attacker, many operating systems situate a process' stack at the same memory location each time a program runs. To account for slight variance, an attacker can precede the shellcode with a sequence of "NOP" instructions that do nothing.<sup><a href="#f6_4" name="b6_4">4</a></sup> Because jumping anyplace into this NOP sequence will cause execution to slide into the shellcode, this is called a <em>NOP sled</em>.<sup><a href="#f6_102" name="b6_102">102</a></sup> The <em>exploit string</em>, the input sent by the attacker, is thus</p>
<pre>NOP NOP NOP ... <em>shellcode new-return-address</em></pre>
<p>The space taken up by the NOP sled and the shellcode must be equal to the distance from the start of the buffer to the return address on the stack, otherwise the new return address won't be written to the correct spot on the stack. The saved frame pointer on the stack doesn't have to be preserved, either, because execution won't be returning to the caller anyway.</p>
<p>There are several other issues that arise for an attacker:</p>
 
<div align="center">
<img src="img/mja01/fig66.gif" alt="Figure 6.6. Stack smashing attack"/>
<p><strong>Figure 6.6. Stack smashing attack</strong></p>
</div>
<ul>
<li>The length of the exploit string must be known, but the <em>exact</em> location on the stack may not be, due to the NOP sled. The addresses of strings in the shellcode cannot be hardcoded as a result - for example, shellcode may need a string containing the path to a user shell like <tt>/bin/sh</tt>. Some architectures allow addresses to be specified relative to the program counter's value, called <em>PC-relative addressing</em>. Other architectures, like the Intel x86, don't have PC-relative addressing, but do allow PC-relative subroutine calls. On the x86, a PC-relative jump from one part of the shellcode to another part of the shellcode will leave the caller's location on top of the stack. This location is the stack address of the shellcode.</li>
<li>Depending on the code being attacked, some byte values can terminate the input before the buffer is overflowed. In Figure 6.5, for instance, a newline character terminates the input. The exploit string cannot contain these input-terminating values. An attacker must rewrite their exploit string if necessary, to compute the forbidden values instead of containing them directly. For example, an ASCII NUL character (byte value 0) can be computed by XORing a value with itself.</li>
<li>A buffer may be too small to hold the shellcode. One possible workaround is to write the shellcode <em>after</em> writing the new return address.
<p>Another possibility is to use the program's environment. Most operating systems allow <em>environment variables</em> to be set, which are variable names and values that are copied into a program's address space when it starts running. If an attacker controls the exploited program's environment, they can put their shellcode into an environment variable. Instead of making the
 
new return address point to the overwritten buffer, the attacker points the new return address to the environment variable's memory location (Figure 6.7).<sup><a href="#f6_5" name="b6_5">5</a></sup></p></li>
</ul>
<div align="center">
<img src="img/mja01/fig67.gif" alt="Figure 6.7. Environmentally-friendly stack smashing"/>
<p><strong>Figure 6.7. Environmentally-friendly stack smashing</strong></p>
</div>
<h5><a name="c6122"></a>6.1.2.2 Frame Pointer Overwriting</h5>
<p>What if a buffer can be overrun by only one byte? Can an attack be staged? Under some circumstances, it can, except instead of overwriting the return address on the stack, the attack overwrites a byte of the saved frame pointer. This is a <em>frame pointer overwriting</em> attack.<sup><a href="#f6_103" name="b6_103">103</a></sup></p>
<p>The success of this attack relies on two factors:</p>
<ol>
<li>Some architectures demand that data be <em>aligned</em> in memory, meaning that the data must start at a specific byte boundary. For example, integers may be required to be aligned to a four-byte boundary, where the last two bits of the data's memory address are zero. When necessary, compilers will insert <em>padding</em> - unused bytes - to ensure that alignment constraints are met. There must be no padding on the stack between the buffer and the saved frame pointer for a frame pointer overwrite to work. Otherwise, writing one byte beyond the buffer would only alter a padding byte, not the saved frame pointer. Padding aside, no other data items can reside between the buffer and saved frame pointer, for similar reasons.
 
<pre class="source">
	def main():
	   fill_buffer()

	def fill_buffer():
	   character buffer[100]
	   i = 0
	   ch = input()
	   while i &lt;= 100 and ch <img src="/img/cache/f9bb70af966a4abbd08b776e6c5971ad.gif" alt="\ne" valign="middle"/> NEWLINE:
	      <img src="/img/cache/4c694dc1de6dc48ccaf9f6b9614c0900.gif" alt="\text{buffer_i}" valign="middle"/> = ch
	      ch = input()
	      i = i + 1
</pre>
<div align="center">
<p><strong>Figure 6.8. Code that goes just a little too far</strong></p>
</div></li>
<li>The architecture must be little-endian. <em>Endianness</em> refers to the way an architecture stores data in memory. For example, consider the four-byte hexadecimal number aabbccdd. A <em>big-endian</em> machine would store the most significant byte first in memory; a <em>little-endian</em> machine like the Intel x86 would store the least significant byte first:
<table summary="byte order">
<tr><th>&nbsp;</th><th>X</th><th>X+1</th><th>X+2</th><th>X+3</th></tr>
<tr><th>Big-endian</th><td>aa</td><td>bb</td><td>cc</td><td>dd</td></tr>
<tr><th>Little-endian</th><td>dd</td><td>cc</td><td>bb</td><td>aa</td></tr>
</table>
<p>On a big-endian machine, a frame pointer overwrite would change the most significant byte of the saved frame pointer; this would radically change where the saved frame pointer points in memory. However, on a little-endian machine, the overwrite changes the least significant byte, causing the saved frame pointer to only change slightly.</p></li>
</ol>
<p>When the called subroutine returns, it restores the saved frame pointer from the stack; the caller's code will then use that frame pointer value. After a frame pointer attack, the caller will have a distorted view of where its stack frame is.</p>
<p>For example, the code in Figure 6.8 allows one byte to be written beyond the buffer, because it erroneously uses &lt;= instead of &lt;.<sup><a href="#f6_6" name="b6_6">6</a></sup> Figure 6.9 shows the stack layout before and after the attack. By overwriting the buffer and changing the saved frame pointer, the attacker can make the saved frame pointer point <em>inside</em> the buffer, something the attacker controls. The attacker can then forge a stack frame for the caller, convincing the caller's code to use fake stack frame values, and eventually return to a return address of the attacker's choice. The exploit string would be</p>
 
<pre>NOP NOP NOP ... <em>shellcode fake-stack-frame
fake-saved-frame-pointer shellcode-address
           new-frame-pointer-byte</em></pre>
<p>A saved frame pointer attack isn't straightforward to mount, but serves to demonstrate two things. First, an off-by-one error is enough to leave an exploitable weakness. Second, it demonstrates that just guarding the return address on the stack is insufficient as a defense.</p>
<h5><a name="c6123"></a>6.1.2.3 Returns into Libraries</h5>
<p>The success of basic stack smashing attacks relies on the shellcode they inject into the stack-allocated buffer. One suggested defense against these attacks is to make the stack's memory nonexecutable. In other words, the CPU would be unable to execute code in the stack, even if specifically directed to do so.</p>
<p>Unfortunately, this defense doesn't work. If an attacker can't run arbitrary code, they can still run other code. As it happens, there is a huge repository of interesting code already loaded into the address space of most processes: shared library code.<sup><a href="#f6_104" name="b6_104">104</a></sup> An attacker can overwrite a return address on the stack to point to a shared library routine to execute. For example, an attacker may call the system library routine, which runs an arbitrary command.</p>
<div align="center">
<img src="img/mja01/fig69.gif" alt="Figure 6.9. Frame pointer overwrite attack"/>
<p><strong>Figure 6.9. Frame pointer overwrite attack</strong></p>
</div>
 
<div align="center">
<img src="img/mja01/fig6a.gif" alt="Figure 6.10. A normal function call with arguments"/>
<p><strong>Figure 6.10. A normal function call with arguments</strong></p>
</div>
<p>Arguments may be passed to library routines by the attacker by writing <em>beyond</em> the return address in the stack. Figure 6.10 shows the initial stack contents a subroutine would expect to see when called with arguments; Figure 6.11 shows a retum-to-library attack which passes arguments. Notice the extra placeholder required, because the called library function expects a return address on the stack at that location.</p>
<p>This attack is often called a <em>return-to-libc</em> attack, because the C shared library is the usual target, but the attack's concept is generalizable to any shared library.</p>
<h5><a name="c6124"></a>6.1.2.4 Heap Overflows</h5>
<p>This next attack is somewhat of a misnomer. A <em>heap overflow</em> is a buffer overflow, where the buffer is located in the heap or the data segment.<sup><a href="#f6_105" name="b6_105">105</a></sup> The idea is not to overwrite the return address or the saved frame pointer, but to overwrite other variables that are adjacent to the buffer. These are more "portable" in a sense, because heap overflows don't rely on assumptions about stack layout, byte ordering, or calling conventions.</p>
<p>For example, the following global declarations would be allocated to the data segment:</p>
<pre class="source">
	character buffer[123]
	function pointer p
</pre>
<p>Overflowing the buffer allows an attacker to change the value of the function pointer p, which is the address of a function to call. If the program performs a function call using p later, then it jumps to the address the attacker specified; again, this allows an attacker to run arbitrary code.</p>
 
<div align="center">
<img src="img/mja01/fig6b.gif" alt="Figure 6.11 Return-to-library attack, with arguments"/>
<p><strong>Figure 6.11 Return-to-library attack, with arguments</strong></p>
</div>
<p>The range of possibilities for heap overflow attacks depends on the variables that can be overwritten, how the program uses those variables, and the imagination of the attacker.</p>
<h5><a name="c6125"></a>6.1.2.5 Memory Allocator Attacks</h5>
<p>One way heap overflows can be used is to attack the dynamic memory allocator. As previously mentioned, space is dynamically allocated from the heap. The allocator needs to maintain bookkeeping information for each block of memory that it oversees in the heap, allocated or unallocated. Allocators find space for this information by overallocating memory - when a program requests an X-byte block of memory, the allocator reserves extra space:</p>
<ul>
<li>Before the block, room for bookkeeping information.</li>
<li>After the block, space may be needed to round the block size up. This may be done to avoid fragmenting the heap with remainder blocks that are too small to be useful, or to observe memory alignment constraints.</li>
</ul>
<p>The key observation is that the bookkeeping information is stored in the heap, following an allocated block. Exploiting a heap overflow in one block allows the bookkeeping information for the following block to be overwritten, as shown in Figure 6.12.<sup><a href="#f6_106" name="b6_106">106</a></sup></p>
 
<div align="center">
<img src="img/mja01/fig6c.gif" alt="Figure 6.12. Overflowing the heap onto bookkeeping information"/>
<p><strong>Figure 6.12. Overflowing the heap onto bookkeeping information</strong></p>
</div>
<div align="center">
<img src="img/mja01/fig6d.gif" alt="Figure 6.13. Dynamic memory allocator's free list"/>
<p><strong>Figure 6.13. Dynamic memory allocator's free list</strong></p>
</div>
<p>By itself, this isn't terribly interesting, but memory allocators tend to keep track of free, unallocated memory blocks in a data structure called a <em>free</em> list. As in Figure 6.13, the free list will be assumed here to be a doubly-linked list, so that blocks can be removed from the list easily. When an allocated block is freed, the allocator checks to see if the block immediately following it is also free; if so, the two can be merged to make one larger block of free memory. This is where the free list is used: the already-free block must be unlinked from the free list, in favor of the merged block.</p>
<p>A typical sequence for unlinking a block from a doubly-linked list is shown in Figure 6.14. The blocks on the list have been abstracted into uniform list nodes, each with two pointers as bookkeeping information, a "previous" pointer pointing to the previous list node, and a "next" pointer pointing to the next node. From the initial state, there are two steps to unlink a node <em>B</em>:</p>
<ol>
<li>The next node, <em>C</em>, is found by following <em>B</em>'s next pointer. <em>C</em>'s previous pointer is set to the value of <em>B</em>'s previous pointer.</li>
 
<li><em>B</em>'s previous pointer is followed to find the previous node, <em>A</em>, <em>A</em>'s next pointer is set to the value of <em>B</em>'s next pointer.</li>
</ol>
<div align="center">
<img src="img/mja01/fig6e.gif" alt="Figure 6.14. Normal free list unlinking"/>
<p><strong>Figure 6.14. Normal free list unlinking</strong></p>
</div>
<p>Now, say that an attacker exploits a heap overflow in the allocated block immediately before <em>B</em>, and overwrites <em>B</em>'s list pointers. <em>B</em>'s previous pointer is set to the address of the attacker's shellcode, and <em>B</em>'s next pointer is assigned the address of a code pointer that already exists in the program. For example, this code pointer may be a return address on the stack, or a function pointer in the data segment. The attacker then waits for the program to free the memory block it overflowed.</p>
<p>Figure 6.15 illustrates the result. The memory allocator finds the next adjacent block (<em>B</em>) free, and tries to merge it. When the allocator unlinks <em>B</em> from the list, it erroneously assumes that <em>B</em>'s two pointers point to free list nodes. Following the same two steps as above, the allocator overwrites the targeted code pointer with the shellcode's address in the first step. This was the primary goal of the exploit. The second step writes a pointer just past the start of the shellcode. This would normally render the shellcode unrunnable, but the shellcode can be made to start with a jump instruction, skipping over the part of the shellcode that is overwritten during unlinking.</p>
<p>After the allocator's unlinking is complete, the targeted code address points to the shellcode, and the shellcode is run whenever the program uses that overwritten code address.</p>
 
<div align="center">
<img src="img/mja01/fig6f.gif" alt="Figure 6.15. Attacked free list unlinking"/>
<p><strong>Figure 6.15. Attacked free list unlinking</strong></p>
</div>
<h4><a name="c613"></a>6.1.3 Integer Overflows</h4>
<p>In most programming languages, numbers do not have infinite precision. For instance, the range of integers may be limited to what can be encoded in 16 bits.<sup><a href="#f6_7" name="b6_7">7</a></sup> This leads to some interesting effects:<sup><a href="#f6_107" name="b6_107">107</a></sup></p>
<ul>
<li>Integer overflows, where a value "wraps around." For example, 30000 + 30000 = -5536.</li>
 
<li>Sign errors. Mixing signed and unsigned numbers can lead to unexpected results. The unsigned value 65432 is -104 when stored in a signed variable, for instance.</li>
<li>Truncation errors, when a higher-precision value is stored in a variable with lower precision. For example, the 32-bit value 8675309 becomes 24557 in 16 bits.</li>
</ul>
<p>Few languages check for these kinds of problems, because doing so would occasionally impose additional overhead, and more occasionally, the programmer actually intended for the effect to occur.</p>
<p>At this point in the chapter, it should come as little surprise that these effects can be exploited by an attacker - they are collectively called <em>integer overflow</em> attacks. Usually the attack isn't direct, but uses an integer overflow to cause other types of weaknesses, like buffer overflows.<sup><a href="#f6_108" name="b6_108">108</a></sup></p>
<p>The code in Figure 6.16 has such a problem, and is derived from real code. All numbers are 16 bits long: n is the number of elements in an array to be read in; size is the size in bytes of each array element; <tt>totalsize</tt> is the total number of bytes required to hold the array. If an attacker's input results in n being 1234 and size being 56, their product is 69104, which doesn't fit in 16 bits - <tt>totalsize</tt> is set to 3568 instead. As a result of the integer overflow, only 3568 bytes of dynamic memory are allocated, yet the attacker can feed in 69104 bytes of input in the loop that follows, giving a heap overflow.</p>
<pre class="source">
	n = input_number()
	size = input_number()
	totalsize = n * size

	buffer = allocate_memory(totalsize)

	i = 0
	buffer_pointer = buffer
	while i &lt; n:
	   <img src="/img/cache/9c05d6ea0120623dd3eeac3d2145f7d9.gif" alt="\text{buffer_pointer_{0..size-1}}" valign="middle"/> = input_N_bytes(size)
	   buffer_pointer = buffer_pointer + size
	   i = i + 1
</pre>
<div align="center">
<p><strong>Figure 6.16. Code with an integer overflow problem</strong></p>
</div>
 
<h4><a name="c614"></a>6.1.4 Format String Vulnerabilities</h4>
<div class="epigraph">
'Perhaps one of the most interesting en'ors that we discovered was a result of an unusual interaction of two parts of csh, along with a little careless programming. The following string will cause the VAX version of csh to crash<br/>
<div align="center">!o%8f</div>
and the following string
<div align="center">!o%888888888f</div>
will hang... most versions of csh.'
<p>- Barton Miller et al.<sup><a href="#f6_109" name="b6_109">109</a></sup></p>
</div>
<br clear="all"/>
<p>Format functions in C take as input a "format string" followed by zero or more arguments. The format string tells the function how to compose the arguments into an output string; depending on the format function, the output string may be written to afile,the standard output location, or a buffer in memory.<sup><a href="#f6_8" name="b6_8">8</a></sup> Format string problems, the cause of the errors in the above quote, were a curiosity in 1990 when those words were published. By 1999, format string problems were recognized as a security problem, and they were being actively exploited by 2000.<sup><a href="#f6_110" name="b6_110">110</a></sup></p>
<p>The canonical example of a format function is <code>printf</code>:</p>
<div class="c" style="font-family:monospace;color: #000066;  border: solid thin #c2c1b1; background: #d6d5c5;">&nbsp; &nbsp; &nbsp; &nbsp; <span style="color: #993333;">char</span> <span style="color: #339933;">*</span>s <span style="color: #339933;">=</span> <span style="color: #ff0000;">&quot;is page&quot;</span><span style="color: #339933;">;</span><br/>
&nbsp; &nbsp; &nbsp; &nbsp; <span style="color: #993333;">int</span> n <span style="color: #339933;">=</span> <span style="color: #0000dd;">125</span><span style="color: #339933;">;</span><br/>
&nbsp; &nbsp; &nbsp; &nbsp; <a style="color: #000060;" href="http://www.opengroup.org/onlinepubs/009695399/functions/printf.html"><span style="color: #000066;">printf</span></a><span style="color: black;">&#40;</span><span style="color: #ff0000;">&quot;Hello, world!&quot;</span><span style="color: black;">&#41;</span><span style="color: #339933;">;</span><br/>
&nbsp; &nbsp; &nbsp; &nbsp; <a style="color: #000060;" href="http://www.opengroup.org/onlinepubs/009695399/functions/printf.html"><span style="color: #000066;">printf</span></a><span style="color: black;">&#40;</span><span style="color: #ff0000;">&quot;This %s %d.&quot;</span><span style="color: #339933;">,</span> s<span style="color: #339933;">,</span> n<span style="color: black;">&#41;</span><span style="color: #339933;">;</span><br/>
&nbsp;</div>
<p>The first call to <code>printf</code> prints <tt>Hello, world!</tt>; its format string doesn't contain any special directives telling <code>printf</code> to look for any additional arguments. The second call, on the other hand, does - %s says to interpret the next unread argument (s) as a pointer to a string, and %d treats the next unread argument (n) as an integer. The result is the output</p>
<pre>This is page 125.</pre>
<p>Saying "the next <em>unread</em> argument" implies that <code>printf</code> consumes the arguments as it formats the output string, and this is exactly what happens. Figure 6.17 shows the stack layout for a call to <code>printf</code>, assuming again that arguments are passed on the stack. As a format function reads its arguments, it effectively steps a pointer through the stack, where the pointer identifies the next argument to be read.</p>
<p>Format functions exhibit a touching faith in the correctness of the format string. A format function has no way of knowing how many arguments were <em>really</em> passed by its caller, which can be disastrous if an attacker is able to supply any part of a format string.<sup><a href="#f6_111" name="b6_111">111</a></sup> For example, if the program contains</p>
<div class="c" style="font-family:monospace;color: #000066;  border: solid thin #c2c1b1; background: #d6d5c5;">&nbsp; &nbsp; &nbsp; &nbsp; <a style="color: #000060;" href="http://www.opengroup.org/onlinepubs/009695399/functions/printf.html"><span style="color: #000066;">printf</span></a><span style="color: black;">&#40;</span>error<span style="color: black;">&#41;</span><span style="color: #339933;">;</span><br/>
&nbsp;</div>
 
<p>and an attacker manages to set the variable error to "%s%s%s%s", then the program will almost certainly crash. Printf's attacker-specified format string tells it to grab the next four items off the stack and treat each one as a pointer to a string. The problem is that the next four items on the stack <em>aren't</em> pointers to strings, so <code>printf</code> will make wild memory references in an effort to format its alleged strings.</p>
<div align="center">
<img src="img/mja01/fig6h.gif" alt="Figure 6.17. Stack layout for calling a format function"/>
<p><strong>Figure 6.17. Stack layout for calling a format function</strong></p>
</div>
<p>As is, this attack can be used to print out the contents of a target program's stack: an attacker can craft a format string which walks up the stack, interpreting each stack item as a number and printing the result. Changing <tt>error</tt> to "%d%d%d%d" in the above example would be enough to print the stack contents. This is one possible way that addresses can be discovered for a later stack smashing attack.</p>
<p>Even more is possible if the attacker can control a format string located in the stack. The code in Figure 6.18 is a common scenario, where a buffer's contents are formatted for later output. The <code>snprintf</code> function is a format function with two additional arguments, a buffer and the buffer's length; <code>snprintf</code> writes its formatted output to this buffer. It also demonstrates that a format string vulnerability can be exploited indirectly, as the flaw here is in the call to <code>printf</code>, not <code>snprintf</code>.</p>
<p>With this code, the attacker's format string can be the ungainly construction</p>
<pre>"\x78\x56\x34\x12 %d%d%d%d%d%d%d %n"</pre>
 
<div class="c" style="font-family:monospace;color: #000066;  border: solid thin #c2c1b1; background: #d6d5c5;"><span style="color: #993333;">void</span> print_error<span style="color: black;">&#40;</span><span style="color: #993333;">char</span> <span style="color: #339933;">*</span>s<span style="color: black;">&#41;</span><br/>
<span style="color: black;">&#123;</span><br/>
&nbsp; &nbsp; &nbsp; &nbsp; <span style="color: #993333;">char</span> buffer<span style="color: black;">&#91;</span><span style="color: #0000dd;">123</span><span style="color: black;">&#93;</span><span style="color: #339933;">;</span><br/>
&nbsp; &nbsp; &nbsp; &nbsp; <a style="color: #000060;" href="http://www.opengroup.org/onlinepubs/009695399/functions/snprintf.html"><span style="color: #000066;">snprintf</span></a><span style="color: black;">&#40;</span>buffer<span style="color: #339933;">,</span> <span style="color: #993333;">sizeof</span><span style="color: black;">&#40;</span>buffer<span style="color: black;">&#41;</span><span style="color: #339933;">,</span><br/>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span style="color: #ff0000;">&quot;Error: %s&quot;</span><span style="color: #339933;">,</span> s<span style="color: black;">&#41;</span> <span style="color: #339933;">;</span><br/>
&nbsp; &nbsp; &nbsp; &nbsp; <a style="color: #000060;" href="http://www.opengroup.org/onlinepubs/009695399/functions/printf.html"><span style="color: #000066;">printf</span></a><span style="color: black;">&#40;</span>buffer<span style="color: black;">&#41;</span><span style="color: #339933;">;</span><br/>
<span style="color: black;">&#125;</span><br/>
&nbsp;</div>
<div align="center">
<p><strong>Figure 6.18. Code with a format string vulnerability</strong></p>
</div>
<p>The buffer, a local variable on the stack, contains <code>printf</code>'s format string after the call to snprintf. Printf is thus called with a format string that the attacker has supplied in part:</p>
<pre>"Error: \x78\x56\x34\x12 %d%d%d%d%d%d%d %n"</pre>
<p>There are four parts to this format string.</p>
<ol>
<li><tt>Error:</tt> is added by <code>snprintf</code>. It plays no part in this attack and can be ignored.</li>
<li><tt>\x78\x56\x34\x12</tt> is the address 12345678 in little-endian format; in C strings, \x introduces a pair of hexadecimal digits.</li>
<li><tt>%d%d%d%d%d%d%d</tt>, used as mentioned above to walk up the stack's contents.</li>
<li>%n is a format string directive. It tells <code>printf</code> to interpret the next unread argument as a pointer to an integer. Printf writes the number of bytes it's formatted so far into the pointed-to integer. Through this mechanism, the attacker has a way to have a value written to an arbitrary memory location.</li>
</ol>
<p>The stack layout during an attack is given in Figure 6.19. The attacker's format string causes <code>printf</code> to walk up the stack, printing integers, until the next unread argument is the address the attacker encoded in the format string. (Remember that the buffer is in the stack, so the attacker's format string is there too.) The %n takes the attacker's address and writes a number at that address. The attacker can control the number written by adding junk characters to the format string, changing the number of bytes <code>printf</code> formats, and consequently the number written for %n.</p>
<p>Like other attacks, if an attacker can make a single specified value change, then the possibility of running shellcode exists.</p>
<h4><a name="c615"></a>6.1.5 Defenses</h4>
<p>The underlying moral in studying these technical vulnerabilities is to never, ever, <em>ever</em> trust input to a program. Having bulletproof input routines and bug-free
 
code is the best defense to technical vulnerabilities, but expecting this of all software is like asking Santa Claus for world peace - well intentioned, but unlikely to happen in the near future.</p>
<div align="center">
<img src="img/mja01/fig6j.gif" alt="Figure 6.19. Format string attack in progress"/>
<p><strong>Figure 6.19. Format string attack in progress</strong></p>
</div>
<p>In the meantime, two types of defenses can be considered, ones that are specific to a type of vulnerability, and ones that are more general.</p>
<h5><a name="c6151"></a>6.1.5.1 Vulnerability-Specific Defenses</h5>
<p>Defenses can be directed to guarding against certain types of vulnerability. For example:</p>
<dl>
<dt><strong>Format string vulnerabilities</strong></dt><dd>
<ul>
<li>Source code auditing is a particularly effective defense, because the number of format functions is relatively small, and it is easy to search source code for calls to format functions.<sup><a href="#f6_112" name="b6_112">112</a></sup></li>
 
<li>Remove support for %n in format functions, or only allow constant format strings that an attacker can't change.<sup><a href="#f6_113" name="b6_113">113</a></sup> This defense would break existing code in addition to violating the C specification.</li>
<li>If a format function knew how many arguments it had been called with, then it could avoid reading nonexistent arguments. Unfortunately, this information isn't available at run-time.
<p>A program's source code can be altered to supply this information. Calls to known format functions can be wrapped in macros that keep track of the number of arguments passed. Even this doesn't always work, because nonstandard format functions may be used, or standard format functions may be used in unusual ways. For example, the code may save a function pointer to <code>printf</code> and call it later, rather than calling <code>printf</code> directly.</p></li>
</ul></dd>
<dt><strong>Stack smashing</strong></dt><dd>
<ul>
<li>As mentioned before, one defense against stack smashing is to mark the stack's memory as nonexecutable; the same idea can be extended to the data and heap segments. This is not a complete defense, since a return-to-library attack is still possible, but it does close one attack vector.
<p>Some programs legitimately need to have executable code in odd places in memory, like just-in-time compilers and nested C functions.<sup><a href="#f6_9" name="b6_9">9</a></sup> An alternative memory protection approach ensures that memory pages can be writable or executable, <em>but</em> not both at the same time. This provides the same protection, but with moreflexibilityfor legitimate programs.<sup><a href="#f6_10" name="b6_10">10</a></sup></p></li>
<li>The control information in the stack, the return address and the saved frame pointer, can be guarded against inappropriate modification. This method prevents stack smashing attacks, and also catches some buggy programs. The way the control information is guarded is by using <em>canaries</em>.
<p>Miners used to use live canaries as a safety precaution. A buildup of toxic gases in a mine would kill a canary before a human, so canaries were taken down into mines as an early-warning system. Finding a metabolically-challenged canary meant that it was time for a coffee break on the surface.</p>
<p>For stack smashing defense, a <em>canary</em> is a value which is strategically located in the stack frame, between the local variables and the control information (Figure 6.20). A canary can't withstand an attack- in theory - and if the canary is corrupted, then an attack may have occurred, so the program should issue an alert and exit immediately.<sup><a href="#f6_114" name="b6_114">114</a></sup></p>
 
<div align="center">
<img src="img/mja01/fig6k.gif" alt="Figure 6.20. Canary placement"/>
<p><strong>Figure 6.20. Canary placement</strong></p>
</div>
<p>Support for canaries is provided by a language's compiler. Space for the canary must be added in each stack frame, code must be added at subroutine entry to initialize the canary, and code at subroutine exit must verify the canary's value for correctness. With all this code being added, overhead is a concern for canary-based defenses.</p>
<p>An attacker trying to compromise a program using canaries would have to overflow a buffer and overwrite control information as usual, <em>and</em> write the correct canary value so that the attack isn't discovered. There are three types of canary, distinguished by how they try and prevent an attacker from writing the correct canary value:</p>
<ol>
<li>Terminator canaries. Assuming that the most common type of stack smashing involves input and strings, a terminator canary uses a constant canary value which is a combination of four bytes, line and string terminators all: carriage return, newline, NUL, and -1 for good measure. The hope is that an attacker, sending these bytes to overwrite the canary correctly, would unwittingly end their input before the exploit succeeds.</li>
 
<li>Random canaries. The canary value can also be changed to prevent an attacker from succeeding; the theory is that an attacker must know the canary value in order to construct an exploit string. A random canary is a secret canary value that is changed randomly each time a program runs.<sup><a href="#f6_11" name="b6_11">11</a></sup> The random canary value for a program is stored in a global location, and is copied from this global location to a stack frame upon subroutine entry. The global location may possibly be situated in a read-only memory page to avoid being altered by an attacker. However, note that the presence of a format string vulnerability can be used by an attacker to find out the secret canary value.</li>
<li>Random XOR canaries. This is a random canary, with some or all of the control information XORed in with the canary for each stack frame. Any successful attack must set the canary appropriately - not an impossible task, but not an easy one either.</li>
</ol>
<p>Canaries can be extended to guard against some heap overflows as well, by situating a canary in the bookkeeping information of each dynamically-allocated block.<sup><a href="#f6_115" name="b6_115">115</a></sup> A general problem with canaries of any sort is that they only provide a perimeter guard for a memory area, and a program can still be attacked by overflowing a buffer onto other, unguarded variables within the guarded memory area.<sup><a href="#f6_116" name="b6_116">116</a></sup> A partial remedy is to alter the memory layout of variables, so that buffers are situated as close to a canary as possible, with no non-buffer variables in between.<sup><a href="#f6_117" name="b6_117">117</a></sup></p>
</li>
</ul></dd>
</dl>
<p>Generally, defenses to specific vulnerabilities that rely on the availability of source code or compilers won't work. Source code is not always available, as in the cases of third-party libraries and legacy code. Even if source code is available, compilers may not be, or users may lack the expertise or time to make source code changes, recompile, and reinstall.</p>
<h5><a name="c6152"></a>6.1.5.2 General Defenses</h5>
<p>Since most of the technical vulnerabilities stem from the use of programming languages with weaknesses, like the lack of bounds checking, one general approach is to stop using those languages. No more C, no more C++. This suggestion ignores many realities: legacy code, programmer training, programmer and management biases towards certain programming languages, the cost and availability of tools and compilers, constraints from third-party libraries. In any case, even if use of "weak" programming languages was stopped, history has shown that existing applications in those languages would linger in active use for decades.</p>
<p>A related idea is not to change programming languages, but to repair problems with an existing language after the fact. For example, bounds checking
 
could be added to C programs. Current approaches to bounds checking C code are dogged by problems: incomplete protection, breaking existing code. This is also an area where adding 'less than 26%' overhead is deemed to make a tool practical for use.<sup><a href="#f6_118" name="b6_118">118</a></sup></p>
<p>A more feasible defense is to randomize the locations of as many addresses as possible. If the locations of the stack, shared libraries, program code, and heap-allocated memory change each time the program is run, then an attacker's task is made more difficult.<sup><a href="#f6_119" name="b6_119">119</a></sup> However, it also makes legitimate debugging more difficult, in terms of finding spurious bugs, if these locations change non-deterministically. There is also evidence that the amount of randomization that can be provided is insufficient to prevent attacks completely.<sup><a href="#f6_120" name="b6_120">120</a></sup> A brute-force attack on a well-chosen target is possible, albeit much slower than attacking a system without any randomization.</p>
<p>A program's code can also be monitored as it runs, akin to behavior blocking anti-virus techniques.<sup><a href="#f6_12" name="b6_12">12</a></sup> The monitoring system looks for potential attacks by watching for specific abnormal behaviors, like a function return jumping into a buffer, or a return instruction not returning to its call site. The tricky part is pausing the monitored program's execution at critical points so that checks may be performed, without introducing excessive overhead, without modifying the monitored program's code. A solution comes in the form of caching:</p>
<ul>
<li>The monitoring system maintains a cache of code chunks that have already been checked against the monitor's security policy.</li>
<li>Cached code chunks run directly on the CPU, rather than using slow emulation, and a chunk returns control back to the monitor when it's done running.</li>
<li>Each control transfer is checked - if the destination corresponds to an already-cached code chunk, then execution goes to the cached chunk. Otherwise, the destination code chunk is checked for security violations and copied into the code cache.</li>
</ul>
<p>Code chunks in the cache can be optimized, mitigating some of the monitoring overhead.</p>
<h4><a name="c616"></a>6.1.6 Finding Weaknesses</h4>
<p>How do attackers find technical weaknesses in the first place? They can find the vulnerabilities themselves:</p>
<ul>
<li>Source code can be studied for vulnerabilities, when attacking a system where the source is available.<sup><a href="#f6_13" name="b6_13">13</a></sup> Even when a system is closed-source, it may be derived in part from a system with available source code.</li>
 
<li>Disassembly listings of programs and libraries can be manually searched, looking for opportunities. For example, an attacker could look for buffer-handling code or calls to format functions. While this may sounds daunting, it is never wise to underestimate the amount of free time an attacker will dedicate to a task like this.</li>
<li>Instead of poring over disassembly listings, an attacker can reconstruct a facsimile of the target program's source code using tools for reverse engineering, like decompilers. This provides a slightly higher-level view onto the target code.</li>
<li>Vulnerabilities can be discovered even without direct access to the target program or its source code. Treating the target program as a "black box" might be necessary if the target program is running on a remote machine for which the attacker doesn't have access.<sup><a href="#f6_14" name="b6_14">14</a></sup> For example, an attacker can look for buffer overflows by feeding a program inputs of various lengths until a suspicious condition is seen, like abruptly-terminated output. More information, such as the buffer's length, can be found through trial-and-error at that point by performing a binary search using different input lengths. Computers excel at repeating such mundane tasks, and finding the length of a buffer can be automated.<sup><a href="#f6_15" name="b6_15">15</a></sup>
<p>In general, any research on automated program-testing can be applied by an attacker. Such methods have a demonstrated ability to find long sequences of inputs which cause a program to misbehave.<sup><a href="#f6_16" name="b6_16">16</a></sup></p></li>
</ul>
<p>The other option an attacker has is to wait for someone else to find a vulnerability, or at least point the way:</p>
<ul>
<li>There are a number of <em>full disclosure</em> mailing lists. Advocates of full disclosure argue that the best way to force software vendors to fix a vulnerability is to release all its details, and possibly even code that exploits the vulnerabilities. (The extreme contrast to this is <em>security through obscurity</em>, which holds that hiding security-related details of a system means that attackers will <em>never</em> be able to figure them out. Again, underestimating an attacker is a bad strategy.) An exploit made available on a full-disclosure list can either be used directly, or might be used to indicate the direction of more serious problems in the targeted code.</li>
<li>A vendor security patch is a wealth of information. Either the patch itself can be studied to see what vulnerability itfixed,or a system can be compared before and after applying a patch to see what changed.
<p>Tools are available to help with the comparison task. All but the most trivial alteration to the patched executables will result in a flurry of binary changes: branch instructions and their targets are moved; information about
 
a program's symbols changes as code moves around; new code optimization opportunities are found and taken by the code's compiler. For this reason, tools performing a straight binary comparison will not yield much useful information to an attacker.<sup><a href="#f6_121" name="b6_121">121</a></sup></p>
<p>Useful binary comparison tools must filter out nonessential differences in the binary code. This is related to the problem of producing small patches for binary executables. Any observed difference between two executables must be characterized as either a primary change, a direct result of the code being changed, or a secondary change, an artifact of a primary change.<sup><a href="#f6_122" name="b6_122">122</a></sup> For example, an inserted instruction would be a primary change; a branch offset moved to accommodate the insertion is a secondary change. Spotting secondary changes can be done several ways:</p>
<ul>
<li>An architecture-dependent tool effectively disassembles the code to find instructions like branches which tend to exhibit secondary changes.<sup><a href="#f6_123" name="b6_123">123</a></sup></li>
<li>An architecture-independent tool can guess at the same information by assuming that code movements are small, only affecting the least-significant bytes of addresses.<sup><a href="#f6_124" name="b6_124">124</a></sup></li>
</ul>
<p>Naturally an attacker would only be interested in learning about primary changes, after probable secondary changes have been identified.</p>
<p>Other binary comparison approaches build "before" and "after" graphs of the code, using information like the code's control flow. A heuristic attempt is made to find an isomorphism between the graphs; in other words, the graphs are "matched up" as well as possible. Any subgraph that can't be matched indicates a possible change in the corresponding code.<sup><a href="#f6_125" name="b6_125">125</a></sup></p>
</li>
</ul>
<p>The Holy Grail for an attacker is the <em>zero-day exploit</em>, an exploit for a vulnerability that is made the same day as the vulnerability is announced - hopefully the same day that a patch for the vulnerability is released. From an attacker's point of view, the faster an exploit appears, the fewer machines that will be patched to plug the hole. In practice, software vendors are not always fast or forthcoming,<sup><a href="#f6_17" name="b6_17">17</a></sup> and an exploit may be well-known long before a patch for the vulnerability manifests itself.</p>
<h3><a name="c62"></a>6.2 Human Weaknesses</h3>
<p>Humans are the weakest link in the chain of security. Humans forget to apply critical security patches, they introduce exploitable bugs, they misconfigure software in vulnerable ways. There is even an entire genre of attacks based on tricking people, called <em>social engineering</em>.</p>
<p>Classic social engineering attacks tend to be labor-intensive, and don't scale well. Some classic ploys include:<sup><a href="#f6_126" name="b6_126">126</a></sup></p>
 
<ul>
<li>Impersonation. An attacker can pretend to be someone else to extract information from a target. For example, a "helpless user" role may convince the target to divulge some useful information about system access; an "important user" role may demand information from the target.<sup><a href="#f6_127" name="b6_127">127</a></sup></li>
<li>Dumpster diving. Fishing through garbage for useful information. "Useful" is a broad term, and could include discarded computer hard drives and backups with valuable data, or company organization charts suitable for assuming identities. Identity theft is another use for such information.</li>
<li>Shoulder surfing. Discovering someone's password by watching them over their shoulder as they enter it in.</li>
</ul>
<p>These classic attacks have limited application to malware. Even impersonation, which doesn't require the attacker to have a physical presence, works much better on the phone or in person.<sup><a href="#f6_128" name="b6_128">128</a></sup></p>
<p>Technology-based social engineering attacks useful for malware must be amenable to the automation of both information gathering and the use of gathered information. For example, usemames and passwords can be automatically used by malware to gain initial access to a system. They can be collected automatically with social engineering:</p>
<ul>
<li>Phony pop-up boxes, asking the user to re-enter their username and password.</li>
<li>Fake email about winning contests, directing users to an attacker's web site. There, the user must create an account to register for their "prize" by providing a username and password. People tend to re-use usernames and passwords to reduce the amount they must remember, so there is a high probability that the information entered into the attacker's web site will yield some real authentication information.
<p>The same principle can be used to lure people to an attacker's website to foist drive-by downloads on them. The website can exploit bugs in a user's web browser to execute arbitrary code on their machine, using the technical weaknesses described earlier.</p></li>
<li><em>Phishing</em> attacks send email which tricks recipients into visiting the attacker's web site and entering information. For example, a phishing email might threaten to close a user's account unless they update their account information. The attacker's web site, meanwhile, is designed to look exactly like the legitimate web site normally visited to update account information. The user enters their username and password, and possibly some other personal information useful for identity theft or credit card fraud, thus giving all this information to the attacker. Malware can use phishing to harvest usernames and passwords.</li>
</ul>
 
<blockquote>
<pre>
	If you receive an email titled "It Takes Guts to Say
	'Jesus'" do NOT open it. It will erase everything on
	your hard drive.
	
	Forward this letter out to as many people as you can.
	This is a new, very malicious virus and not many
	people know about it. This information was announced
	yesterday morning from IBM; please share it with
	everyone that might access the internet. Once again,
	pass this along to EVERYONE in your address book so
	that this may be stopped,

	AOL has said that this is a very dangerous virus and
	that there is NO remedy for it at this time. Please
	practice cautionary measures and forward this to all
	your online friends ASAP.
</pre>
</blockquote>
<div align="center">
<p><strong>Figure 6.21. "It Takes Guts to Say 'Jesus'" virus hoax</strong></p>
</div>
<p>User education is the best defense against known and unknown social engineering attacks of this kind. Establishing security policies, and teaching users what information has value, gives users guidelines as to the handling of sensitive information like their usemames and passwords.<sup><a href="#f6_129" name="b6_129">129</a></sup></p>
<p>Social engineering may also be used by malware to spread, by tricking people into propagating the malware along. And, one special form of "malware" that involves no code uses social engineering extensively: virus hoaxes.</p>
<h4><a name="c621"></a>6.2.1 Virus Hoaxes</h4>
<div class="epigraph">
'This virus works on the honor system. Please forward this message to everyone you know, then delete all the files on your hard disk.'
<p>- Anonymous<sup><a href="#f6_18" name="b6_18">18</a></sup></p>
</div>
<br clear="all"/>
<p>A <em>virus hoax</em> is essentially the same as a chain letter, but contains "information" about some fictitious piece of malware. A virus hoax doesn't do damage itself, but consumes resources - human and computer - as the hoax gets propagated. Some hoaxes may do damage <em>through</em> humans, advising a user to make modifications to their system which could damage it, or render it vulnerable to a later attack.</p>
<p>There are three parts to a typical hoax email:<sup><a href="#f6_130" name="b6_130">130</a></sup></p>
<ol>
<li>The hook. This is something that grabs the hoax recipient's attention.</li>
<li>The threat. Some dire warning about damage to the recipient's computer caused by the alleged virus, which may be enhanced with confusing "technobabble" to make the hoax sound more convincing.</li>
<li>The request. An action for the recipient to perform. This will usually include forwarding the hoax to others, but may also include modifying the system.</li>
</ol>
<p>Some examples are given in Figures 6.21 and 6.22.<sup><a href="#f6_19" name="b6_19">19</a></sup> Figure 6.21 is a classic virus hoax, whose only goal is to propagate. The virus hoax in Figure 6.22 is
 
slightly more devious, sending Windows users on a mission to find bear-shaped icons. As it turns out, this is the icon for a Java debugger utility which is legitimately found on Windows.</p>
<blockquote><pre>
	I found the little bear in my machine because of that I am sending this
	message in order for you to find it in your machine. The procedure is
	very simple:

	The objective of this e-mail is to warn all Hotmail users about a new
	virus that is spreading by MSN Messenger. The name of this virus is
	jdbgmgr.exe and it is sent automatically by the Messenger and by the
	address book too. The virus is not detected by McAfee or Norton and it
	stays quiet for 14 days before damaging the system.

	The virus can be cleaned before it deletes the files from your system.
	In order to eliminate it, it is just necessary to do the following
	steps:
	1. Go to Start, click "Search"
	2.- In the "Files or Folders option" write the name jdbgmgr.exe
	3.- Be sure that you are searching in the drive "C"
	4.- Click "find now"
	5.- If the virus is there (it has a little bear-like icon with the name
	of jdbgmgr.exe DO NOT OPEN IT FOR ANY REASON
	6.- Right click and delete it (it will go to the Recycle bin)
	7.- Go to the recycle bin and delete it or empty the recycle bin.

	IF YOU FIND THE VIRUS IN ALL OF YOUR SYSTEMS SEND THIS MESSAGE TO ALL
	OF YOUR CONTACTS LOCATED IN YOUR ADDRESS BOOK BEFORE IT CAN CAUSE ANY
	DAMAGE.
</pre></blockquote>
<div align="center">
<p><strong>Figure 6.22. "jdbgmgr.exe" virus hoax</strong></p>
</div>
<p>Why does a virus hoax work? It relies on some of the same persuasion factors as social engineering:<sup><a href="#f6_131" name="b6_131">131</a></sup></p>
<ul>
<li>A good hook elicits a sense of excitement, in the same way that a committee meeting doesn't. Hooks may claim some authority, like IBM, as their information source; this is an attempt to exploit the recipient's trust in authority.</li>
<li>The sense of excitement is enhanced by the hoax's threat. Overloading the recipient with technical-sounding details, in combination with excitement, creates an enhanced emotional state that detracts from critical thinking. Consequently, this means that the hoax may be subjected to less scrutiny and skepticism than it might otherwise receive.</li>
<li>The request, especially the request to forward the hoax, may be complied with simply because the hoax was persuasive enough. There may be other factors involved, though. A recipient may want to feel important, may want to ingratiate themselves to other users, or may genuinely want to warn others. A hidden agenda may be present, too - a recipient may pass the
 
hoax around, perceiving the purported threat as a way to justify an increase in the computer security budget.</li>
</ul>
<p>Virus hoaxes seem to be on the decline, possibly because they are extremely vulnerable to spam filtering. Even in the absence of technical solutions, education is effective. Users can be taught to verify a suspected virus hoax against anti-virus vendors' databases before sending it along; if the mail is a hoax, the chances are excellent that others have received and reported the hoax already.</p>
 
<p><strong>Notes for Chapter 6</strong></p>
<p><a name="f6_1" href="#b6_1">1</a> This is based on a simplified Unix memory model, with a few exceptions: the code segment is called the text segment, and what is lumped together here as the data segment is really a combination of the Unix data and bss segments.</p>
<p><a name="f6_2" href="#b6_2">2</a> Again, this is a simplification. An optimizing compiler may place some or all of a subroutine's stack frame into registers if possible, to avoid costly memory references. Some architectures, like the SPARC, are specifically designed to facilitate this.</p>
<p><a name="f6_3" href="#b6_3">3</a> Assumptions, assumptions, assumptions. RISC architectures tend not to push the return address, but dump it into a register so that it can be saved only if necessary.</p>
<p><a name="f6_4" href="#b6_4">4</a> An alternative is to replicate the new return address several times at the end, especially when the exact distance from the buffer to the return address on the stack isn't known.</p>
<p><a name="f6_5" href="#b6_5">5</a> On some systems, the stack location isn't consistent across executions of a program, but the environment variable location is, so the environment variable trick provides an alternative attack vector.</p>
<p><a name="f6_6" href="#b6_6">6</a> This assumes that array indexing starts from zero.</p>
<p><a name="f6_7" href="#b6_7">7</a> 16-bit numbers are used in this section for clarity, but the same idea works for numbers with any finite precision.</p>
<p><a name="f6_8" href="#b6_8">8</a> This is a simplified explanation, and doesn't take into account format functions for input, various obscure format functions, and format functions that take an opaquely-typed variable argument list rather than the arguments themselves.</p>
<p><a name="f6_9" href="#b6_9">9</a> The latter being a peculiarity of the "gcc" dialect of C, the implementation of which is described in Breuel [48].</p>
<p><a name="f6_10" href="#b6_10">10</a> OpenBSD allows this with their "W^X" scheme [85].</p>
<p><a name="f6_11" href="#b6_11">11</a> For multithreaded programs, each thread has its own stack. The random canary could thus be changed on a per-thread basis, with the canary's correct value placed in thread-local storage instead of a global location.</p>
<p><a name="f6_12" href="#b6_12">12</a> Kiriansky et al. [164] call this "program shepherding," and build their system on top of HP's Dynamo dynamic optimization system [25]. Renert [261] does largely the same thing, code cache and all (albeit permitting more general security policies), but neglects to mention the highly-related Dynamo work.</p>
<p><a name="f6_13" href="#b6_13">13</a> This includes, but is not restricted to, open-source systems. "Available" doesn't necessarily imply "freely," "easily," or "widely."</p>
<p><a name="f6_14" href="#b6_14">14</a> Yet.</p>
<p><a name="f6_15" href="#b6_15">15</a> This technique of finding "blind" buffer overflows is described in [84, 194].</p>
 
<p><a name="f6_16" href="#b6_16">16</a> For example, Chan et al. [60] apply an evolutionary learning algorithm to testing the game AI in Electronic Arts' FIFA-99 game.</p>
<p><a name="f6_17" href="#b6_17">17</a> To be fair - at least on the vendor speed issue - patches must be thoroughly tested, and the same vulnerability may exist in several of a vendor's products [224].</p>
<p><a name="f6_18" href="#b6_18">18</a> This is one of many variants of the "honor system virus" circulating. No traces of this particular one seem evident before May 2000, right after the release of the (non-hoax) ILOVEYOU email worm. An honor system virus was posted to Usenet around this time, but it's unclear if it is the original source or merely another derivative [181].</p>
<p><a name="f6_19" href="#b6_19">19</a> There are many different versions of these hoaxes floating around; the ones given here are edited to include the essential features of each.</p>
<p><a name="f6_100" href="#b6_100">100</a> Anderson [12].</p>
<p><a name="f6_101" href="#b6_101">101</a> This section is based on Aleph One [8].</p>
<p><a name="f6_102" href="#b6_102">102</a> Erickson [100].</p>
<p><a name="f6_103" href="#b6_103">103</a> The description of this attack is based on klog [167].</p>
<p><a name="f6_104" href="#b6_104">104</a> This section is based on [231, 292].</p>
<p><a name="f6_105" href="#b6_105">105</a> This section is based on Conover [78].</p>
<p><a name="f6_106" href="#b6_106">106</a> The description of this vulnerability is based on Solar Designer [293] and an anonymous author [18].</p>
<p><a name="f6_107" href="#b6_107">107</a> This categorization is due to Howard [147].</p>
<p><a name="f6_108" href="#b6_108">108</a> blexim [36], who also provides the XDR code from which Figure 6.16 was derived.</p>
<p><a name="f6_109" href="#b6_109">109</a> Miller et al. [209, page 39].</p>
<p><a name="f6_110" href="#b6_110">110</a> Cowan etal. [80].</p>
<p><a name="f6_111" href="#b6_111">111</a> This format string vulnerability discussion is based on scut [284].</p>
<p><a name="f6_112" href="#b6_112">112</a> Koziol etal. [171].</p>
<p><a name="f6_113" href="#b6_113">113</a> The defenses against format string vulnerabilities are from Cowan et al. [80].</p>
<p><a name="f6_114" href="#b6_114">114</a> This ornithological discussion is based on Wagle and Cowan [339].</p>
<p><a name="f6_115" href="#b6_115">115</a> Robertson et al. [266].</p>
<p><a name="f6_116" href="#b6_116">116</a> BulbaandKil3r[51].</p>
<p><a name="f6_117" href="#b6_117">117</a> Etoh[102].</p>
<p><a name="f6_118" href="#b6_118">118</a> Astonishingly, this claim is made in Ruwase and Lam [272, page 159].</p>
<p><a name="f6_119" href="#b6_119">119</a> A number of systems do this now: see Drepper [93] and de Raadt [85]. This type of randomization is one way to avoid software monocultures; see Just and Cornwall [157] for a discussion of other techniques.</p>
 
<p><a name="f6_120" href="#b6_120">120</a> Shacham et al. [285]. A related attack on instruction set randomization can be found in Sovarel et al. [296].</p>
<p><a name="f6_121" href="#b6_121">121</a> Hunt and Mcllroy [148] describe the early Unix diff utility.</p>
<p><a name="f6_122" href="#b6_122">122</a> We follow the terminology from Baker et al. [24].</p>
<p><a name="f6_123" href="#b6_123">123</a> Baker etal. [24].</p>
<p><a name="f6_124" href="#b6_124">124</a> Percival [246].</p>
<p><a name="f6_125" href="#b6_125">125</a> Flake [110] and Sabin [273].</p>
<p><a name="f6_126" href="#b6_126">126</a> Granger [128].</p>
<p><a name="f6_127" href="#b6_127">127</a> Also in Allen [10].</p>
<p><a name="f6_128" href="#b6_128">128</a> Harl [136].</p>
<p><a name="f6_129" href="#b6_129">129</a> Granger [129].</p>
<p><a name="f6_130" href="#b6_130">130</a> CIAC [72].</p>
<p><a name="f6_131" href="#b6_131">131</a> Based on Gordon et al. [126], Gragg [127], and Granger [128].</p>
 
 
<h2><a name="c7"></a>Chapter 7 Worms</h2>
<p>The general structure of a worm is:</p>
<pre class="source">
	def worm():
	    propagate()
	if trigger() is true:
	    payload()
</pre>
<p>At this level of abstraction, there is no distinction between a worm and a virus. (For comparison, the virus pseudocode is on page 27.) The real difference is in how they propagate. Propagating by infecting other code is the domain of a virus; actively searching for vulnerable machines across a network makes a worm. A worm can either be said to <em>infest</em> or <em>infect</em> its victims; the latter term will be used here. A single copy of a worm will be called a worm <em>instance</em>, where it's necessary to avoid ambiguity.</p>
<p>In some cases, worms are classified by the primary method they use for transport. A worm using instant messaging (IM) to spread is called an <em>IM worm</em>, and a worm using email is an <em>email worm</em>. For example, many email worms arrive as an email attachment, which the user is tricked into running. When run, the worm harvests email addresses off the machine and mails itself to those addresses.</p>
<p>Tricking users into doing something is social engineering, and this is one mechanism that worms use to infect machines. Another mechanism that worms exploit for infection are technical weaknesses. A user doesn't have to be tricked into running an email attachment, if just viewing the email allows the worm's code to execute via a buffer overrun. A user doesn't have to be involved at all, if the worm spreads using buffer overruns between long-running network server processes on different machines.</p>
 
<p>A worm can also exploit existing, legitimate transactions. For example, consider a worm able to watch and modify network communications, especially one located on a network server machine. The worm can wait for legitimate transfers of executable files - file transfers, network filesystem use - and either substitute itself in place of the requested executable file, or insert itself into the requested file in a virus-like fashion.<sup><a href="#f7_100" name="b7_100">100</a></sup></p>
<p>Most of the details about worms have already been covered in earlier chapters, like technical weaknesses and human weaknesses. Worms can also employ the same techniques that viruses do in order to try and conceal themselves; worms can use encryption, and can be oligomorphic, polymorphic, or metamorphic. This chapter therefore only examines the propagation which makes worms distinct from viruses, beginning with a look at two historically important worms.</p>
<h3><a name="c71"></a>7.1 Worm History</h3>
<p>The origins of the term "worm" were mentioned in Section 2.1.5, along with some examples of early worms. This section examines two of these in more depth: the Xerox worm and the Internet worm.</p>
<h4><a name="c711"></a>7.1.1 Xerox PARC, c. 1982</h4>
<div class="epigraph">
'All worm behavior ceased. Unfortunately the embarrassing results were left for all to see: 100 dead machines scattered around the building.'
<p>- John Shoch and Jon Hupp<sup><a href="#f7_101" name="b7_101">101</a></sup></p>
</div>
<br clear="all"/>
<p>The worm used in the Xerox PARC experiments of the early 1980s wasn't <em>intentionally</em> malicious, despite the above quote. It was intended as a framework for distributed computation, to make use of otherwise unused CPU time. A user would write a program to run in parallel, on many different machines - this program would sit atop the worm mechanism, and the worm would manage the mechanics of making the program run on different machines.</p>
<p>It would be highly unusual to see a worm now that places an artificial limit on its own propagation, but that was exactly what the Xerox worm did. The Xerox worm was composed of multiple <em>segments</em>, by way of analogy to real biological worms; at most one worm segment could run on any one machine. A bounded, finite number of segments were started, and all segments kept in contact with one another. If the worm lost contact with a segment (for example, someone rebooted the machine that the segment was running on), then the other segments sought another machine upon which to run a segment.</p>
<p>Safety mechanisms were built in to the worm. This was done in part to assuage user concerns about having such a beast running on their computer; segments were not allowed to perform disk accesses, for example. Keeping segments in contact had a safety benefit, too, in that the entire worm could be shut down with a single command. And was, in the case from which the above
 
quote was taken. The worm had gone out of control through an odd set of circumstances, and had to be stopped.</p>
<p>One of the key insights the researchers at Xerox PARC made from their worm experiments was that managing worm growth and stability are hard problems.</p>
<h4><a name="c712"></a>7.1.2 The Internet Worm, November 1988</h4>
<div class="epigraph">
'We are currently under attack from an Internet VIRUS. It has hit UC Berkeley, UC San Diego, Lawrence Livermore, Stanford, and NASA Ames.'
<p>- Peter Yee<sup><a href="#f7_102" name="b7_102">102</a></sup></p>
</div>
<br clear="all"/>
<p>Whether people called it a worm or a virus, the Internet worm was a major wake-up call for Internet security.<sup><a href="#f7_1" name="b7_1">1</a></sup> It worked in three stages:</p>
<dl>
<dt><strong>Stage 1</strong></dt><dd>The first stage was for the worm to get a shell on the remote machine being targeted. The worm would use one of three methods to acquire a shell, mostly relying on security holes of mythic proportion:
<ol>
<li>Users read and send email using mail programs which are generically called <em>mail user agents</em>. The actual gory details of transmitting and delivering mail across the network are handled by daemons called <em>mail transport agents</em>. Mail user agents send mail by passing it off to a mail transport agent, which in turn can talk to mail transport agents on different machines, to send the mail along its merry way.
<p><tt>Sendmail</tt> was a widely-used mail transport agent at the time of the Internet worm. An example of sending mail, by talking to the <tt>sendmail</tt> daemon, is in Figure 7.1. Simple commands are used to identify the connecting machine, specify the mail's sender and receiver, send the mail, and complete the connection.</p>
<p>Older versions of <tt>sendmail</tt> also supported a "debug" command, which allowed a remote user to specify a <em>program</em> as the email's recipient, without any authentication. The Internet worm trivially exploited this to start a shell on the remote machine.</p></li>
		<li>The <tt>finger</tt> program was a user program which could be used to discover information about another Unix user; indeed, it was once common to sit in a terminal room and finger people. A sample output is shown in Figure 7.2.
			<p>This example gets its information from the local machine only, but using an @ sign in the command line</p>
			<pre>finger <a class="__cf_email__" href="/cdn-cgi/l/email-protection" data-cfemail="97f6eef4f8f4fcd7f4e7e4f4b9e2f4f6fbf0f6e5eeb9f4f6">[email&#160;protected]</a><script data-cfhash='f9e31' type="text/rocketscript">/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */</script></pre>
			<p>would cause information to be requested about the user from the specified machine. Finger would make a network connection to the <em>finger daemon</em> on the remote machine and send a query about the user.</p>
<!--p.145-->
<pre>
220 mail.cpsc.ucalgary.ca ESMTP Sendmail
<strong>helo mymachine</strong>
250 mail.cpsc.ucalgary.ca Hello mymachine, pleased to meet you
<strong>mail from: elvis</strong>
250 2.1.0 elvis... Sender ok
<strong>rcpt to: aycock</strong>
250 2.1.5 aycock... Recipient ok
<strong>data</strong>
354 Enter mail, end with ''." on a line by itself
<strong>From: elvis
To: aycock
Siibject: the present you sent
Thank you, thank you very much.
Sincerely,
The King
.</strong>
250 2.0.0 hAQHNJxY0022 01 Message accepted for delivery
<strong>quit</strong>
221 2.0.0 mail.cpsc.ucalgary.ca closing connection
</pre>
<div align="center">
	<p><strong>Figure 7.1. A conversation with sendmail</strong></p>
</div>
<pre>
% <strong>finger aycock</strong>
Login: aycock                           Name: John Aycock
Directory: /home/aycock                 Shell: /bin/sh
On since Wed Nov 26 09:38 (MST) on pts/2 from serverl
No mail.
No plan.
</pre>
<div align="center">
	<p><strong>Figure 7.2. Finger output</strong></p>
</div>
			<p>The finger daemon read input from the network connection using C's <code>gets</code> function, which does no bounds checking on what it reads in. The Internet worm would exploit this by performing a stack smashing attack against the finger daemon to get a shell.</p>
		</li>
		<li>Several utility programs used to permit a user to run commands on a remote machine. The Internet worm tried two of these in an effort to get a remote shell: <tt>rexec</tt> and <tt>rsh</tt>.
			<p>Rexec required a password to log in to the remote machine. The worm's third stage would guess at passwords, trying obvious ones like the username, and mounting a dictionary attack too. A <em>dictionary attack</em> is where an attacker has a dictionary of commonly-used words, which are
<!--p.146-->
			tried one by one as potential passwords. The worm's third stage carried a list of 432 words that it used for this purpose.</p>
			<p>Rsh could be even more accommodating for getting a remote shell. It had a facility where users coming from specified "trusted" hosts didn't even have to supply a password.</p>
		</li>
	</ol></dd>
	<dt><strong>Stage 2</strong></dt><dd>Once a shell was obtained on the remote machine, the worm would send commands to create, compile, and run a small C program on the machine being infected. This program was portable across the prevalent Unix architectures of the time, and had another technical advantage. Because it was sent in source form, it was immune to damage from communication channels which only passed through seven bits out of eight, which would have destroyed a binary executable file.
		<p>The compiled program was a "grappling hook" which was used to pull the worm's Stage 3 executable files onto the machine being infected.<sup><a href="#f7_103" name="b7_103">103</a></sup> When run, the grappling hook would make a network connection <em>back</em> to the infecting machine (whose worm instance was expecting the incoming connection). This connection was used to transfer several Stage 3 executables, one for each architecture that the worm could infect. These executables would be tried until one succeeded in running.</p></dd>
	<dt><strong>Stage 3</strong></dt><dd>At this point, the worm was fully established on the infected machine, and would begin trying to propagate itself to other machines.
		<p>Some rudimentary stealth measures were deployed. The worm named itself "sh" to appear as a user shell, and modified its command-line arguments; both these would make the running worm process look unremarkable to a casual observer. Previous remnants, like temporary files used for compiling the grappling hook, were removed. Finally, the worm prevented "core dump" files from being created - a Unix system will create core dump files for processes incurring fatal errors, or upon receipt of an explicit user request to do so. This latter step prevented easy capture of worm samples for analysis.</p>
		<p>New target machines were selected using information from the infected machine. Information from network interfaces, network routing tables, and various files containing names of other computers were all used to locate new machines to try and infect.</p>
	</dd>
</dl>
<p>The Internet worm carried no destructive payload. Worm damage was collateral, as each worm instance simply used lots and lots of machine and network resources.</p>
<!--p.147-->
<div align="center">
	<img src="img/mja01/fig73.gif" alt="Figure 7.3. TCP connection establishment" />
	<p><strong>Figure 7.3. TCP connection establishment</strong></p>
</div>
<h3><a name="c72"></a>7.2 Propagation</h3>
<p>Humans are slow, compared to computers and computer networks. Worms thus have the potential to spread very, very quickly, because humans don't have to be involved in the process of worm propagation.<sup><a href="#f7_104" name="b7_104">104</a></sup></p>
<p>At the extreme end of the scale are <em>fast burners</em>, worms that propagate as fast as they possibly can. Some of these worms have special names, reflecting their speed. A <em>Warhol worm</em> infects its vulnerable population in less than 15 minutes; this name comes from artist Andy Warhol's famous quote 'In the future everyone will be famous for fifteen minutes.' A <em>flash worm</em> goes one better, and infects the vulnerable population in a matter of seconds.</p>
<p>How can a worm propagate this quickly? With a combination of these methods:</p>
<ul>
	<li>Shorten the initial startup time. Recalling the worm growth curve in Figure 1.2 (page 5), this shifts the growth curve to the left.</li>
	<li>Minimize contention between instances of the worm. This includes avoiding general contention in network traffic, as well as avoiding pointless attempts to re-infect the same machine.</li>
	<li>Increase the rate at which potential target machines are probed, by scanning them in parallel. This is a tradeoff, because such network activity can result in network traffic contention.</li>
	<li>Attack using low-overhead network protocols. The less back-and-forth that a network protocol requires, the faster a worm using that protocol can spread. The Slammer worm, for example, used the User Datagram Protocol (UDP) to infect SQL database servers using a buffer overflow.<sup><a href="#f7_105" name="b7_105">105</a></sup> UDP is a lightweight, connectionless protocol: there is no overhead involved to set up a logical network connection between two computers trying to communicate. From Slammer's point of view, this meant one network packet, one new victim.
		<p>In contrast, worms using a connection-based network protocol like the Transmission Control Protocol (TCP) have several packets' worth of overhead to establish a connection, before any exploit can even be started.<sup><a href="#f7_106" name="b7_106">106</a></sup></p>
<!--p.148-->
		<p>Figure 7.3 shows two computers establishing a connection using TCP. Each computer must send one SYN packet to the other, and acknowledge the other's SYN packet with an ACK. (One ACK is "piggybacked" on a SYN packet, so only three packets are exchanged in practice.) All this network traffic occurs <em>before</em> a worm is able to talk to and exploit a server, making TCP-based worms much slower in comparison to UDP-based worms.</p></li>
</ul>
<p>At the other end of the speed scale are <em>surreptitious worms</em> that deliberately propagate slowly to avoid notice. Such a worm might hide in normal network traffic, like the network traffic for file-sharing. Slow propagation might be used to build up a zombie army for a massive DDoS attack, or for any other purpose for which a botnet might be used.</p>
<p>In the remainder of this section, the initial spread of a worm is considered, as are ways that a worm finds new victim machines to infect.</p>
<h4><a name="c721"></a>7.2.1 Initial Seeding</h4>
<p>Worms need to be injected into a network somehow. The way that a worm is initially released is called <em>seeding</em>, A single network entry point would be relatively easy to trace back to the worm author, and start the worm's growth curve at its lowest point. An effective seeding method should be as anonymous and untraceable as possible, and distribute many instances of the worm into the network.</p>
<p>Three possibilities have been suggested:</p>
<ul>
	<li>Wireless networks. There are many, many wireless networks connected to the Internet with little or no security enabled.<sup><a href="#f7_107" name="b7_107">107</a></sup> Using wireless networks for seeding satisfies the anonymity criterion, although physical proximity to the wireless access point by the worm writer is required, making this option not entirely risk-free. Barring a co-ordinated release, however, this method of seeding doesn't scale well to injecting large worm populations. A co-ordinated release is risky, too, as many people will know about the worm and its creator.</li>
	<li>Spam. Seeding a worm by spamming the worm out to people can satisfy both effectiveness criteria: anonymity and volume. Spamming can be used to seed worms even when the worm doesn't normally propagate through email.</li>
	<li>Botnets. Botnets may be used in several ways for seeding and, like spamming, meet both effectiveness criteria. Botnets may be used to send the worm's seeding spam, and they may be also used to release the worm directly in a highly-distributed way.</li>
</ul>
<p>Access to common network services can be had in a hard-to-trace way, so this list is far from complete.</p>
<!--p.149-->
<div align="center">
	<img src="img/mja01/fig74.gif" alt="Figure 7.4. IP address partitioning" />
	<p><strong>Figure 7.4. IP address partitioning</strong></p>
</div>
<h4><a name="c722"></a>7.2.2 Finding Targets</h4>
<p>On the Internet, a machine is identified in two ways: by a domain name and an Internet Protocol (IP) address. Domain names are a convenience for humans; they are human-readable and are quietly mapped into IP addresses. IP addresses, which are just numbers, are used internally for the real business of networking, like routing Internet traffic from place to place.</p>
<p>IP addresses come in two flavors, distinguished by a version number: the most prevalent kind now are version 4 addresses (<em>IPv4</em>), but support for version 6 addresses (<em>IPv6</em>) is increasing. IPv4 addresses are shorter, only 32 bits compared to IPv6's 128 bits, and the same principles apply to both in terms of worm propagation; this book will use IPv4 addresses for conciseness.</p>
<p>The bits of an IP address are partitioned to facilitate routing packets to the correct machine. Part of the address describes the network, part identifies the computer (host) within that network. IP addresses are categorized based on their size:</p>
<table summary="IP network classes">
<tr><th>Network Class</th><th>Network Bits</th><th>Host Bits</th></tr>
<tr><td>Class A</td><td>8</td><td>24</td></tr>
<tr><td>Class B</td><td>16</td><td>16</td></tr>
<tr><td>Class C</td><td>24</td><td>8</td></tr>
</table>
<p>For example. Figure 7.4 breaks down the IP address for the web server at the University of Calgary's Department of Computer Science. The University of Calgary has a class B address, 136.159; its host part is further subdivided, to identify a subnet, 5, and the exact host on that subnet, 17.</p>
<p>Why is this relevant to worms? A worm has to identify a potential target machine. For worm propagation, it is substantially easier for a worm to guess at an IP address and find a target than it is for a worm to guess correctly at a domain name.</p>
<!--p.150-->
<p>A worm looking for machines to infect is said to <em>scan</em> for targets; this is different from the use of "scan" to describe anti-virus activity. There are five basic strategies that a worm can use to scan:</p>
<dl>
	<dt><strong>Random scanning</strong></dt><dd>A worm may pick a target by randomly choosing a value to use as an IP address. This was done, for example, by Code Red I. Choosing an IP address randomly can select a target literally anywhere in the world.</dd>
	<dt><strong>Localized scanning</strong></dt><dd>Random scanning is good for widespread distribution, but it's a hit-and-miss approach for worms exploiting technical vulnerabilities to spread. It is much likelier that computers on the same network, in the same administrative domain, are going to be maintained in a similar fashion. For example, if one Windows machine on a network has an unpatched buffer overflow, the chances are good that another Windows machine on the same network is going to be unpatched too.
		<p><em>Localized scanning</em> tries to take advantage of this. Target machines are again chosen randomly, but with a bias towards local machines; a "local machine" is heuristically selected by taking advantage of the IP address partitioning described above. For example. Code Red II picked target IP addresses in this way:<sup><a href="#f7_108" name="b7_108">108</a></sup></p>
		<table summary="">
		<tr><th>Probability</th><th>Target Selection</th></tr>
		<tr><td>1/8</td><td>All four bytes randomly chosen</td></tr>
		<tr><td>3/8</td><td>Only last two bytes randomly chosen</td></tr>
		<tr><td>4/8</td><td>Last three bytes randomly chosen</td></tr>
		</table></dd>
	<dt><strong>Hit-list scanning</strong></dt><dd>Prior to worm release, a "hit-list" can be compiled which contains the IP addresses of some machines known to be vulnerable to a technical flaw the worm plans to exploit. Compiling such a list is a possible application for a previously-released surreptitious worm, or a botnet. The list need not be 100% accurate, since it will only be used as a starting point, and doesn't need to contain a large number of IP addresses - 50,000 or less are enough.
		<p>After its release, the worm starts by targeting the machines named in the hit-list. Each time the worm successfully propagates, it divides the remainder of the list in half, sending half along with the new worm instance. Once the list is exhausted, the worm can fall back onto other scanning strategies.</p>
		<p>Hit-list scanning is useful for two reasons:</p>
		<ol>
			<li>Avoiding contention. The hit-list keeps multiple instances of a worm from targeting the same machines.
<!--p.151-->
<div align="center">
	<img src="img/mja01/fig75.gif" alt="Figure 7.5. Permutation scanning" />
	<p><strong>Figure 7.5. Permutation scanning</strong></p>
</div></li>
			<li>Speeding up initial spread. By providing a list of known targets, slow propagation by trial-and-error is avoided, and the worm's growth curve shifts to the left as a result.</li>
		</ol>
		<p>A variation on the hit-list scheme precompiles a list of <em>all</em> vulnerable machines on the Internet, and sends it along with the worm in compressed form.</p>
	</dd>
<!--p.152-->
	<dt><strong>Permutation scanning</strong></dt><dd>If a worm is able to tell whether or not a target candidate is already infected, then other means of contention avoidance can be used. <em>Permutation scanning</em> is where instances of a worm share a common permutation of the IP address space, a pseudo-random sequence over all <img src="/img/cache/f19901f1c817ad846a411e6712e8db66.gif" alt="2^{32}" valign="middle" /> possible IP address values. Each new instance is given a position in the sequence at which to start infecting, and the worm continues to work through the sequence from there. Figure 7.5 has an example for a ten-value permutation.
		<p>If a machine is encountered which is already infected, then the worm picks a new spot in the sequence randomly. This gives the worm a simple mechanism for distributed coordination without any communication overhead between worm instances. (Interestingly, peer-to-peer networks for file sharing share the same need for low-overhead distributed coordination.<sup><a href="#f7_109" name="b7_109">109</a></sup>)</p>
		<p>This coordination mechanism can be used by the worm to heuristically detect saturation, too. If a worm instance continually finds already-infected machines, despite randomly resituating itself in the permutation sequence, then it can serve as an indicator that most of the vulnerable machines have been infected. More generally, a worm can mathematically model its own growth curve, to estimate how close it is to the saturation point.<sup><a href="#f7_110" name="b7_110">110</a></sup> The saturation point can signal the opportune time to release a payload, because there is little more to do in terms of spreading, and countermeasures to the worm are doubtlessly being deployed already.</p></dd>
	<dt><strong>Topological scanning</strong></dt><dd>Information on infected machines can be used to select new targets, instead of using a random search. This is called <em>topological scanning</em>, because the worm follows the topology of the information it finds.
		<p>The topology followed may or may not coincide with the physical network topology. A worm may follow information about a machine's network interfaces to new target hosts, but other types of information can result in propagation along <em>social</em> networks. Email worms can mail themselves to email addresses they mine off an infected machine, and IM worms can send themselves to people in a victim's "buddy list."<sup><a href="#f7_111" name="b7_111">111</a></sup></p>
		<p>Topological scanning is particularly useful for propagation in large, sparse address spaces. The Internet worm, for example, used topological scanning due to the relatively small number of machines in the IP address space of 1988. In contrast, random scanning would waste a lot of effort locating targets in such an address space.</p></dd>
	<dt><strong>Passive Scanning</strong></dt><dd>A surreptitious worm can wait for topological information to come to it. A <em>passive scanning</em> worm can eavesdrop, or <em>sniff</em>, network traffic to gather information about:<sup><a href="#f7_112" name="b7_112">112</a></sup>
<!--p.153-->
	<ul>
		<li>Valid IP addresses. The worm can gather the addresses of potefttial targets in a way that dodges some of the worm countermeasures in the next chapter.</li>
		<li>Operating system and services. A worm can benefit from knowing a target machine's operating system type, operating system version, network services, and network service versions.<sup><a href="#f7_2" name="b7_2">2</a></sup> Worms able to exploit multiple technical weaknesses can pick a suitable infection vector, and other worms can rule out unsuitable targets.</li>
		<li>Network traffic patterns. A slow worm can limit its network activity to times when there is normally legitimate network activity. The other network activity can act as cover traffic for the worm's operation.</li>
	</ul></dd>
</dl>
<p>In some cases targets have already been identified for other reasons, and a worm need only extract the information. For example, the Santy worm exploited a flaw in web software, and used Google to search for targets.<sup><a href="#f7_113" name="b7_113">113</a></sup></p>
<p>Putting all the pieces together - virus-like concealment, exploitation of technical and human weaknesses, hijacking legitimate transactions, extremely rapid spreading - worms are a very potent type of malware. Equally potent defensive measures are needed.</p>
<!--p.154-->
<p><strong>Notes for Chapter 7</strong></p>
<p><a name="f7_1" href="#b7_1">1</a> It wasn't just Yee that referred to it as a virus. Of the two primary sources used for this section, one calls it a virus, one a worm, both argue their case: Eichin and Rochlis [97] and Spafford [298].</p>
<p><a name="f7_2" href="#b7_2">2</a> This is called <em>passive fingerprinting</em> [301].</p>
<p><a name="f7_100" href="#b7_100">100</a> Nazario et al. [230].</p>
<p><a name="f7_101" href="#b7_101">101</a> Shoch and Hupp [287, page 176]. This section on the Xerox worm was based on this source too.</p>
<p><a name="f7_102" href="#b7_102">102</a> Yee [350].</p>
<p><a name="f7_103" href="#b7_103">103</a> The term "grappling hook" is from Eichin and Rochlis [97].</p>
<p><a name="f7_104" href="#b7_104">104</a> This section is based on Stamford et al. [304].</p>
<p><a name="f7_105" href="#b7_105">105</a> Sz6randPerriot[315].</p>
<p><a name="f7_106" href="#b7_106">106</a> McKusick et al. [202].</p>
<p><a name="f7_107" href="#b7_107">107</a> Stampf [302] mentions the worm potential in wireless forms of communication.</p>
<p><a name="f7_108" href="#b7_108">108</a> CERT [55].</p>
<p><a name="f7_109" href="#b7_109">109</a> Wiley [346].</p>
<p><a name="f7_110" href="#b7_110">110</a> Vogt [337]. Ma et al. [190] analyze self-stopping worms in great detail.</p>
<p><a name="f7_111" href="#b7_111">111</a> Hindocha and Chien [142].</p>
<p><a name="f7_112" href="#b7_112">112</a> Nazarioetal. [230].</p>
<p><a name="f7_113" href="#b7_113">113</a> Hypponen [150].</p>
<!--p.155-->
<!--p.156-->
<h2><a name="c8"></a>Chapter 8 Deworming</h2>
<p>Work on handling worms, from a defender's point of view, can be classified three ways: defense, worm capture and containment, and automatic counter-measures. This chapter follows that organization.</p>
<div align="center">
	<img src="img/mja01/fig81.gif" alt="Figure 8.1. An example network" />
	<p><strong>Figure 8.1. An example network</strong></p>
</div>
<p>Most of the techniques described here can be illustrated on a network like the one in Figure 8.1. An internal network is connected to the Internet through some
<!--p.157-->
computer at the network's perimeter. The nature of this perimeter computer has been left deliberately vague; it can be a dedicated network router or a general-purpose computer, which may be performing a variety of defensive tasks in addition to shuffling network packets back and forth. The internal network has a critical subnet, a set of machines which special pains must be taken to protect. There is the user and their computer, which is a host on the network. Finally, a computer acting as a "honeypot" may be present, whose role will be described in Section 8.2.1.</p>
<p>First, defense.</p>
<h3><a name="c81"></a>8.1 Defense</h3>
<p>How can worms be kept out in the first place? Looking at the path from the Internet to the user in Figure 8.1, defensive measures can be taken at any point along that path.</p>
<h4><a name="c811"></a>8.1.1 User</h4>
<p>User education can't be forgotten. Education is especially useful to prevent the propagation of email worms that require an attachment to be run by a human. Users can also be thought of as finely-attenuated sensors which detect the most insignificant slowdowns in network speed, a fact to which any network administrator can attest. Network traffic from worms that is otherwise hidden may be detected by users.</p>
<h4><a name="c812"></a>8.1.2 Host</h4>
<p>The next line of defense is the user's computer; defenses deployed here are called <em>host-based</em> defenses. Some of the best defenses against worms are the most mundane: applying patches, limiting the amount of available services on a machine. From there, defenses specific to likely attack vectors are the next step, followed by anti-virus software being used on the host to look for worms.</p>
<h5><a name="c8121"></a>8.1.2.1 Patching</h5>
<p>Many intrusions by malware are completely preventable. A lot of worms do not exploit previously-unknown vulnerabilities, but use known vulnerabilities for which a patch is available. Illustrated in Figure 8.2, the rate of patching is an exponential decay curve which never reaches zero.<sup><a href="#f8_100" name="b8_100">100</a></sup> In other words, many machines remain vulnerable for a long period of time after a patch is available, and some machines are never patched. The situation is even worse: the overall patching rate does not change dramatically even when a widely-publicized worm is circulating, exploiting the vulnerability.<sup><a href="#f8_1" name="b8_1">1</a></sup> Studies of a number of security vulnerabilities for which patches are available have shown similar, discouraging results.<sup><a href="#f8_101" name="b8_101">101</a></sup></p>
<!--p.158-->
<div align="center">
	<img src="img/mja01/fig82.gif" alt="Figure 8.2. Rate of patching over time" />
	<p><strong>Figure 8.2. Rate of patching over time</strong></p>
</div>
<p>There may be a variety of excellent reasons for the laxity of patching.<sup><a href="#f8_102" name="b8_102">102</a></sup> Qualified personnel may not be available to apply a patch, or may not have time. People may not know about the patch. Bureaucratic issues may preclude proper maintenance, or policy matters may prevail - for example, some companies require updates to be tested before distributing them. This policy may be seen to be a prudent precaution, because applying some patches (especially hastily-prepared ones made in response to a vulnerability) may break more software than it fixes.</p>
<p>New commodity operating systems (e.g., Windows, MacOS) have automated update systems which notify a user that updates are available, and lead them through the process of downloading and installing the updates. Not everyone runs the newest version of an operating system, and policy may trump the use of automated updates, but in the long term, automated update systems will probably have a positive impact on security.</p>
<h5><a name="c8122"></a>8.1.2.2 Limiting Available Services</h5>
<p>The reasoning for limiting available services comes from two premises:</p>
<ol>
	<li>Worms exploit technical weaknesses, like buffer overflows, in network servers. (Here, a server refers to the program that supplies a particular service.)</li>
	<li>Technical weaknesses are likely to evenly manifest themselves throughout network server code.</li>
</ol>
<p>Based on these premises, the conclusion can be drawn that the more network servers a machine runs, the likelier it is that some technical weakness is exposed that a worm can exploit. The corollary is that the fewer network servers a machine runs, the more secure it is.</p>
<p>While the soundness of this logic may be debated, the general idea of reducing potential attack vectors to defend against worms is a good one. There are
<!--p.159-->
pragmatic aspects to limiting network servers, too, in that it also limits the amount of software to patch.</p>
<p>The hard part is determining which servers to shut down. This can involve much trial-and-error even for experts, turning off one server after another to see if it affects the machine's operation. Some effects may only be apparent after an extended period of time, if a server is shut down that only sees occasional use.</p>
<h5><a name="c8123"></a>8.1.2.3 Countermeasures against Specific Weaknesses</h5>
<p>Besides trying to reduce the number of running servers that might contain weaknesses, countermeasures can be used to guard against specific kinds of technical weakness that are exploited by worms. A number of these were presented in Section 6.1.5, such as:</p>
<ul>
	<li>Canaries to detect buffer overflows</li>
	<li>Randomizing memory locations to make finding memory addresses harder</li>
	<li>Code monitoring to watch for unusual behavior</li>
</ul>
<p>Countermeasures to specific technical weaknesses are certainly an important part of worm defense. However, such countermeasures are based on assumptions about how worms are likely to break into a system. They are of little use against any new types of technical vulnerability that do not happen to be guarded against, and they do not catch worms that use social engineering to spread.</p>
<h5><a name="c8124"></a>8.1.2.4 Anti-Virus Software</h5>
<p>Anti-virus software can and does look for worms, but there are three major problems that hamper anti-virus software's effectiveness:</p>
<ol>
	<li>To function properly, anti-virus software detecting known worms needs an up-to-date virus database, but virus database updates cannot be prepared and deployed fast enough to counter rapidly-spreading worms.</li>
	<li>Some powerful anti-virus techniques are unusable: integrity checking and emulation certify a program as malware-free at the start of the program's execution. These techniques are useless against a worm that injects its code into a running program which has already been declared clean.</li>
	<li>A worm need not reside someplace that anti-virus software can analyze. Many anti-virus techniques are geared to catch malware that writes itself somewhere in a computer's filesystem; a worm that exploits a buffer overflow in a long-running network server can remain in memory only, undetected.</li>
</ol>
<!--p.160-->
<p>This suggests that anti-virus software is no panacea for worm defense. The last problem, detecting in-memory threats, can at least be addressed.</p>
<h5><a name="c8125"></a>8.1.2.5 Memory Scanning</h5>
<p>Searching for in-memory threats is called <em>memory scanning</em>. Once, memory scanning was an easy task for anti-virus software: the amount of physical memory on machines was small, and any program could read from any part of the memory. Now, two features have made their way into almost all operating systems, both of which complicate memory scanning:</p>
<dl>
	<dt><strong>Memory protection.</strong></dt><dd>Hardware protection prevents one process from accessing another process' memory unless they have both explicitly agreed to share memory. This memory protection greatly increases system stability, because it limits the effect of a misbehaving process - malicious or otherwise. The drawback from the anti-virus point of view is that memory protection prevents a memory scanner from directly looking at other processes' memory.</dd>
	<dt><strong>Virtual memory.</strong></dt><dd>The operating system and hardware can conspire to provide <em>virtual memory</em> to processes. With virtual memory, each process thinks it has an enormous amount of memory to use, more memory than is physically available. The virtual memory contents are stored on disk, and the physical memory acts as a cache for the virtual memory. The operating system, with hardware support, traps virtual memory references that refer to virtual memory pages which are not currently present in physical memory. The operating system arranges for the absent memory pages to be loaded from disk to physical memory, possibly evicting some physical memory contents to make room.
		<p>Disks are orders of magnitude slower than physical memory. If a process were to randomly access its virtual memory, it would slow to a crawl waiting for memory pages to be loaded from disk. Fortunately, that rarely happens. Most programs naturally exhibit a high degree of <em>locality of reference</em>, meaning that they tend to reference only a small set of memory areas at any given time.<sup><a href="#f8_2" name="b8_2">2</a></sup> As a program's execution continues, the set of memory areas referenced changes to a <em>different</em> small set of memory areas, and so on. The memory pages currently required by a process are called its <em>working set</em>, and the operating system ideally keeps all processes' working sets in physical memory to minimize slow disk activity.</p>
		<p>Virtual memory is a huge convenience for programmers, because it reduces the need to work around physical memory restrictions. The net effect of virtual memory for anti-virus software is that a memory scanner doesn't have everything immediately accessible that it needs to scan.</p></dd>
</dl>
<!--p.161-->
<p>An operating system can have memory protection without having virtual memory; virtual memory can be supported without having strong memory protection between processes, but this is not normally done. The remainder of this section only considers operating systems with both memory protection and virtual memory, because it is the hardest case to handle.</p>
<p>There are several different ways that memory scanning can be implemented in such operating systems:<sup><a href="#f8_103" name="b8_103">103</a></sup></p>
<ul>
	<li>As an ordinary user process, anti-virus software can scan memory by using operating system facilities intended for debugging. Debuggers need to examine (and modify) the memory of a process being debugged, and operating systems have an API to support debuggers.<sup><a href="#f8_104" name="b8_104">104</a></sup> Anti-virus software can use this API, pretending to be a debugger, to examine and scan processes' memory. This avoids memory protection issues.
		<p>Care must be taken when scanning the memory of a process. Attempting to scan all the virtual memory that a process uses will force the operating system to bring in memory pages from disk, an incredibly slow operation in comparison to accessing physical memory. The victim process being scanned would have its working set of memory pages decimated until the operating system slowly recovers them. If possible, querying the operating system to determine what memory pages are already present in memory, and only scanning those pages, reduces unpleasantness with a process' virtual memory. The alternative is grim: one memory scanner increased the resident memory usage of a poor process being scanned by over 2000%.</p>
		<p>Memory scanning can further be limited, beyond restricting it to in-memory pages. Ideally, assuming that the anti-virus software already examined a process' executable code in the filesystem before it ran, the only thing that requires rescanning is memory that has been changed. Extracting this information from the operating system is not always possible, however.</p>
		<p>Not all processes can be debugged by a user process, for security reasons. For example, processes belonging to another user or privileged system processes will not permitjust any user process to attach a debugger to them. The anti-virus software must run with escalated privileges to allow it to "debug" all other processes.</p></li>
	<li>Some of the problems with the memory scanning implementation above can be avoided if the anti-virus software runs as part of the operating system kernel. Kernel-based anti-virus software will have permission to access all processes' memory, avoiding access problems.
		<p>A memory scanner can be integrated more deeply into the kernel for even better effect. Tying a memory scanner into the operating system's virtual
<!--p.162-->
		memory manager would still avoid permission problems, plus give the memory scanner access to information about modified and in-memory pages.</p></li>
</ul>
<p>Once a worm or other malware is discovered in memory, memory disinfection can be done by terminating the offending process completely. Riskier options are to terminate suspect threads within the infected process, or to patch the code in the infected process as it runs. Operating systems share memory pages between processes when possible, as for shared library code or read-only program code segments, and consequently many processes may be affected by an infection - the best memory disinfection may be a reboot. Disinfection may be an ultimately futile operation anyway, because if the infection vector was a technical weakness, then a worm can re-exploit the machine right away.</p>
<p>Any of the above implementations of memory scanning leave another window of opportunity for worms, because the scanning is not done continuously. Rescanning memory continuously, for each memory write, would involve a prohibitive amount of overhead except perhaps for interpreted languages that already proudly sport prohibitive overhead.</p>
<p>Philosophically, it is not clear that memory scanning by anti-virus software is a good idea. Memory scanning necessarily weakens the memory protection between unrelated code, in this case the anti-virus software and the process' code being examined. Strong memory protection was implemented in operating systems for good reason, and circumventing it may only introduce new attack vectors. Anti-virus software that doesn't scan memory, in combination with other defenses, may be a wiser choice.</p>
<h4><a name="c813"></a>8.1.3 Perimeter</h4>
<p>The first line of defense for a network is at its perimeter. The computer at the perimeter forming the defense may be a general-purpose computer, or a special-purpose computer like a router. In either case, there are several functions the perimeter computer may serve to block incoming worms. Two functions, firewalls and intrusion detection systems, are presented in their "pure" form below; in practice, the perimeter computer may perform both of these functions and more.</p>
<h5><a name="c8131"></a>8.1.3.1 Firewalls</h5>
<p>A <em>firewall</em> is software which filters incoming network traffic to a network; if the software runs on a computer dedicated to this task, then that computer is also referred to as a firewall.<sup><a href="#f8_3" name="b8_3">3</a></sup> Firewall software can be run on a perimeter computer, a host computer, or both.</p>
<p>Each network packet in the traffic has two basic parts, header and data. This is analogous to sending a letter: the envelope has the addresses of the letter's sender and receiver, and the letter's contents are sealed inside the envelope. A
<!--p.163-->
packet's header has the sender and receiver information, and its data contains the packet contents that are meant for the recipient. Basic firewalls filter network packets based on header information:<sup><a href="#f8_4" name="b8_4">4</a></sup></p>
<ul>
	<li>The source IP address, or the computer that the packet purportedly comes from.</li>
	<li>The source port. Each computer has a fixed number of virtual "ports" for communication; the combination of an IP address and port identifies a communications endpoint.</li>
	<li>The IP address of the computer where the packet is destined.</li>
	<li>The destination port. The network servers providing services usually reside at well-known port numbers, so that a computer trying to contact a service knows where to direct its request.</li>
	<li>The protocol type. Filtering on the protocol type results in a very coarse-grained discrimination between different traffic types: connection-based (TCP), connectionless (UDP), and low-level control messages (ICMP).</li>
</ul>
<p>A firewall will have a set of rules which describe the policy it should implement - in other words, which packets it should pass through, and which packets it should drop. A firewall could look at a packet's data too, called <em>deep packet inspection</em>, but the format and semantics of the data depend on exactly where the packet is going. Making sense of the packet data would require the firewall to understand the language of every network service, and doing so would both slow and complicate a firewall, just like opening and reading a letter is slower and more complicated than glancing at its envelope.</p>
<p>As a worm defense, afirewallprovides a similar function to limiting available network services. A firewall prevents a worm from communicating with, and possibly exploiting, vulnerable network servers. It only defends against <em>outside</em> attacks, so any worm that makes it past the firewall (e.g., an email worm that a user runs on their computer behind the firewall) can operate with impunity.</p>
<h5><a name="c8132"></a>8.1.3.2 Intrusion Detection Systems</h5>
<p>An <em>intrusion detection system</em> analyzes incoming network traffic, performing deep packet inspection to watch for packets or sequences of packets that signal attacks.<sup><a href="#f8_105" name="b8_105">105</a></sup> Like firewalls, intrusion detection systems can run on the perimeter computer or a host computer. Like criminals, intrusion detection systems go by a wide variety of names:</p>
<ul>
	<li>Intrusion detection systems (IDS).</li>
	<li>Host intrusion detection systems (HIDS), for host-based IDS.
<!--p.164-->
<div align="center">
	<img src="img/mja01/fig83.gif" alt="Figure 8.3. Signatures in network traffic" />
	<p><strong>Figure 8.3. Signatures in network traffic</strong></p>
</div></li>
	<li>Network intrusion detection systems (NIDS), for network-based IDS. These need not only be at the network perimeter. IDS (and firewalls) may also be deployed internally to a network, to add extra worm protection for critical internal subnets.<sup><a href="#f8_106" name="b8_106">106</a></sup></li>
	<li>Intrusion prevention systems (IPS).<sup><a href="#f8_5" name="b8_5">5</a></sup> "Prevention" implies that an attack is thwarted rather than just noticed. Although there are no commonly agreed upon definitions, an IPS would imply a system that filters traffic like a firewall, but that is able to do deep packet inspection like an IDS.<sup><a href="#f8_107" name="b8_107">107</a></sup> In contrast, an IDS doesn't filter traffic, only watches it and raises an alarm if suspicious activity is found.</li>
</ul>
<p>For worms, an intrusion detection system can either look for the signatures of known worms, or for generic signatures of technical weakness exploits like a NOP sled.<sup><a href="#f8_6" name="b8_6">6</a></sup> Exactly the same algorithms can be used for IDS as for signature matching in anti-virus software, along with which come the same signature updating issues.<sup><a href="#f8_7" name="b8_7">7</a></sup></p>
<p>IDS signature matching is not completely straightforward, because of the properties of network traffic. A signature may not be obvious in the stream of input packets (Figure 8.3):</p>
<ul>
	<li>Packets containing a signature may arrive out of order.</li>
	<li>A packet may be <em>fragmented</em>, broken into smaller pieces which may be sent out of sequence.</li>
</ul>
<p>Network traffic can be deliberately crafted to present an IDS with these non-obvious signatures. The host machine receiving the packets will reassemble
<!--p.165-->
the fragments and reorder the packets. The IDS <em>should</em> reconstruct the correct packet stream also, but in practice may not do so correctly or may reconstruct it differently than the receiving host. In either case, the result is exploitable. One solution is <em>traffic normalization</em>, which ensures that network traffic is in a canonical, unambiguous form for the IDS and other hosts by reordering and reassembling packets if necessary.<sup><a href="#f8_108" name="b8_108">108</a></sup> Even so, a worm may defy easy signature-based detection by being encrypted, polymorphic, or metamorphic.</p>
<div align="center">
	<img src="img/mja01/fig84.gif" alt="Figure 8.4. Traffic accepted by an IDS and a host" />
	<p><strong>Figure 8.4. Traffic accepted by an IDS and a host</strong></p>
</div>
<p>Other avenues of attack are possible against an IDS. With the exception of a host-based IDS, an IDS runs on a separate machine which may have different behavior than the hosts it protects. Figure 8.4 shows the results: an IDS may accept traffic that a host will reject; a host may accept traffic that an IDS will reject. An IDS may also see traffic that never reaches a host. For example, each IP packet has a "time-to-live" counter (TTL) that is decremented once for every time the packet is sent from machine to machine during its delivery; when the counter reaches zero, the packet is discarded. Figure 8.5 shows an attack exploiting the TTL counter. The traffic has been constructed so that the IDS receives extra packets that prevent it from seeing the attack signature, yet the extra packets expire due to a low TTL value before reaching the targeted host.<sup><a href="#f8_109" name="b8_109">109</a></sup></p>
<p>The fact that an IDS can detect but not block attacks is exploitable too. In the simplest case, a fast-spreading worm attacks and executes its payload before an IDS alarm is responded to. But an IDS is a so-called <em>fail-open</em> system, meaning that it leaves the network accessible in the event that the IDS fails. A more advanced attack would first overload the IDS with a denial of service, then perform the worm infection while the IDS is struggling and unable to raise an alarm.</p>
<p>Finally, an IDS is a real-time system.<sup><a href="#f8_8" name="b8_8">8</a></sup> It must be able to keep up with the maximum rate of network traffic. Powerful, accurate, but high-overhead
<!--p.166-->
detection techniques are not suitable for use in an IDS. Taken together, all these drawbacks make an IDS yet another partial worm defense.</p>
<div align="center">
	<img src="img/mja01/fig85.gif" alt="Figure 8.5. TTL attack on an IDS" />
	<p><strong>Figure 8.5. TTL attack on an IDS</strong></p>
</div>
<h3><a name="c82"></a>8.2 Capture and Containment</h3>
<p>If defense is about keeping a worm <em>out</em>, then capture and containment is about keeping a worm in. This may seem like a counterintuitive thing to do, but if a worm has breached the primary defenses, then limiting the worm's spread may be the best remaining option. It has even been suggested that it is naive to assume that all machines can remain clean during a worm attack, and that some machines may have to be sacrificed to ensure the survival of the majority.<sup><a href="#f8_110" name="b8_110">110</a></sup></p>
<p>Worm containment can limit internal spread within a network. This reduces the amount of worm infections to clean up, and also has wider repercussions. Containing a worm and preventing it from spreading to other people's networks is arguably part of being a good Internet neighbor, but more practically, reasonable containment measures may limit legal liability. Two containment measures are presented in this section, reverse firewalls and throttling.</p>
<p>Worm capture can be done for a variety of reasons. Capturing a worm can provide an early warning of worm activity. It can also slow and limit a worm's spread, depending on the type of worm and worm capture. Honeypots are one method of worm capture.</p>
<!--p.167-->
<h4><a name="c821"></a>8.2.1 Honeypots</h4>
<p><em>Honeypots</em> are computers that are meant to be compromised, computers which may be either real or emulated. Early documented examples were intended to bait and study human attackers,<sup><a href="#f8_9" name="b8_9">9</a></sup> but honeypots can be used equally well to capture worms.<sup><a href="#f8_10" name="b8_10">10</a></sup></p>
<p>There are three questions related to honeypots:</p>
<ol>
	<li>How is a honeypot built? A honeypot should be constructed so that a worm is presented with an environment complete enough to infect. In addition, a honeypot should ideally be impossible for a worm to break out of, and a honeypot should be easy to restore to a pristine state. Emulators are often considered for honeypot systems, because they meet all these criteria.
		<p>The major problem with using emulators for honeypots is also a problem when using emulators for anti-virus software: it may be possible for a worm to detect that it is in an emulator.<sup><a href="#f8_111" name="b8_111">111</a></sup> For example, a worm can look for device names provided by common emulators.</p></li>
	<li>How is a worm drawn to a honeypot? A honeypot should be located in an otherwise-unused place in the network, and not be used for any other purpose except as a honeypot. The reasoning is that a honeypot should have no reason to receive legitimate traffic - <em>all</em> traffic to a honeypot is suspicious.<sup><a href="#f8_112" name="b8_112">112</a></sup> A honeypot doesn't generate network traffic by itself, the downside being that a passive scanning worm will be able to avoid the honeypot.
		<p>One honeypot with one IP address in an large network stands little chance of being targeted by a worm scanning randomly or quasi-randomly. A large range of consecutive addresses can be routed to a single honeypot to supply a larger worm target.<sup><a href="#f8_113" name="b8_113">113</a></sup></p>
		<p>Other mechanisms can be used to lure the discriminating worm. A honeypot can provide a fake shared network directory containing goat files, for worms that spread using such shared directories - the goat files and shared directory can be periodically checked for changes that may signify worm activity. Email worms can be directed to a honeypot by salting mailing lists with fake email addresses residing on the honeypot.</p></li>
	<li>What can a honeypot do with a worm? It can capture samples of worms, and be used to gauge the overall amount and type of worm activity. A honeypot is one way to get an early warning of worms; more ways will be seen in Section 8.3.
		<p>Honeypots can deliberately respond slowly, to try and slow down a worm's spread. This type of honeypot system is called a <em>tarpit</em>.<sup><a href="#f8_114" name="b8_114">114</a></sup> A worm that
<!--p.168-->
		scans for infectible machines in parallel will not be susceptible to a tarpit, however.<sup><a href="#f8_115" name="b8_115">115</a></sup></p>
		<p>Certain types of worms can be severely impacted by honeypot capture. A hit-list worm passes half its targets along to each newly-infected machine, so hitting a honeypot cuts the worm's spread from that point by half.<sup><a href="#f8_116" name="b8_116">116</a></sup></p></li>
</ol>
<p>It is questionable whether or not honeypots are as useful against worms as other means of defense. Early warning of a spreading worm is useful, but there are other ways to receive a warning, and worm capture is not generally useful to anyone except specialists.</p>
<h4><a name="c822"></a>8.2.2 Reverse Firewalls</h4>
<p>A <em>reverse firewall</em> filters outgoing traffic from a network, unlike a normal firewall which filters incoming traffic. In practice, filtering in both directions would probably be handled by the same software or device.</p>
<p>As with firewalls, the key to an effective reverse firewall is its policy: what outbound connections should be permitted? The principle is that a worm's connections to infect other machines will not conform to the reverse firewall policy, and the worm's spread is thus blocked. The decision is based on the same packet header information as was used for a firewall, including source and destination IP addresses and ports. For example, policy may dictate that no machine in the critical network of Figure 8.1 may have an outgoing Internet connection, or that a user's computer may only connect to outside machines on port 80, the usual port for a web server.</p>
<p>A host-based reverse firewall can implement a finer-grained policy by restricting Internet access on a per-application basis. Only certain specified applications are allowed to open network connections, and then only connections in keeping with the reverse firewall's outbound traffic policy. A worm, as a newly-installed executable unknown to the reverse firewall, could not open network connections to spread. In theory. Still, worm activity is possible in the presence of a host-based reverse firewall:</p>
<ul>
	<li>A worm can use alternative methods to spread when faced with a reverse firewall, such as placing itself in shared network directories. As a result, no worm code is run on the host being monitored by the reverse firewall.</li>
	<li>Legitimate code that is already approved to access the Internet can be subverted by a worm. A worm can simply fake user input to an existing mail program to spread via email, for instance. A worm could exhibit viral behavior, too, infecting an existing "approved" executable by indirect means, like a web browser plug-in, or more direct means that a virus would normally use. To guard against the latter case, a host-based reverse firewall can use integrity checking to watch for changes to approved executables.</li>
<!--p.169-->
	<li>Social engineering may be employed by a worm. A host-based reverse firewall may prompt the user with the name of the program attempting to open a network connection, for the user to permit or deny the operation. This would typically happen under two circumstances:
	<ol>
		<li>The program has never opened a network connection before. This would be the case for a worm, newly-installed software, or old, installed software that has never been used.</li>
		<li>The program was approved to use the network before, but has changed; a software upgrade may have occurred, or the program's code may have been infected.</li>
	</ol>
	<p>A surreptitious worm could patiently wait until a user installs or upgrades software, then open a network connection. The user is likely to assume the reverse firewall's prompt is related to the legitimate software modification and permit the worm's connection. The worm may also give its executable an important-sounding name, which the reversefirewallwill dutifully report in the user prompt, intimidating the user into allowing the worm's operation for fear that their computer won't work properly.</p>
	<p>Legitimate applications may farm out Internet operations to a separate program. Legitimate user prompts from a reverse firewall can request network access approval for software with radically-different names than the application that the user ran. Users will likely approve any user prompts made shortly after they initiate an action in an application, and a worm can exploit this to sneakily receive a user's approval for its network operations.</p></li>
</ul>
<p>The underlying problem with a reverse firewall is that it tries to block unauthorized activity by watching network connection activity, an action performed by worms <em>and</em> legitimate software. False positives are guaranteed, which open the possibility of circumventing the reverse firewall.</p>
<h4><a name="c823"></a>8.2.3 Throttling</h4>
<p>A reverse firewall can be improved upon by taking context into account. Instead of watching for single connections being opened, the <em>overall</em> rate of new connections can be monitored. A system that limits the rate of outgoing connections that a given machine is allowed to make is called a <em>throttle</em>.<sup><a href="#f8_117" name="b8_117">117</a></sup></p>
<p>A throttle doesn't attempt to distinguish between worms and legitimate software, nor does it try to prevent worms from entering. It only considers outbound connections, and throttles the rate at which <em>all</em> programs make them. As a throttle only slows down the connection rate, as opposed to dropping connections, no harm is done even if there are false positives - everything still works, just more slowly.</p>
<!--p.170-->
<div align="center">
	<img src="img/mja01/fig86.gif" alt="Figure 8.6. Network traffic throttling" />
	<p><strong>Figure 8.6. Network traffic throttling</strong></p>
</div>
<p>The throttling process can be refined with more context. Most connections are established to machines that were recently connected to; this is similar to the locality of reference exploited by virtual memory. For example, a web browser may initially connect to a web site to download a web page, followed by subsequent connections to retrieve images for the page, followed by requests for linked web pages at the site. A working set of recently-connected machines can be kept for a throttled host. Connections to machines in the working set proceed without delay, as do new connections which fit into the fixed-length working set. Other connections are delayed by a second, not long enough to cause grief, but effective for slowing down fast-moving worms. Extreme worm-like behavior can be caught with the context provided by the throttling delay. Too many outstanding new connections can cause a machine to be locked out from the network.<sup><a href="#f8_11" name="b8_11">11</a></sup></p>
<p>TCP connections are started by the connecting machine sending a SYN packet, and a throttle can use these SYN packets as an indicator of new connections. In Figure 8.6, a pair of machines are throttled with a working set of size two. The uninfected machine's new connection to machine <em>B</em> passes through immediately, because <em>B</em> was connected to recently, and is therefore present in the working set. The infected machine has its connection to machine <em>A</em> go through, because there is one free spot in the working set; machine <em>D</em> is in the working set, and that connection goes through as well. The other two connections the infected machine makes are delayed. With adaptations, the throttle concept can be extended beyond TCP to UDP, as well as higher-level applications like email and instant messaging.<sup><a href="#f8_118" name="b8_118">118</a></sup></p>
<!--p.171-->
<p>Throttles are designed around heuristics characterizing "normal" network usage. Like other heuristic systems, throttles can be evaded by avoiding behavior that the heuristics look for.<sup><a href="#f8_119" name="b8_119">119</a></sup> For example, a worm can slow its rate of spreading down, avoiding the lockout threshold; the number of infection attempts each worm instance makes can be constrained to the throttle's working set size to avoid delays. Because throttles are not widely used at present, a worm's best strategy may be to ignore the possibility of throttles altogether, as they will not significantly impact the overall worm spread.</p>
<p>One criticism leveled at throttles is that they may slow down some programs, like mail transport agents, that can legitimately have a high connection rate.<sup><a href="#f8_120" name="b8_120">120</a></sup> Different throttling mechanisms which address this criticism can be devised by using additional context information. Worms poking randomly for victims on the network will have a higher probability of failure than legitimate programs - either there is no machine at the address the worm generates, or the machine there doesn't supply a suitably-exploitable service.<sup><a href="#f8_121" name="b8_121">121</a></sup> A throttle can take the success of connection attempts into account.</p>
<p>A <em>credit-based</em> throttle assigns a number of credits to each host it monitors, akin to a bank account. Only hosts with a positive account balance are allowed to make outbound connections; a zero balance will result in a host's outgoing connections being delayed or blocked completely. A host starts with a small initial balance, and its account is debited for each connection attempt the host makes, and credited for each of the connection attempts that succeed. Host account balances are reexamined every second for fairness: too-high balances are pared back, and hosts with persistent zero balances are credited.<sup><a href="#f8_12" name="b8_12">12</a></sup></p>
<p>Unfortunately, a credit-based throttle doesn't fare well against worms that violate its assumptions about worm scanning. A worm using hit-list, topological, or passive scanning would naturally tend to make successful connections, for instance. Special attacks can be crafted, too. A worm need only mix its random scans with connections to hosts (infected or otherwise) that are known to respond, to avoid being throttled.</p>
<p>In computer science, sometimes solving the general problem is easier than solving a more specific problem. Instead of trying to discern worm traffic from legitimate traffic, or watching individual hosts' new connections, a general problem can be considered: how can network load be fairly balanced? Allocating bandwidth such that high-bandwidth applications (fast-spreading worms, DDoS attacks, file transfers, streaming media) do not starve low-bandwidth applications (web browsing, terminal sessions) may effectively throttle the speed and network impact of worm spread.<sup><a href="#f8_122" name="b8_122">122</a></sup></p>
<h3><a name="c83"></a>8.3 Automatic Countermeasures</h3>
<p>The losses from worm attacks can be reduced in other ways besides slowing worm propagation. Especially for fast-spreading worms, automatic
<!--p.172-->
counter-measures are the only possible defense that can react quickly enough. There are two problems to solve:</p>
<ol>
	<li>How to detect worm activity. Activity detection serves as the trigger for automatic countermeasures.</li>
	<li>What countermeasures to take. The reaction must be appropriate to the threat, keeping in mind that worm detection may produce false positives. Severing the Internet connection every time someone in Marketing gets overzealous surfing the web will not be tolerated for long.</li>
</ol>
<p>Several methods to detect worm activity have been seen already. Worm capture using honeypots is one method; detecting a sudden spike in excessive throttling is another. Trying to capture various facets of worm behavior leads to other methods, for example:</p>
<ul>
	<li>A worm may exploit a vulnerability in one particular network server, located at a well-known port. A worm activity monitor can watch for a lot of incoming <em>and</em> outgoing traffic destined to one port. This can be qualified by the number of distinct IP address destinations, on the premise that legitimate traffic between two machines may involve heavy use of the same port, but worms will try to fan out to many different machines.<sup><a href="#f8_123" name="b8_123">123</a></sup></li>
	<li>Most network applications refer to other machines using human-readable domain names, which are mapped into IP addresses with queries to the <em>domain name system</em> (DNS). Worms, on the other hand, mostly scan using IP addresses directly. Worm activity may thus be characterized by correlating DNS queries with connection attempts - connections not preceded by DNS requests may signify worms.<sup><a href="#f8_124" name="b8_124">124</a></sup> Unfortunately, this registers false positives for some legitimate applications, so a Draconian reaction based on this classifier is not the best idea.</li>
</ul>
<p>What reaction should be taken to worm activity? Some examples of automatic countermeasures are below.<sup><a href="#f8_125" name="b8_125">125</a></sup></p>
<ul>
	<li>Affected machines can be cut off from the network to prevent further worm spread. A more aggressive approach may be taken to guard critical networks, which may be automatically isolated to try and prevent a worm from getting inside them.</li>
	<li>Targeted network servers can be automatically shut down.</li>
	<li>Filtering rules can be inserted automatically into firewalls to block the hosts from which worm activity is originating.<sup><a href="#f8_13" name="b8_13">13</a></sup> Or, a filter can drop packets addressed to the port of a targeted network server,<sup><a href="#f8_126" name="b8_126">126</a></sup> which is less resource-intensive as the number of worm-infected machines increases.</li>
</ul>
<!--p.173-->
<p>Automatic countermeasures must be deployed judiciously, because an attacker can also use them, deliberately triggering the countermeasures to perform a DoS attack.<sup><a href="#f8_127" name="b8_127">127</a></sup> This danger can be mitigated by providing automatic countermeasure systems with a <em>whitelist</em>, a list of systems which are not allowed to be blocked.<sup><a href="#f8_128" name="b8_128">128</a></sup></p>
<!--p.174-->
<p><strong>Notes for Chapter 8</strong></p>
<p><a name="f8_1" href="#b8_1">1</a> Strictly speaking, the worm release causes another, smaller, exponential patching decay curve [262].</p>
<p><a name="f8_2" href="#b8_2">2</a> This is especially true of code, somewhat less so for data, although some data structures and algorithms play more nicely with virtual memory than others. Stacks, for example, exhibit a high degree of locality when they are accessed with "push" and "pop" operations.</p>
<p><a name="f8_3" href="#b8_3">3</a> Thefirewallsdescribed here would be classed as "packet-filtering" firewalls. Cheswick and Bellovin, for example, distinguish between several different kinds of firewall [68].</p>
<p><a name="f8_4" href="#b8_4">4</a> The header information here is based on the information available for the widely-used IP protocol suite.</p>
<p><a name="f8_5" href="#b8_5">5</a> The acronyms NIPS and HIPS have tragically failed to materialize.</p>
<p><a name="f8_6" href="#b8_6">6</a> Although as with viruses, a worm may try to disguise this feature, possibly by using junk code instead of a NOP sled [253].</p>
<p><a name="f8_7" href="#b8_7">7</a> This describes only signature-based IDS. Another type, anomaly-based IDS, watches for traffic abnormalities rather than any specific feature [20]. Watching for abnormalities that may signify worm activity is examined in Section 8.3.</p>
<p><a name="f8_8" href="#b8_8">8</a> A soft real-time system, that is.</p>
<p><a name="f8_9" href="#b8_9">9</a> For example, Cheswick's famous observations of "Berferd" [67]. There do not seem to be any publicly-documented examples prior to 1990 [301]. It is interesting that a 1980 report specifically excluded a threat scenario which corresponds to a honeypot: 'Penetrator Not Authorized Use of Computer' but 'Penetrator Authorized to Use Data/Program Resource' [11, page 7].</p>
<p><a name="f8_10" href="#b8_10">10</a> Foulkes and Morris [115] and Overton [236]. A "virus trap" patent application in the mid-1990s arguably suggests this use of honeypots, but there the trap is used to execute programs before they are transferred to a protected machine [281].</p>
<p><a name="f8_11" href="#b8_11">11</a> The original work used a working set of length five, and a lockout threshold of 100 [325].</p>
<p><a name="f8_12" href="#b8_12">12</a> Suggested values are an initial balance of ten credits, a debit of one for initiated connections, and a credit of two for successful connections. Hosts over their initial balance have a third of their credits clawed back each second, and hosts are given one credit after having a zero balance for four seconds [276].</p>
<p><a name="f8_13" href="#b8_13">13</a> One vendor calls this <em>shunning</em> [73].</p>
<!--p.175-->
<p><a name="f8_100" href="#b8_100">100</a> This section is based on Rescorla [262] except where otherwise noted. For simplicity, applying workarounds and upgrading to new, fixed software versions are also considered "patching" here because they all have the same net effect: fixing the vulnerability.</p>
<p><a name="f8_101" href="#b8_101">101</a> Arbaugh et al. [19], Moore et al. [212], and Provos and Honeyman [255].</p>
<p><a name="f8_102" href="#b8_102">102</a> Arbaugh et al. [19] and Provos and Honeyman [255].</p>
<p><a name="f8_103" href="#b8_103">103</a> These, and the disinfection options, are based on Szor [310].</p>
<p><a name="f8_104" href="#b8_104">104</a> Rosenberg [268].</p>
<p><a name="f8_105" href="#b8_105">105</a> This section is based on Ptacek and Newsham [256] unless otherwise noted.</p>
<p><a name="f8_106" href="#b8_106">106</a> Foulkes and Morris [115].</p>
<p><a name="f8_107" href="#b8_107">107</a> Desai [88].</p>
<p><a name="f8_108" href="#b8_108">108</a> Handleyetal. [135].</p>
<p><a name="f8_109" href="#b8_109">109</a> Paxson [243].</p>
<p><a name="f8_110" href="#b8_110">110</a> Ford and Thompson [114].</p>
<p><a name="f8_111" href="#b8_111">111</a> Holz and Raynal [145] and Krawetz [173].</p>
<p><a name="f8_112" href="#b8_112">112</a> Oudot[234].</p>
<p><a name="f8_113" href="#b8_113">113</a> Foulkes and Morris [115] describe this, and the "other mechanisms" below. Overton [236] also talks about luring worms with fake shared network resources.</p>
<p><a name="f8_114" href="#b8_114">114</a> Oudot and Holz [235].</p>
<p><a name="f8_115" href="#b8_115">115</a> Oudot [234].</p>
<p><a name="f8_116" href="#b8_116">116</a> Nazario [229].</p>
<p><a name="f8_117" href="#b8_117">117</a> This section is based on Twycross and Williamson [325] except where otherwise noted.</p>
<p><a name="f8_118" href="#b8_118">118</a> See Twycross and Williamson [325] (UDP), Williamson [347] (email), and Williamson et al. [348] (instant messaging).</p>
<p><a name="f8_119" href="#b8_119">119</a> These suggestions are from Staniford et al. [303].</p>
<p><a name="f8_120" href="#b8_120">120</a> This, the credit-based throttle, and attacks on the credit-based throttle, are from Schechter et al. [276].</p>
<p><a name="f8_121" href="#b8_121">121</a> ChenandRanka[62].</p>
<p><a name="f8_122" href="#b8_122">122</a> Matrawyetal. [197].</p>
<p><a name="f8_123" href="#b8_123">123</a> Chen and Heidemann [63].</p>
<p><a name="f8_124" href="#b8_124">124</a> Whyteetal. [345].</p>
<p><a name="f8_125" href="#b8_125">125</a> Foulkes and Morris [115] and Oudot [234].</p>
<p><a name="f8_126" href="#b8_126">126</a> Chen and Heidemann [63].</p>
<p><a name="f8_127" href="#b8_127">127</a> Jung et al. [156] and Ptacek and Newsham [256].</p>
<p><a name="f8_128" href="#b8_128">128</a> Jung et al. [156] and Whyte et al. [345].</p>
<!--p.176-->
<h2><a name="c9"></a>Chapter 9 Applications</h2>
<p>Malware can arguably be used in many areas, for better or worse. This chapter briefly looks at a number of "applications" for malicious software, for want of a better word. The applications are roughly grouped in order of increasing gravity: good (benevolent malware), annoying (spam), illegal (access-for-sale worms and cryptovirology), and martial (information warfare and cyberterrorism).</p>
<h3><a name="c91"></a>9.1 Benevolent Malware</h3>
<p>"Benevolent malicious software" is obviously a contradiction in terms. Normally specific types of malware would be named - a benevolent virus, a benevolent worm. The generic term <em>benevolent malware</em> will be used to describe software which would otherwise be classified as malware, yet is intended to have a "good" effect.<sup><a href="#f9_100" name="b9_100">100</a></sup></p>
<p>Real attempts at benevolent malware have been made.<sup><a href="#f9_1" name="b9_1">1</a></sup> For example:</p>
<ul>
	<li>Den Zuk, a boot-sector infecting virus in 1988, did no damage itself but removed the Pakistani Brain and Ohio viruses from a system. Later variants had the nasty habit of reformatting disks.<sup><a href="#f9_101" name="b9_101">101</a></sup></li>
	<li>In 2001, the Cheese worm circulated, trying to clean up after the Lion (li0n) worm that had hit Linux systems. The problem was that Cheese's operation produced a lot of network traffic.<sup><a href="#f9_102" name="b9_102">102</a></sup></li>
	<li>The Welchia worm tried to clean up Blaster-infected machines in 2003, even going so far as to automatically apply an official Microsoft patch for the bug that Blaster exploited.<sup><a href="#f9_103" name="b9_103">103</a></sup> Again, Welchia produced so much network traffic that the cure was worse than the disease.</li>
</ul>
<p>These latter two can be thought of as "predator" worms. Such a predator worm could both destroy existing instances of its target worm, as well as immunize a
<!--p.177-->
machine against further incursions through a particular infection vector. Studies have been done simulating the effect that a well-constructed predator worm would have on its worm "prey." Simulations predict that, <em>if</em> a predator worm and immunization method are ready in advance, then a predator worm can significantly curtail the spread of a target worm.<sup><a href="#f9_104" name="b9_104">104</a></sup> However, a number of hurdles remain, legal, ethical, and technical.</p>
<p>Legally, a predator worm is violating the law by breaking into machines, despite its good intentions. It may be possible to unleash a predator worm in a private network, in which the predator worm's author has permission for their worm to operate, but there is a risk of the worm escaping from an open network.</p>
<p>Ethically, releasing a predator worm on the Internet at large affects machines whose owners haven't permitted such an activity, and past examples have inspired no confidence that a predator worm's impact would be beneficial in practice. Even if a predator worm's network activity were controlled, unexpected software interactions could be expected on machines that are infected. A worm's effect would have to be incredibly damaging to society, far more so than any seen to date, before a predator worm's actions could be seen to contribute to a universal good.</p>
<p>Technically, there are the issues of control, compatibility, and consumption of resources mentioned above. There is also the thorny issue of verification: what is a predator worm doing, and how can its behavior be verified? Has a predator worm been subverted by another malware writer, and how can anti-virus software distinguish good worms from bad?<sup><a href="#f9_105" name="b9_105">105</a></sup></p>
<p>Of all the possible applications for benevolent malware, including predator worms, there has been no "killer application," a problem for which benevolent malware is clearly the best solution. Everything doable by benevolent malware can also be accomplished by other, more controlled means.</p>
<p>One possible niche for benevolent malware is the area of mobile agents. A <em>mobile agent</em> is a program that transfers itself from one computer to another as it performs one or more tasks on behalf of a user.<sup><a href="#f9_106" name="b9_106">106</a></sup> For example, a user's mobile agent may propagate itself from one airline site to another, in search of cheap airfares. From the point of view of malware, mobile agents bear more than a passing resemblance to rabbits, and serious questions have been raised about mobile agent security.<sup><a href="#f9_107" name="b9_107">107</a></sup> As was the case for benevolent malware, mobile agents may be a solution in search of a problem: one analysis concluded that mobile agents had overall advantages, but 'With one rather narrow exception, there is nothing that can be done with mobile agents that cannot also be done with other means.'<sup><a href="#f9_108" name="b9_108">108</a></sup></p>
<h3><a name="c92"></a>9.2 Spam</h3>
<p>An infected computer may just be a means to an end. Malware can install open proxy servers, which can be used to relay spam.<sup><a href="#f9_2" name="b9_2">2</a></sup> It can also turn infected
<!--p.178-->
machines into zombies that can be used for a variety of purposes, like conducting DDoS attacks. In either case, the malware writer would use the infected computer later, with almost no chance of being caught.</p>
<p>A zombie network can be leveraged to send more effective spam: infected computers can be viewed as repositories of legitimate email corpora. Malware can mine information about a user's email-writing style and social network, then use that analysis to customize new spam messages being sent out, so that they appear to originate from the user.<sup><a href="#f9_109" name="b9_109">109</a></sup> For example, malware can use saved email to learn a user's typical habits for email capitalization, misspellings, and signatures. The malware can then automatically mimic those habits in spam sent to people the user normally emails; these people are also discovered through malware mining saved email.</p>
<h3><a name="c93"></a>9.3 Access-for-Sale Worms</h3>
<p><em>Access-for-sale</em> worms are the promise of scalable, targeted intrusion. A worm author creates a worm which compromises machines and installs a back door on them. Access to the back door is transferred by the worm author to a "cyberthief," who then uses the back door to break into the machine.<sup><a href="#f9_3" name="b9_3">3</a></sup></p>
<p>Access to a machine's back door would be unique to a machine, and guarded by a cryptographic key. By transferring the key, a worm author grants back door access to that one machine. There is a fine granularity of control, because access is granted on a machine-by-machine basis.</p>
<p>Why would access to a single machine be of interest, when entire botnets can be had? Crime, particularly stealing information which may later be used for blackmail or identity theft. The value of such access increases in proportion to its exclusivity - in other words, a competitor must not be allowed to obtain and sell access too. Ironically, this means that a good access-for-sale worm must patch the vulnerabilities in a machine it compromises, to prevent a competing access-for-sale worm from doing the same thing.</p>
<p>There are two "business models" for access-for-sale worms:</p>
<ol>
	<li>Organized crime. A crime organization retains the services of a worm author and a group of cyberthieves, shown in Figure 9.1. The worm author creates and deploys an access-for-sale worm, and the back door keys are distributed to the cyberthieves. This keeps the "turf" divided amongst the cyberthieves, who then mine the compromised machines for information.<sup><a href="#f9_4" name="b9_4">4</a></sup> Due to the insular nature of organized crime, countermeasures that come between the worm author and cyberthieves are unlikely to work. Standard worm countermeasures are the only reliable defenses.</li>
	<li>Disorganized crime. Here, the worm author sells a back door key to a cyberthief. Compromised machines must first be advertised to potential customers by the worm author: this may be as crude as posting a list on some
<!--p.179-->
	<div align="center">
		<img src="img/mja01/fig91.gif" alt="Figure 9.1. Organized crime and access-for-sale worms" />
		<p><strong>Figure 9.1. Organized crime and access-for-sale worms</strong></p>
	</div>
	<div align="center">
		<img src="img/mja01/fig92.gif" alt="Figure 9.2. Disorganized crime and access-for-sale worms" />
		<p><strong>Figure 9.2. Disorganized crime and access-for-sale worms</strong></p>
	</div>
<!--p.180-->	
	underground website, or an infected machine may leak a unique identifier on some covert channel that a customer can detect.<sup><a href="#f9_5" name="b9_5">5</a></sup> The customer-cyberthief buys the back door access key for their chosen target machine from the worm author, which is used by the cyberthief to break in. The whole process is shown in Figure 9.2.
	<p>This model admits two additional defenses. First, the worm author's reputation can be attacked. The worm author and cyberthief probably don't know one another, so an access key sale is based on the seller's reputation and a certain amount of trust. One defense would make an infected machine continue to look infected, even after the machine has been cleaned, in the hopes of damaging the seller's reputation. Second, law enforcement authorities could set up honeypots and sell access as if the honeypots were access-for-sale machines. This would keep the doughnut budget in good stead, and might lead to the capture of some cyberthieves, or at least increase the cyberthieves' risk substantially.</p></li>
</ol>
<p>The access-for-sale worm would require some verification mechanism to ensure that an access key did in fact come from the worm author. This mechanism can be constructed using <em>public-key cryptography</em>, where a message is strongly encrypted and decrypted using different keys: a <em>private key</em> known only to the message sender, and a <em>public key</em> known to everyone.<sup><a href="#f9_110" name="b9_110">110</a></sup></p>
<p>The access-for-sale worm can carry the worm author's public key with it, and each compromised machine can be assigned a unique identifier (based on its network address, for example). When the worm author transfers an access key, they encrypt the machine's unique identifier with their private key; the worm can decrypt and verify the identifier using the public key. If a <em>symmetric</em> cryptographic scheme were used, where the same key is used for encryption <em>and</em> decryption, then capturing a worm sample would reveal the secret key, permitting access to all of the worm's back doors.</p>
<h3><a name="c94"></a>9.4 Cryptovirology</h3>
<p>Using viruses and other malware for extortion is called <em>cryptovirology</em>.<sup><a href="#f9_111" name="b9_111">111</a></sup> After a virus has deployed its payload and been discovered, the effects of its payload should be devastating and irreversible for the victim, but reversible for the virus writer. The virus writer can then demand money to undo the damage.</p>
<p>For example, such a virus - a <em>cryptovirus</em> - could strongly encrypt the victim's data such that only the virus author can decrypt it.<sup><a href="#f9_6" name="b9_6">6</a></sup> The cryptovirus can employ public-key cryptography to avoid having to carry a capturable, secret decryption key with it to each new infection. The victim's data is encrypted using the virus writer's public key, and the virus writer can supply their private key to decrypt the data once a ransom is paid.</p>
<!--p.181-->
<p>Even on fast computers, public-key encryption would be slow to encrypt large directories andfilesystems. There are faster options for a cryptovirus:</p>
<ul>
	<li>The cryptovirus can randomly generate a unique secret key for each infection. This secret key is used to strongly encrypt the victim's data using a faster, symmetric strong encryption algorithm. The cryptovirus then strongly encrypts the random secret key with the virus writer's public key and stores the result in a file. The victim transmits the file along with the ransom money; the virus writer is then able to recover the unique secret key without revealing their private key.</li>
	<li>Hardware mechanisms can be used. Some ATA hard drives have a rarely-used feature which allows their contents to be password-protected, rendering the contents unusable even if the computer is booted from different media. A cryptovirus can set this hard drive password if the feature is available.<sup><a href="#f9_112" name="b9_112">112</a></sup>
		<p>This can be used in conjunction with the randomly-generated unique key scheme above, but the cryptovirus couldn't store the encrypted secret key file on the encrypted hard drive. If no other writable media is available, the cryptovirus could simply display the encrypted secret key on the screen for the victim to write down.</p></li>
</ul>
<p>Both options avoid the virus writer needing a different public/private key pair for each new infection, lest a victim pay the ransom and publish the private decryption key for other victims as a public service.</p>
<p>There are only a few known instances of malware using encryption for extortion. The AIDS Trojan horse of 1989 was sent on floppy disks, mass-mailed worldwide via regular postal mail. It was an informational program relating to the (human) AIDS virus, released under a curious software license. The license gave it leave to render a computer inoperable unless the user paid for the software ($189 or $378, depending on the leasing option). It was true to its word: after approximately 90 reboots, the Trojan encrypted filenames using a simple substitution cipher.<sup><a href="#f9_113" name="b9_113">113</a></sup></p>
<p>More recently, the PGPCoder Trojan encrypted files with specific filename extensions, roughly corresponding to likely user document types. A text file was left behind in each directory where files were encrypted, with instructions on how to buy the decryptor: a bargain at $200.<sup><a href="#f9_114" name="b9_114">114</a></sup></p>
<h3><a name="c95"></a>9.5 Information Warfare</h3>
<p><em>Information warfare</em> is the use of computers to supplement or supplant conventional warfare. Computers can play a variety of roles in this regard, including acquiring information from an adversary's computers, planting information in their computers, and corrupting an adversary's data. Information warfare can also be applied in an isolating capacity, in an 'information embargo' that
<!--p.182-->
prevents an adversary from getting information in or out.<sup><a href="#f9_115" name="b9_115">115</a></sup> This section concentrates on malware-related information warfare only.</p>
<p>Computers are a great equalizer, and information warfare is a key weapon in <em>asymmetric</em> warfare, a form of warfare where an enemy possesses a decided advantage in one or more areas.<sup><a href="#f9_116" name="b9_116">116</a></sup> For example, the United States currently enjoys an advantage over many countries in terms of weaponry, and countries that cannot respond in kind have been proactively developing computer attack capabilities to counter this perceived threat.<sup><a href="#f9_7" name="b9_7">7</a></sup></p>
<p>Laws, rules of engagement, and the level of conflict may constrain information operations. Legally, it is unclear whether information warfare constitutes warfare; this is an important point, as it governs what international law applies to information warfare. For example, civilian targets are usually off limits in conventional warfare, but information warfare may not be able to avoid substantial collateral damage to civilian computers and network infrastructure.<sup><a href="#f9_117" name="b9_117">117</a></sup> A conservative approach is that malware may never be used in peacetime, but may be deployed by intelligence agencies as the conflict level rises. In all-out war, both intelligence agencies and the military may use malware. Ultimately, information warfare of any kind may be limited if an adversary's communications infrastructure has been destroyed or otherwise disabled.<sup><a href="#f9_118" name="b9_118">118</a></sup></p>
<p>It is interesting to think of malware-based information warfare as an electronic countermeasure.<sup><a href="#f9_119" name="b9_119">119</a></sup> An <em>electronic countermeasure</em>, or ECM, is any electronic means used to deny an enemy use of electronic technology, like radar jamming. Early jamming ECM was roughly analogous to a DoS attack, but current ECM systems heavily employ deception, making an enemy see false information.<sup><a href="#f9_8" name="b9_8">8</a></sup> A comparison of traditional ECM and malware is below.</p>
<dl>
	<dt><strong>Persistence</strong></dt><dd>
	<ul>
		<li>Traditional ECM: The effect of the ECM only lasts as long as the transmission of the jamming signal or false information.</li>
		<li>Malware: The effect of malware lingers until the malware is stopped by the adversary. This longer persistence allows targets to be attacked in advance, with the malware lying dormant until needed.</li>
	</ul></dd>
	<dt><strong>Targeting</strong></dt><dd>
	<ul>
		<li>Traditional ECM: Only direct targeting of an adversary's systems is possible.</li>
		<li>Malware: Both direct and indirect targeting is possible through connected, but weaker, points in an adversary's defenses.
			<p>Malware can be a double-edged sword. Careful thought must be given to the design of malware for information warfare, so that it doesn't start targeting the computers of the <em>original</em> attacker and their allies.<sup><a href="#f9_120" name="b9_120">120</a></sup></p></li>
	</ul></dd>
<!--p.183-->
	<dt><strong>Deception</strong></dt><dd>
	<ul>
		<li>Traditional ECM: Possible.</li>
		<li>Malware: Also possible. There are many possibilities for presenting false information to an adversary without them being aware of it.</li>
	</ul></dd>
	<dt><strong>Range of effects</strong></dt><dd>
	<ul>
		<li>Traditional ECM: Because the targets are special-purpose devices with limited functionality, the range of effects that ECM can elicit from their targets is similarly limited.</li>
		<li>Malware: The targets are more general-purpose computers, and the malware's effects can be designed to fit the situation. For example:<sup><a href="#f9_121" name="b9_121">121</a></sup>
		<ul>
			<li>Logic bombs.</li>
			<li>Denials of service at critical times.</li>
			<li>Precision-guided attacks, to destroy a single machine or file.</li>
			<li>Intelligence gathering, looking for specific, vital information. After the information is found, there is also the problem of smuggling it out. One possibility for worm-based intelligence gathering is to allow the information to propagate <em>with</em> the worm, in strongly-encrypted form, and intercept a copy of the worm later.<sup><a href="#f9_122" name="b9_122">122</a></sup></li>
			<li>A <em>forced quarantine</em> virus, which deliberately makes its presence known to an adversary. The adversary must isolate the affected machines, thus fragmenting and reducing the effectiveness of the adversary's computing infrastructure.</li>
		</ul></li>
	</ul></dd>
	<dt><strong>Reliability</strong></dt><dd>
	<ul>
		<li>Traditional ECM: It is unknown until ECM is used whether or not it will work, a detriment to the planning of military operations.</li>
		<li>Malware: Depending on the setting, malware may be able to signal indicating that it is in place and ready for use. Whether or not it will actually work is still unknown, as with traditional ECM.</li>
	</ul></dd>
	<dt><strong>Continuity</strong></dt><dd>
	<ul>
		<li>Traditional ECM: Must continually overcome the target, even if the target adapts to the attack using electronic counter-counter measures (ECCM).</li>
		<li>Malware: An adversary's defenses must only be overcome once, at their weakest point, unlike traditional ECM which attacks the strongest point.</li>
	</ul></dd>
</dl>
<!--p.184-->
<p>The way that malware is inserted into an adversary's system may be more exotic in information warfare. Direct transmission is still an option, either by self-replication or by espionage. Indirect transmission is possible, too, such as passing malware through third parties like military contractors or other software vendors, who may be oblivious to the malware transmission. Malware may be present, but dormant, in systems sold by a country to its potential future enemies. Another indirect means of transmission is to deliberately leak details of a malware-infected system, and wait for an enemy to copy it.<sup><a href="#f9_123" name="b9_123">123</a></sup></p>
<h3><a name="c96"></a>9.6 Cyberterrorism</h3>
<div class="epigraph">
	'We do not use the term 'ice pick terrorism' to define bombings of ice-pick factories, nor would we use it to define terrorism carried out with ice picks. Thus we question the use of the term cyberterrorism to describe just any sort of threat or crime carried out with or against computers in general.'
	<p>- Sarah Gordon and Richard Ford<sup><a href="#f9_124" name="b9_124">124</a></sup></p>
</div>
<br clear="all" />
<p>The United Nations has been unable to agree on a definition of terrorism.<sup><a href="#f9_125" name="b9_125">125</a></sup> A definition of <em>cyber</em>terrorism that is universally agreed-upon is equally elusive. This lack of a standard cyberterrorism definition makes the classification of individual acts hard to pin down. Is malware that launches a DDoS attack against a government web site cyberterrorism? What about malware that simply carries a string with an anti-government slogan?</p>
<p>Terrorism has been compared to theater, in that terrorists want to maximize the emotional impact of their attacks. From the terrorists' point of view, an effective terrorist act is one that puts people in constant fear of their lives. Terrorist acts that merely irritate people are not effective.</p>
<p>By this token, cyberterrorist acts cannot be useful as terrorist tools unless their effect tangibly protrudes into the real world. Being unable to electronically access a bank account is inconvenient, but doesn't strike the fear of death into victims as would a cyberterrorist attack against nuclear facilities, the power grid, or hospitals. Luckily, no one is colossally stupid enough to connect such vital systems to the Internet.</p>
<p>In lieu of such attacks against critical systems, cyberterrorist acts might play the same role as malware does in information warfare. Cyberterrorism can be used as a complement to traditional, real-world physical attacks, to confuse an enemy by disrupting computer-based communications for rescue efforts, or by sowing disinformation during a terrorist attack. Prior to an attack, misleading intelligence traffic can be generated. Terrorists have unfortunately shown themselves to be very good at lateral thinking, and a cyberterrorist attack is likely to strike something unexpected and undefended.</p>
<p>Are stricter laws and standards needed for these new weapons, these Internet-connected computers?<sup><a href="#f9_126" name="b9_126">126</a></sup></p>
<!--p.185-->
<p><strong>Notes for Chapter 9</strong></p>
<p><a name="f9_1" href="#b9_1">1</a> The benevolent effect may be accidental in some unusual cases. A man surrendered himself to German police after receiving a (false) message from a variant of the Sober worm claiming that he was being investigated. When the police <em>did</em> investigate, they found child pornography on the man's computer [264].</p>
<p><a name="f9_2" href="#b9_2">2</a> For example, Sobig did this [188].</p>
<p><a name="f9_3" href="#b9_3">3</a> The eye-roll-inducing term "cyberthief" is due to Schechter and Smith [277], on whom this section is based. Arguably, the thieves aren't hackers/crackers, because the machine is pre-cracked for their safety and comfort.</p>
<p><a name="f9_4" href="#b9_4">4</a> This would presumably be "cyberturf."</p>
<p><a name="f9_5" href="#b9_5">5</a> A <em>covert channel</em> is a means of transmitting information which was never intended for that purpose. For example, information can be leaked from an infected machine in unused <em>or</em> used network packet bits [269]. The problem of trying to prevent information leaks via covert channels is referred to as the <em>confinement problem</em> [179].</p>
<p><a name="f9_6" href="#b9_6">6</a> Strictly speaking, the original cryptovirus definition <em>requires</em> the use of strong, public-key cryptography [352]. A more general view of cryptoviruses, without the public-key requirement, is taken here.</p>
<p><a name="f9_7" href="#b9_7">7</a> For example, countries possessing or developing offensive computer virus capabilities include Bulgaria [204], China [49, 71, 232], Cuba [204], North Korea [49], Russia [321], Singapore [49], and Taiwan [49].</p>
<p><a name="f9_8" href="#b9_8">8</a> Falsehoods are limited by law and convention. Falsely seeming to have a larger force than actually exists, or falsely appearing to be attacking elsewhere to draw off enemy troops are completely acceptable feints. Pretending to surrender in order to lure out and ambush enemy troops is called an act of <em>perfidy</em> and is prohibited [130].</p>
<p><a name="f9_100" href="#b9_100">100</a> Cohen [75] makes a case for benevolent viruses.</p>
<p><a name="f9_101" href="#b9_101">101</a> McAfee [199].</p>
<p><a name="f9_102" href="#b9_102">102</a> Barber [26].</p>
<p><a name="f9_103" href="#b9_103">103</a> Perriot and Knowles [250].</p>
<p><a name="f9_104" href="#b9_104">104</a> Predator worms and their effects are studied in Toyoizumi and Kara [323], and Gupta and DuVarney [134].</p>
<p><a name="f9_105" href="#b9_105">105</a> These issues are discussed at length by Bontchev [40].</p>
<p><a name="f9_106" href="#b9_106">106</a> White [344].</p>
<p><a name="f9_107" href="#b9_107">107</a> See, for example, Harrison et al. [138] and Jansen and Karygiannis [152].</p>
<p><a name="f9_108" href="#b9_108">108</a> Harrison et al. [138, page 17].</p>
<!--p.186-->
<p><a name="f9_109" href="#b9_109">109</a> Aycock and Friess [23].</p>
<p><a name="f9_110" href="#b9_110">110</a> Schneier [279].</p>
<p><a name="f9_111" href="#b9_111">111</a> This section is based on Young and Yung [352].</p>
<p><a name="f9_112" href="#b9_112">112</a> Bogeholz [37] and Vidstrom [335].</p>
<p><a name="f9_113" href="#b9_113">113</a> Bates [29] andFerbrache [103].</p>
<p><a name="f9_114" href="#b9_114">114</a> Panda Labs [240]. The $200 figure is from Panda Labs too [241].</p>
<p><a name="f9_115" href="#b9_115">115</a> The concept and term are from Kanuck [158, page 289].</p>
<p><a name="f9_116" href="#b9_116">116</a> O'Brien and Nusbaum [232].</p>
<p><a name="f9_117" href="#b9_117">117</a> Ellis [99] and Greenberg et al. [130].</p>
<p><a name="f9_118" href="#b9_118">118</a> This conservative approach and the point about communications infrastructure is from the Department of the Army [140].</p>
<p><a name="f9_119" href="#b9_119">119</a> The material on electronic countermeasures is based on Cramer and Pratt [82] unless otherwise noted.</p>
<p><a name="f9_120" href="#b9_120">120</a> From [16].</p>
<p><a name="f9_121" href="#b9_121">121</a> With the exception of intelligence gathering, these are also mentioned (occasionally using slightly different terminology) in Thomas [321].</p>
<p><a name="f9_122" href="#b9_122">122</a> Young and Yung [352].</p>
<p><a name="f9_123" href="#b9_123">123</a> These insertion possibilities are from [16, 82, 321].</p>
<p><a name="f9_124" href="#b9_124">124</a> Gordon and Ford [125, page 645], upon which this section is based.</p>
<p><a name="f9_125" href="#b9_125">125</a> Schaechter [275].</p>
<p><a name="f9_126" href="#b9_126">126</a> Ellis [99] examines the same suggestion in the context of information warfare.</p>
<!--p.187-->
<!--p.188-->
<h2><a name="ca"></a>Chapter 10 People and communities</h2>
<h3><a name="ca1"></a>10.1 Malware Authors</h3>
<div class="epigraph">
	'... [virus writers] have a chronic lack of girlfriends, are usually socially inadequate and are drawn compulsively to write self-replicating codes.'
	<p>- Jan Hruska, Sophos<sup><a href="#fa_100" name="ba_100">100</a></sup></p>
</div>
<br clear="all" />
<p>Very little is known about virus writers, much less malware authors in general. The reason for this is simple: very few of them are ever found.<sup><a href="#fa_1" name="ba_1">1</a></sup> Furthermore, the limited research that <em>has</em> been done does not support Hruska's quote above. The two big questions that the existing research begins to answer are who writes malware, and why do they do it?</p>
<h4><a name="ca11"></a>10.1.1 Who?</h4>
<p>Humans are a diverse lot, and there is always a danger when generalizing about any group of people. The work that has been done on virus writers has looked at four factors: age, sex, moral development, and technical skill.<sup><a href="#fa_101" name="ba_101">101</a></sup></p>
<p>The age of virus writers is varied. There are the stereotypical young adolescents, but also college students and employed adult professionals. The explosive growth of malware has really only taken place since the mid-1980s, and it is possible that older virus writers will be seen as time goes on.</p>
<p>Virus writers are predominantly male, with only occasional exceptions.<sup><a href="#fa_2" name="ba_2">2</a></sup> Females are typically regarded as inferior in the virus community, so it wouldn't be a particularly welcoming environment for them. There is also a theory that gender differences in moral development may partially explain the lack of females.<sup><a href="#fa_102" name="ba_102">102</a></sup></p>
<p>With respect to ethical and moral development, not all virus writers are the same, and some fall within "normal" ranges. There is also a general distaste for deliberately destructive code amongst the virus writers studied, and there is no one directly targeted by viruses - with the possible exception of anti-virus
<!--p.189-->
researchers! The lack of interest in destruction is borne out by the relatively small amount of malware which tries to do damage. The main reason that ethically-normal virus writers stop writing viruses is simply that they grow out of it.</p>
<p>Finally, there are the technical skills of virus writers, which are often derided by the anti-virus community. As with any software development, the barrier to entry is low for virus writing, and consequently a fair degree of bad programming is seen in virus writing as it is in any programming discipline. However, virus writers with real impact must have a variety of skills to field techniques like stealth and polymorphism, or employ lateral thinking to exploit new and unguarded attack vectors. Arguably the skill level of virus writers is a direct reflection of the increasing sophistication level of anti-virus defenses.<sup><a href="#fa_103" name="ba_103">103</a></sup></p>
<h4><a name="ca12"></a>10.1.2 Why?</h4>
<p>Attributing the motivation to write malware to a single factor is a gross oversimplification. In fact, not all driving forces behind the creation of malware may even be conscious motivations.<sup><a href="#fa_104" name="ba_104">104</a></sup> Malware may be written for a variety of reasons, including:</p>
<dl>
	<dt><strong>Fascination with technology.</strong></dt><dd>Exploring technology underpins hacker culture, and the same ideas apply to creating malware. Creating malware, like writing any software, poses an intellectual challenge. In fact, since the anti-virus community acts as an opponent, writing malware may even have a greater draw from a game-playing point of view than other forms of software development.</dd>
	<dt><strong>Fame.</strong></dt><dd>Virus writers are known to form informal groups to exchange information and communicate with like-minded people.<sup><a href="#fa_3" name="ba_3">3</a></sup> As with any group, people may want to achieve fame within their community which would mean creating cleverly-written malware with impact. Having a creation appear on the "top ten" lists of malware that many anti-virus companies maintain for their customers' information can result in prestige for the creator.</dd>
	<dt><strong>Graffiti.</strong></dt><dd>Malware writing can serve as a form of expression in much the same way that graffiti does in the physical world. Arguably, this is a malicious act, but one not specifically targeted to any one person or group.</dd>
	<dt><strong>Revenge.</strong></dt><dd>Malware can be used to exact revenge for some real or imagined slight, by a disgruntled employee or ex-spouse, for instance.</dd>
	<dt><strong>Ideology.</strong></dt><dd>Ideological motivations are difficult to assess unless the malware writer is found, because what appears to be political or religious motivation may just be a red herring. Having said that, there have been some instances which suggest this underlying cause. One version of the Code Red worm
<!--p.190-->
	attempted a DDoS on the White House web site, for instance.<sup><a href="#fa_4" name="ba_4">4</a></sup> The Cager Trojan horse<sup><a href="#fa_105" name="ba_105">105</a></sup> may have been religiously-motivated, because it tried to prevent infected machines from viewing adult web sites - an offender would be presented with a quote from the Qur'an in Arabic, English, and Persian, followed by advice in Persian on how to atone for looking at naughty pictures on the Internet.</dd>
	<dt><strong>Commercial sabotage.</strong></dt><dd>Malware can be hard to target accurately, but some attempts at sabotaging a single company have been seen. This may tie in to schemes for revenge, or possibly financial gain for a malware writer who hopes to take advantage of lower stock prices, for example.</dd>
	<dt><strong>Extortion.</strong></dt><dd>On occasion, malware has been used on a large scale to try and extort money from people.</dd>
	<dt><strong>Warfare and espionage.</strong></dt><dd>Malware can be used for military or intelligence purposes, or as a complement to traditional forms of warfare. Such malware can be employed by both established armies as well as terrorist groups.</dd>
	<dt><strong>Malware battles.</strong></dt><dd>A relatively recent development, malware writers can have their creations fight one another using the Internet as their battleground. This was seen in the Mydoom/Netsky/Bagle episode in 2004.<sup><a href="#fa_106" name="ba_106">106</a></sup></dd>
	<dt><strong>Commercial gain.</strong></dt><dd>Malware skills may be leveraged in various ways by others, resulting in malware authors being paid for their wares. For example, use of worm-constructed botnets may be sold to spammers.</dd>
</dl>
<p>Again, humans are complicated, and their motivations may not be simple.</p>
<p>The graffiti motivation is an interesting one which deserves further research. There is a relatively large amount of research on graffiti artists, and the parallels to virus writers are compelling. Females are marginalized there too; it has been suggested that females express "graffiti urges" in different ways than males,<sup><a href="#fa_107" name="ba_107">107</a></sup> and also that the graffiti subculture is an inherently masculine one.<sup><a href="#fa_108" name="ba_108">108</a></sup> Graffiti writers have an adversarial relationship with the authorities trying to stop them, but the two groups also share a curious bond. Motivations for graffiti writers flow from the adversarial contest, but also a desire for fame within their subculture, and a love of the art. Equivalents to malware battles and commercial gain exist in the graffiti world too.</p>
<h3><a name="ca2"></a>10.2 The Anti-Virus Community</h3>
<p>Malware authors and people in the anti-virus community have one thing in common: there isn't a lot written about either. The anti-virus community is shaped by a number of external forces, including external perceptions of them as well as customer demands and legal minefields.</p>
<!--p.191-->
<h4><a name="ca21"></a>10.2.1 Perceptions</h4>
<p>The most common perception about the anti-virus community is a conspiracy theory. Anti-virus companies have the most to gain by a steady stream of malware, so the argument goes, and anti-virus companies conveniently know how to defend against any new threats. There is no evidence whatsoever that supports this theory.</p>
<p>The evidence that <em>does</em> exist also doesn't support the conspiracy theory. If it were true, then anti-virus companies would want to boost revenue with the least amount of effort on their part - a rational plan. Any malware that wasn't noted by current or potential customers would therefore be wasted effort, and anti-virus researchers would work no more than was necessary.</p>
<p>There is lots and lots of malware that doesn't attract attention, though; not just variants but entire families of malware can go unnoticed by most anti-virus customers. Monitoring anti-virus updates and comparing that information to malware-related media stories is a good demonstration of this fact. The sheer volume of malware is inconsistent with the conspiracy theory, too, because far more effort is being expended by anti-virus researchers than would be necessary to sustain the industry.</p>
<p>Anti-virus researchers do benefit from staying ahead of malware writers, even if they don't produce the malware themselves. Researchers may monitor web sites frequented by malware writers for up-and-coming threats, especially so-called "virus exchange" or "vX" sites. Malware writers have also been known to send their latest creations directly to anti-virus companies, which tends to support the motivation of malware writing as an intellectual thrill rather than a destructive pursuit.<sup><a href="#fa_109" name="ba_109">109</a></sup></p>
<h4><a name="ca22"></a>10.2.2 Another Day in Paradise</h4>
<p>A workday for an anti-virus researcher is long, to begin with. An 80-hour work week is not uncommon for researchers,<sup><a href="#fa_110" name="ba_110">110</a></sup> which can obviously exact a personal toll.</p>
<p>Samples of potential malware candidates can be captured by anti-virus companies' own defensive systems, like firewalls and honeypots. Malware samples may also be submitted by customers; this is the scenario depicted by the flowchart in Figure 10.1.<sup><a href="#fa_111" name="ba_111">111</a></sup> Conceptually, there are two databases kept: one with known malware, the other with known malware-free, or "clean" files. Any submission is first checked against both these databases, in order to avoid re-analyzing a submission and to respond to customers as quickly as possible.</p>
<p>If the submission is absent from both databases, then it must be analyzed. There is still no guarantee that the submission is malicious, so this is the next thing to determine; if the answer is negative, then the clean file database can be updated with the result. Otherwise, for replicating malware, a large number of</p>
<!--p.192-->
<div align="center">
	<img src="img/mja01/figa1.gif" alt="Figure 10.1. Malware analysis workflow" />
	<p><strong>Figure 10.1. Malware analysis workflow</strong></p>
</div>
<!--p.193-->
<p>samples are produced to ensure that all manifestations of the malware variant are able to be detected. (Virus writers can try to derail this process by having their viruses mutate slowly.<sup><a href="#fa_5" name="ba_5">5</a></sup></p>
<p>Adding detection to the anti-virus software comes next. The result is verified against <em>both</em> databases, because detection of the new malware shouldn't interfere with existing detection, nor should it cause false positives. Testing will also try to catch problems on any of the platforms that the anti-virus software runs on. For this reason alone, anti-virus software is more challenging than malware writing, because malware doesn't have a customer base that complains if something goes wrong.</p>
<p>Finally, the malware database gets updated and the customer is notified. Most anti-virus companies have online "malware encyclopedias" which provide details about malware to the public, and these would also be updated at this time.</p>
<p>While a workday for an anti-virus researcher may be long, the workday for an anti-virus company may be endless. Anti-virus companies may maintain offices worldwide, strategically located in different time zones, so that around-the-clock security coverage can be given to their customers.<sup><a href="#fa_112" name="ba_112">112</a></sup></p>
<h4><a name="ca23"></a>10.2.3 Customer Demands</h4>
<p>Anti-virus customers have certain expectations of their anti-virus software, which can be simply stated: 100% perfect detection of known and unknown threats, with no false positives. This is an impossible task, of course.</p>
<p>Complicating matters is that different customers may want different "threats" to be detected. Techniques used by anti-virus software may be applied more generally to locate many types of programs - this is called <em>gray area detection</em>.<sup><a href="#fa_113" name="ba_113">113</a></sup> Anti-virus software may be employed to look for:</p>
<dl>
	<dt><strong>Jokes and games.</strong></dt><dd>"Joke" executables and games may be completely harmless, yet having them may violate corporate IT policies.</dd>
	<dt><strong>Cracking tools.</strong></dt><dd>The legitimacy of programs like password crackers and port scanners may depend on context. System administrators can use these programs to check for vulnerabilities and weak passwords in their own systems, but other users possessing these may be cause for alarm.</dd>
	<dt><strong>Adware.</strong></dt><dd>Spyware is now largely recognized as a threat, but adware may also pose a risk of leaking information to another party. Some people see adware as performing a useful function, and it's not always obvious what programs have been installed quietly, and what programs have been deliberately installed by a user.</dd>
	<dt><strong>Remote administration tools.</strong></dt><dd>Again, RATs may provide a useful service, but their presence may also constitute a security breach or a policy violation.</dd>
</dl>
<!--p.194-->
<div align="center">
	<img src="img/mja01/figa2.gif" alt="Figure 10.2. In the zoo vs. in the wild" />
	<p><strong>Figure 10.2. In the zoo vs. in the wild</strong></p>
</div>
<p>Gray area detection is a delicate matter, because vendors of legitimate software may object to having their product negatively classified by anti-virus software, and there may be legal ramifications for doing so. Some anti-virus vendors attempt to forestall legal action, especially for spyware, through an appeals process which software producers can follow if they feel that their software has been misclassified.<sup><a href="#fa_114" name="ba_114">114</a></sup> More generally, the threat of legal action is possible for any false positive.</p>
<h4><a name="ca24"></a>10.2.4 Engineering</h4>
<p>Malware is often categorized based on where it's located.<sup><a href="#fa_115" name="ba_115"></a></sup> Malware is said to be <em>in the wild</em> if it's actively spreading or otherwise functioning on anyone's computer. Malware not in the wild, which only exists in malware collections and anti-virus research labs, is <em>in the zoo</em>.<sup><a href="#fa_6" name="ba_6">6</a></sup> Accurately determining whether malware is actually in the wild requires omniscience in the general case, so an approximation is used. An organization called the WildList Organization <sup><a href="#fa_116" name="ba_116">116</a></sup> has a worldwide membership of anti-virus experts who verify malware occurrences and report their data, which is combined to form the WildList, a (presumably close) approximation of the malware in the wild at any given time. Malware on the WildList is confusingly referred to as being <em>In the Wild</em> (ItW). As Figure 10.2 shows, this means that malware can be in the wild but not In the Wild, but something In the Wild must be in the wild. Hopefully that clarifies things.<sup><a href="#fa_7" name="ba_7">7</a></sup></p>
<p>An argument can be made, from an engineering point of view, that the only threats that need to be detected are those that are in the wild, since anything in the zoo cannot pose a direct threat. Anti-virus software could potentially be made smaller and faster by only detecting malware in the wild, whose numbers can be several orders of magnitude lower than the total number of threats.<sup><a href="#fa_117" name="ba_117">117</a></sup></p>
<p>From a marketing point of view, however, this would be a bad idea. If company <em>A</em> advertises that they protect against 100,000 threats, and company <em>B</em>'s product only guards against 500 threats - even if they're really the only ones that are in the wild - then company <em>B</em> is at a competitive disadvantage.</p>
<!--p.195-->
<p>Marketing is somewhat of a sore spot in the anti-virus community in any case. Product claims of detecting 100% of known and unknown threats are obviously silly, and misrepresentation is one possible legal concern.<sup><a href="#fa_118" name="ba_118">118</a></sup></p>
<h4><a name="ca25"></a>10.2.5 Open Questions</h4>
<p>There are a number of interesting questions which (at least at this time) have no obvious answer.</p>
<ul>
	<li>Anti-virus products are installed in computer systems in an ideal place to perform any number of tasks,<sup><a href="#fa_119" name="ba_119">119</a></sup> like gray area detection. Should anti-virus software...
	<ul>
		<li>... supply a firewall? This is clearly in the realm of computer security, yet integrating firewall and anti-virus software may make both defenses vulnerable to attack by reducing the amount of software diversity.</li>
		<li>... provide content filtering? More gray area detection, content filtering would block objectionable content - or any content that might violate IT policy - from being received. Filtering might also watch outgoing content too, since sending offensive material (either intentionally, or through zombies) could damage a company's reputation.</li>
		<li>... perform spam detection? Anti-spam is a growing concern for anti-virus companies, although spam detection techniques have comparatively little overlap with malware detection techniques.</li>
		<li>... apply software patches? Where technical weaknesses are exploited by worms, for example, anti-virus disinfection may only be temporary if the vulnerability used as an infection vector is still present. The safest approach is probably not to apply relevant software patches, since doing so may accidentally break software, introducing more customer support and liability issues.</li>
	</ul></li>
	<li>Anti-virus researchers perform reverse engineering and decompilation legitimately as part of theirjobs, and also routinely decipher "protection measures." It's unlikely that any malware author will take them to task for this, but researchers may also trace into legitimate code or need to understand undocumented software mechanisms. At what point does this run afoul of copyright laws?<sup><a href="#fa_120" name="ba_120">120</a></sup></li>
	<li>Users of anti-virus software may occasionally be presented with quarantined files to assess. Are there situations in which looking at these files, or the data within them, violates privacy laws? This may be even riskier in the case of a false positive.</li>
<!--p.196-->
	<li>For computer owners, use of anti-virus software is a widespread practice. Does this mean that computer owners are liable for negligence if they don't use anti-virus software?<sup><a href="#fa_121" name="ba_121">121</a></sup> Do anti-virus companies have a captive market?</li>
</ul>
<!--p.197-->
<p><strong>Notes for Chapter 10</strong></p>
<p><a name="fa_1" href="#ba_1">1</a> This raises the question of where virus writers are physically located. There was once a virus "scene" which shifted from country to country [123], but the Internet has largely made geographical location irrelevant.</p>
<p><a name="fa_2" href="#ba_2">2</a> Gigabyte was one example of a female virus writer [207].</p>
<p><a name="fa_3" href="#ba_3">3</a> Arguably, virus writers form a subculture distinguished by their interest in viruses.</p>
<p><a name="fa_4" href="#ba_4">4</a> This was Code Red version 2, and the DDoS attack was thwarted [211].</p>
<p><a name="fa_5" href="#ba_5">5</a> Bontchev [46] calls this "slow polymorphism."</p>
<p><a name="fa_6" href="#ba_6">6</a> The "zoo" label is often heard applied to viruses, as in "a zoo virus."</p>
<p><a name="fa_7" href="#ba_7">7</a> If this figure were drawn to scale, the In the Wild circle would be a barely-visible dot in comparison to the zoo circle.</p>
<p><a name="fa_100" href="#ba_100">100</a> As quoted in an interview with Reuters [263].</p>
<p><a name="fa_101" href="#ba_101">101</a> Unless otherwise noted, this section and the next are based on [121, 123].</p>
<p><a name="fa_102" href="#ba_102">102</a> Gordon [121] and Bissett and Shipton [35] both suggest this.</p>
<p><a name="fa_103" href="#ba_103">103</a> Suggested, for example, in Nachenberg [217].</p>
<p><a name="fa_104" href="#ba_104">104</a> Bisset and Shipton [35] speculate on unconscious motivations, and suggest some possible conscious motivations, as do Harley et al. [137].</p>
<p><a name="fa_105" href="#ba_105">105</a> Laguerta [177].</p>
<p><a name="fa_106" href="#ba_106">106</a> Covered in a number of places, such as Sherer [286].</p>
<p><a name="fa_107" href="#ba_107">107</a> Landy and Steele [180], and expanded upon by Abel and Buckley [1]. The latter also examines whether graffiti derives from a Freudian urge to smear feces, no doubt an excellent topic for dinner conversation.</p>
<p><a name="fa_108" href="#ba_108">108</a> Macdonald [191], which was used for the remainder of these comments on graffiti as well.</p>
<p><a name="fa_109" href="#ba_109">109</a> These assertions are made in Schmehl [278], and virus trading is mentioned in interviews with virus writers [120].</p>
<p><a name="fa_110" href="#ba_110">110</a> This figure is from Bontchev [41].</p>
<p><a name="fa_111" href="#ba_111">111</a> This section is based on Vibert [334] and (to a lesser degree) Schmehl [278].</p>
<p><a name="fa_112" href="#ba_112">112</a> Kirsner [165].</p>
<p><a name="fa_113" href="#ba_113">113</a> Gray area detection is discussed in Purisma [257].</p>
<p><a name="fa_114" href="#ba_114">114</a> For example, [52].</p>
<p><a name="fa_115" href="#ba_115">115</a> This section is based on Wells [343].</p>
<p><a name="fa_116" href="#ba_116">116</a> http://www.wildlist.org</p>
<p><a name="fa_117" href="#ba_117">117</a> An argument in favor of zoo virus detection is made in Ferric and Perriot [104].</p>
<!--p.198-->
<p><a name="fa_118" href="#ba_118">118</a> Gamertsfelder[116].</p>
<p><a name="fa_119" href="#ba_119">119</a> Purisma [257].</p>
<p><a name="fa_120" href="#ba_120">120</a> These latter three questions are raised and analyzed in Gamertsfelder [116].</p>
<p><a name="fa_121" href="#ba_121">121</a> This was mentioned at the EICAR 2004 conference during the presentation of Vasiu and Vasiu's "Taxonomy of Malware" paper. Opinions are varied: de Villiers concludes (after a lengthy analysis) that'... most cases of virus infection involve negligence' [86, page 169], but Owens [237] is skeptical about individuals being held liable for infections.</p>
<!--p.199-->
<!--p.200-->
<h2><a name="cb"></a>Chapter 11 What should we do?</h2>
<p>A book of this nature would not be complete without some kind of prediction about the future of malware. Such predictions share the distinguished quality of being invariably wrong, so this prediction will cover a wide range of scenarios.</p>
<p><em>Vicious cyberattacks will cause the Internet to melt down, and all malware-relatedproblems will disappear within a year's time.</em></p>
<p>In reality, there is no magic single solution to malware. (And, if there was, be assured that a bread-crumb trail of patents would cover every part of it.) Current and foreseeable defenses are but a house of cards. They are based on assumptions about "typical" malware behavior, and assumptions about malware writers which dramatically underestimate them. One violation of the assumptions and the house of cards comes tumbling down, defenders left scrambling to prop it up again.</p>
<p>What is clear is that no human intervention is possible in some attacks due to their speed. More automatic countermeasures are needed, not necessarily to stop malware completely - there is no such thing as absolute security, after all - but slowing malware down to a manageable rate would be valuable in itself.</p>
<p>As for malware detection, it is an undecidable problem. No perfect solution is possible, and the only way to tackle such a problem is with heuristics. Heuristics, rules of thumb, are fallible. In other words, a technical arms race rages on between attackers and defenders. Whether or not the race is winnable is immaterial now; the finish line is still far off. Many excellent defensive steps that can be taken are not very technical at all, though:</p>
<dl>
	<dt><strong>Plan B.</strong></dt><dd>Organizations, and to some extent individual computer users, must have a plan for disaster recovery. What happens when defenses fail and malware strikes? Can machines be rebuilt, data be restored?</dd>
<!--p.201-->
	<dt><strong>Education.</strong></dt><dd>A broad view of education must be taken. Users must be educated to harden them to social engineering attacks, but education can't stop there. The next generation of computer scientists and computer programmers must be educated in depth about malware. Treating malware as a taboo subject is only security through obscurity.</dd>
	<dt><strong>Vendor pressure.</strong></dt><dd>It must be made clear to software vendors that security is a priority for their customers, a higher priority than more frilly features. Customers can also demand to know why software is riddled with technical weaknesses, which should make customers and vendors both ask some pointed questions of educators and software researchers.</dd>
	<dt><strong>Minimalism.</strong></dt><dd>Users must responsibly use features that are present, which in part comes through education. Enabled features like network servers provide more potential attack vectors than having all such features turned off.
		<p>At the extreme end of the minimalism scale, it can be argued that computers are <em>too</em> general-purpose. Malware affects computers because they are just another form of software for a computer to gleefully run. Special-purpose devices doing one thing, and only one thing, are one way to help avoid exploitable problems.</p></dd>
	<dt><strong>Software updating.</strong></dt><dd>Until less-vulnerable software can be produced, software updating will still be a necessity. Mechanisms and policies that facilitate updating are a good thing.</dd>
	<dt><strong>Layers of defense.</strong></dt><dd>If each defensive technique is only a partial solution, then deploy a variety of defenses. Defenses should ideally be chosen that are based on different underlying assumptions, so that the patchwork defensive quilt will hopefully still work even if some assumptions turn out to be false.</dd>
	<dt><strong>Avoiding monocultures.</strong></dt><dd>In biology, having all members of a species the same is a potentially fatal problem: one disease can wipe the species out. Yet that is exactly the fatal problem the majority of computers exhibit. This isn't necessarily to say that everyone should change operating systems and applications, although that is one coarse-grained way to avoid a monoculture. Monocultures can be avoided in part just by automatically injecting randomness into the data locations and code of programs.
		<p>Diversity can be achieved by separating functionality physically, too. For example, moving firewall functionality to a different physical device makes the overall defenses that much harder to completely overcome.</p></dd>
</dl>
<p>Will malware ever go away? Even if all technical vulnerabilities are fixed, there will still be human vulnerabilities. But the point is academic, because
<!--p.202-->
human nature virtually guarantees the large-scale availability of technical vulnerabilities for the foreseeable future. Suffice it to say that the computer security industry will continue to flourish, and security researchers will be employed for some time to come.</p>
<!--p.203-->
<!--p.204-->
<h2>References</h2>
<p>Many of these sources can be found on the Internet using a search engine, and underground sites tend to move around anyway, so URLs have been omitted except where there appears to be a meaningful single location for a document. The spelling and capitalization of author names/handles in the original sources has been preserved.</p>
<ol>
	<li>E. L. Abel and B. E. Buckley. The Handwriting on the Wall: Toward a Sociology and Psychology of Graffiti. Greenwood Press, 1977.</li>
	<li>B. Acohido and J. Swartz. Going price for network of zombie PCs: $2,000-$3,000. USA Today, 8 September 2004.</li>
	<li>L. M. Adleman. <a href="/lib/ala01.html">An abstract theory of computer viruses</a>. In Advances in Cryptology - CRYPTO '88 (LNCS 403), pages 354-374, 1990.</li>
	<li>P.-M. Agapow. <a href="/lib/apa00.html">Computational brittleness and evolution in machine language</a>. Complexity International, 3, 1996.</li>
	<li>A. V. Aho and M. J. Corasick. Efficient string matching: An aid to bibliographic search. Communications of the ACM, 18(6):333-340, 1975.</li>
	<li>A. V. Aho, M. Ganapathi, and S. W. K. Tjiang. Code generation using tree matching and dynamic programming. Journal of the ACM, 11(4):491-516, 1989.</li>
	<li>I. A. Al-Kadi. Origins of cryptology: the Arab contributions. Cryptologia, XVI(2):97-126, 1992.</li>
	<li>Aleph One. Smashing the stack for fun and profit. Phrack, 7(49), 1996.</li>
	<li>NQ. <a href="/lib/van00.html">Darwin</a>. Software - Practice and Experience, 2:93-96, 1972.</li>
	<li>M. Allen. The use of 'social engineering' as a means of violating computer systems. SANS Information Security Reading Room, 13 August 2001.</li>
	<li>J. P. Anderson. Computer security threat monitoring and surveillance, 15 April 1980.</li>
<!--p.205-->
	<li>J. P. Anderson. Computer security technology planning study: Volume II, October 1972. ESD-TR-73-51,Vol. II.</li>
	<li>Anonymous. <a href="/lib/ajw00.html">Understanding encryption and polymorphism</a>. Written by J. Wells?</li>
	<li>Anonymous. Double trouble. Virus Bulletin, page 5, April 1992.</li>
	<li>Anonymous. Peach virus targets Central Point. Virus Bulletin, pages 17-18, May 1992.</li>
	<li>Anonymous. Disabling technologies - a critical assessment. Jane's International Defense Review, 21(1), 1994.</li>
	<li>Anonymous. Winword.Concept. Virus Bulletin, page 3, October 1995.</li>
	<li>anonymous. Once upon a free() Phrack, 0x0b(0x39), 2001.</li>
	<li>W. A. Arbaugh, W. L. Fithen, and J. McHugh. Windows of vulnerability: A case study analysis. IEEE Computer, 33(12):52-59, 2000.</li>
	<li>S. Axelsson. Aspects of the modelling and performance of intrusion detection. Licentiate thesis, Department of Computer Engineering, Chalmers University of Technology, 2000.</li>
	<li>J. Aycock and K. Barker. Creating a secure computer virus laboratory. In 13th Annual EICAR Conference, 2004. 13pp.</li>
	<li>J. Aycock, R. deGraaf, and M. Jacobson, Jr. Anti-disassembly using cryptographic hash functions. Technical Report 2005-793-24, University of Calgary, Department of Computer Science, 2005.</li>
	<li>J. Aycock and N. Friess. Spam zombies from outer space. Technical Report 2006-808-01, University of Calgary, Department of Computer Science, 2006.</li>
	<li>B. S. Baker, U. Manber, and R. Muth. Compressing differences of executable code. In ACM SIGPLAN Workshop of Compiler Supportfor System Software, 1999.</li>
	<li>V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: A transparent dynamic optimization system. In Proceedings of the ACM SIGPLAN '00 Conference on Programming Language Design and Implementation (PLDI), pages 1-12, 2000.</li>
	<li>B. Barber. Cheese worm: Pros and cons of a "friendly" worm. SANS Information Security Reading Room, 2001.</li>
	<li>A. Bartolich. <a href="/lib/vab00.html">The ELF virus writing HOWTO</a>, 15 February 2003.</li>
	<li>L. E. Bassham and W. T. Polk. Threat assessment of malicious code and human threats. Technical Report IR 4939, NIST, October 1992.</li>
	<li>J. Bates. Trojan horse: AIDS information introductory diskette version 2.0. Virus Bulletin, pages 3-6, January 1990.</li>
	<li>BBC News. Passwords revealed by sweet deal, 20 April 2004.</li>
	<li>BBC News. How to sell your self for a song, 24 March 2005.</li>
	<li>J.R.Bell. Threaded code. Communications of the ACM, 16(6):370-372, 1973.</li>
<!--p.206-->
	<li>G. Benford. Worlds Vast and Various. EOS, 2000.</li>
	<li>J. L. Bentley. Writing Efficient Programs. Prentice-Hall, 1982.</li>
	<li>A. Bissett and G. Shipton. <a href="/lib/mab00.html">Some human dimensions of computer virus creation and infection</a>. InternationalJournal of Human-Computer Studies, 52:899-913, 2000.</li>
	<li>blexim. Basic integer overflows. Phrack, 0x0b(0x3c), 2002.</li>
	<li>H. Bogeholz. At your disservice: How ATA security functions jeopardize your data, c't 8/2005, S. 172: Hard Disk Security, 1 April 2005.</li>
	<li>V. Bontchev. <a href="/lib/avb04.html">Possible virus attacks against integrity programs and how to prevent them</a>. In Virus Bulletin Conference, pages 131-141, 1992.</li>
	<li>V. Bontchev. <a href="/lib/avb01.html">Analysis and maintenance of a clean virus library</a>. In Virus Bulletin Conference, pages 77-89, 1993.</li>
	<li>V. Bontchev. <a href="/lib/avb02.html">Are "good" computer viruses still a bad idea?</a> In Proceedings of the 3rd Annual EICAR Conference, pages 25-47, 1994.</li>
	<li>V. Bontchev. <a href="/lib/avb03.html">Future trends in virus writing</a>, 1994.</li>
	<li>V. Bontchev. Possible macro virus attacks and how to prevent them. Computers &amp; Security, 15(7):595-626, 1996.</li>
	<li>V. Bontchev. <a href="/lib/avb10.html">Macro virus identification problems</a>. Computers &amp; Security, 17(l):69-89, 1998.</li>
	<li>V. Bontchev. Anti-virus spamming and the virus-naming mess: Part 2. Virus Bulletin, pages 13-15, July 2004.</li>
	<li>V. Bontchev. The real reason for the decline of the macro virus. Virus Bulletin, pages 14-15, January 2006.</li>
	<li>V. V. Bontchev. Methodology of Computer Anti-Virus Research. PhD thesis. University of Hamburg, 1998.</li>
	<li>Jordi Bosveld. Online malware scan, http://virusscan.jotti.org/.</li>
	<li>T. M. Breuel. Lexical closures for C-i-H. In USENIX C++ Conference Proceedings, pages 293-304, 1988.</li>
	<li>D. Bristow. Asia: grasping information warfare? Jane's Intelligence Review, 1 December 2000.</li>
	<li>J. Brunner. <a href="/lib/mjb01.html">The Shockwave Rider</a>. Ballantine, 1975.</li>
	<li>Bulba and Kil3r. Bypassing StackGuard and StackShield. Phrack, Oxa(Ox38), 2000.</li>
	<li>CA. eTrust PestPatrol vendor appeal process. CA Spyware Information Center, 25 April 2005. Version 1.1.</li>
	<li>CARO. <a href="/lib/asb01.html">A new virus naming convention</a>, c. 1991.</li>
	<li>K. Carr. Sophos anti-virus detection: a technical overview, October 2002.</li>
<!--p.207-->
	<li>CERT. Cert incident note IN-2001-09. http://www.cert.org/incident.notes/IN-2001-09.html, 6 August 2001.</li>
	<li>K. Cesare. <a href="/lib/akc00.html">Prosecuting computer virus authors: The need for an adequate and immediate international solution</a>. The Transnational Lawyer, 14:135-170, 2001.</li>
	<li>S. Cesare. <a href="/lib/vsc04.html">Linux anti-debugging techniques (fooling the debugger)</a>, 1999.</li>
	<li>S. Cesare. <a href="/lib/vsc02.html">Unix viruses</a>. Undated, post-October 1998.</li>
	<li>D. A. Chambers. Method and apparatus for detection of computer viruses. United States Patent #5,398,196, 14 March 1995.</li>
	<li>B. Chan, J. Denzinger, D. Gates, K. Loose, and J. Buchanan. Evolutionary behavior testing of commercial computer games. In Proceedings of the 2004 IEEE Congress on Evolutionary Computation (CEC), pages 125-132, 2004.</li>
	<li>E. Y. Chen, J. T. Ro, M. M. Deng, and L. M. Chi. System, apparatus and method for the detection and removal of viruses in macros. United States Patent #5,951,698, 14 September 1999.</li>
	<li>S. Chen and S. Ranka. Detecting Internet worms at early stage. IEEEJournal on Selected Areas in Communications, 23(10):2003-2012, 2005.</li>
	<li>X. Chen and J. Heidemann. Detecting early worm propagation through packet matching. Technical Report ISI-TR-2004-585, University of Southern California, Information Sciences Institute, 2004.</li>
	<li>D. M. Chess. <a href="/lib/adc05.html">Virus verification and removal</a>. Virus Bulletin, pages 7-11, November 1991.</li>
	<li>D. M. Chess, R. Ford, J. O. Kephart, and M. G. Swimmer. System and method for detecting and repairing document-infecting viruses using dynamic heuristics. United States Patent #6,711,583, 23 March 2004.</li>
	<li>D. M. Chess, J. O. Kephart, and G. B. Sorkin. Automatic analysis of a computer virus structure and means of attachment to its hosts. United States Patent #5,485,575, 16 January 1996.</li>
	<li>B. Cheswick. An evening with Berferd in which a cracker is lured, endured, and studied. In Proceedings of the Winter USENIX Conference, 1992.</li>
	<li>W. R. Cheswick and S. M. Bellovin. Firewalls and Internet Security: Repelling the Wily Hacker. Addison-Wesley, 1994.</li>
	<li>D. Chi. Detection and elimination of macro viruses. United States Patent #5,978,917, 2 November 1999.</li>
	<li>E. Chien and P. Szor. Blended attacks exploits, vulnerabilities and buffer-overflow techniques in computer viruses. In Virus Bulletin Conference, pages 72-106, 2002.</li>
	<li>Chosun Ilbo. N. Korea's hackers rival CIA, expert warns. Digital Chosunilbo (English Edition), 2 June 2005.</li>
	<li>CIAC. Information about hoaxes. http://hoaxbusters.ciac.org/HBHoaxInfo.html.</li>
<!--p.208-->
	<li>Cisco Systems, Inc. Cisco threat defense system guide: How to provide effective worm mitigation, April 2004.</li>
	<li>F. Cohen. <a href="/lib/afc01.html">Computer viruses: Theory and experiments</a>. Computers &amp; Security, 6(1):22-35, 1987.</li>
	<li>F. B. Cohen. <a href="/lib/afc13.html">A Short Course on Computer Viruses</a>. Wiley, second edition, 1994.</li>
	<li>C. Collberg, C. Thomborson, and D. Low. A taxonomy of obfuscating transformations. Technical Report 148, University of Auckland, Department of Computer Science, 1997.</li>
	<li>Computer Associates. Security advisor center glossary. http://www3.ca.com/securityadvisor/glossary.aspx, 2005.</li>
	<li>M. Conover and wOOwOO Security Team. wOOwOO on heap overflows, 1999.</li>
	<li>E. Cooke, F. Jahanian, and D. McPherson. The zombie roundup: Understanding, detecting, and disrupting botnets. In USENIX SRUTI Workshop, 2005.</li>
	<li>C. Cowan, M. Barringer, S. Beattie, and G. Kroah-Hartman. FormatGuard: Automatic protection from printf format string vulnerabilities. In Proceedings ofthe 10th USENIX Security Symposium, 2001.</li>
	<li>CrackZ. Anti-debugging &amp; software protection advice, 25 April 2003.</li>
	<li>M. L. Cramer and S. R. Pratt. Computer virus countermeasures - a new type of electronic warfare. In L. J. Hoffman, editor. Rogue Programs: Viruses, Worms, and Trojan Horses, chapter 20, pages 246-260. Van Nostrand Reinhold, 1990.</li>
	<li>I. Daniloff. <a href="/lib/aid03.html">Fighting talk</a>. Virus Bulletin, pages 10-12, December 1997.</li>
	<li>I. Dawson. Blind buffer overflows in ISAPI extensions. SecurityFocus, 25 January 2005.</li>
	<li>T. de Raadt. Exploit mitigation techniques. AUUG'2004 Annual Conference.</li>
	<li>M. de Villiers. Computer viruses and civil liability: A conceptual framework. Tort Trial &amp; Insurance Practice Law Journal, 40:1:123-179, 2004.</li>
	<li>J. Dellinger. Re: Prize for most useful computer virus. Risks Digest, 12(30), 1991.</li>
	<li>N. Desai. Intrusion prevention systems: the next step in the evolution of IDS. Security-Focus, 27 February 2003.</li>
	<li>t. detristan, t. ulenspiegel, yann_malcom, and m. s. von underduk. Polymoiphic shellcode engine using spectrum analysis. Phrack, 0x0b(0x3d), 2003.</li>
	<li>R. B. K. Dewar. Indirect threaded code. Communications of the ACM, 18(6):330-331, 1975.</li>
	<li>A. K. Dewdney. <a href="/lib/mad01.html">In the game called Core War hostile programs engage in a battle of bits</a>. Scientific American, 250(5yA4-22, 1984.</li>
	<li>A. K. Dewdney. <a href="/lib/mad02.html">A Core War bestiary of viruses, worms and other threats to computer memories</a>. Scientific American, 252(3yA 4-23, 1985.</li>
<!--p.209-->
	<li>U. Drepper. Security enhancements in Red Hat Enterprise Linux (beside SELinux), 16 June 2004.</li>
	<li>P. Ducklin. Counting viruses. In Virus Bulletin Conference, pages 73-85, 1999.</li>
	<li>T. Duff. Experience with viruses on UNIX systems. Computing Systems, 2(2): 155-171, 1989.</li>
	<li>EICAR. The anti-virus test file, http://www.eicar.org/anti_virus_test_file.htm, 1 May 2003.</li>
	<li>M. W. Eichin and J. A. Rochlis. <a href="/lib/aem02.html">With microscope and tweezers: An analysis of the Internet virus of November 1988</a>. In Proceedings of the 1989 IEEE Symposium on Security and Privacy, pages 326-343, 1989.</li>
	<li>I. K. El Far, R. Ford, A. Ondi, and M. Pancholi. On the impact of short-term email message recall on the spread of malware. In Proceedings of the 14th Annual EICAR Conference, pages 175-189, 2005.</li>
	<li>B. W. Ellis. The international legal implications and limitations of information warfare: What are our options? USAWC Strategy Research Report, 10 April 2001.</li>
	<li>J. Erickson. Hacking: The Art of Exploitation. No Starch Press, 2003.</li>
	<li>F. Esponda, S. Forrest, and P. Helman. A formal framework for positive and negative detection schemes. IEEE Transactions on Systems, Man, and Cybernetics, 34(1):357-373, 2004.</li>
	<li>H. Etoh. Stack protection schemes: (propolice, StackGuard, XP SP2). PacSec/core04 Conference, 2004.</li>
	<li>D. Ferbrache. A Pathology of Computer Viruses. Springer-Verlag, 1992.</li>
	<li>P. Ferrie and F. Perriot. <a href="/lib/apf11.html">Detecting complex viruses</a>. SecurityFocus, 6 December 2004.</li>
	<li>P. Ferrie and H. Shannon. <a href="/lib/apf50.html">It's Zell(d)ome the one you expect</a>. Virus Bulletin, pages 7-11, May 2005.</li>
	<li>P. Ferrie and P. Szor. <a href="/lib/apf47.html">Zmist opportunities</a>. Virus Bulletin, pages 6-7, March 2001.</li>
	<li>E. Filiol. <a href="/lib/aef02.html">Strong cryptography armoured computer viruses forbidding code analysis: The Bradley virus</a>. In Proceedings of the 14th Annual EICAR Conference, pages 216-227, 2005.</li>
	<li>C. Fischer. TREMOR analysis (PC). VIRUS-L Digest, 6(88), 1993.</li>
	<li>N. FitzGerald. A virus by any other name - virus naming updated. Virus Bulletin, pages 7-9, January 2003.</li>
	<li>H. Flake. Structural comparison of executable objects. In Workshop on Detection of Intrusions and Malware &amp; Vulnerability Assessment (DIMVA), 2004.</li>
	<li>B. Flint and M. Hughes. Fast virus scanning using session stamping. United States Patent #6,735,700, 11 May 2004.</li>
<!--p.210-->
	<li>E. Florio. Backdoor.Ryknos. Symantec Security Response, 22 November 2005.</li>
	<li>R. Ford and J. Michalske. Gatekeeper II: New approaches to generic virus prevention. In Virus Bulletin Conference, pages 45-50, 2004.</li>
	<li>R. Ford and H. H. Thompson. The future of proactive virus detection. In 13th Annual EICAR Conference, 2004. 11pp.</li>
	<li>R. Foulkes and J. Morris. Fighting worms in a large corporate environment: A design for a network anti-worm solution. In Virus Bulletin Conference, pages 56-66, 2002.</li>
	<li>L. Gamertsfelder. Anti-virus technologies - filtering the legal issues. In Virus Bulletin Conference, pages 31-35, 2003.</li>
	<li>S. Garfink and M. Landesman. Lies, damn lies and computer virus costs. In Virus Bulletin Conference, pages 20-23, 2004.</li>
	<li>D. Gerrold. <a href="/lib/mdg00.html">When Harlie Was One</a>. Nelson Doubleday, 1972.</li>
	<li>R Gillingwater. Re: Where did they come from ? (PC), comp.virus, 27 November 1989.</li>
	<li>S. Gordon. Faces behind the masks, 1994.</li>
	<li>S. Gordon. <a href="/lib/asg03.html">The generic virus writer</a>. In Virus Bulletin Conference, 1994.</li>
	<li>S. Gordon. What a (Winword.)Concept. Virus Bulletin, pages 8-9, September 1995.</li>
	<li>S. Gordon. <a href="/lib/asg04.html">The generic virus writer II</a>. In Virus Bulletin Conference, 1996.</li>
	<li>S. Gordon. Spyware 101: Exploring spyware and adware risk assessment. In 14th Annual EICAR Conference, pages 204-215, 2005.</li>
	<li>S. Gordon and R. Ford. Cyberterrorism? Computers &amp; Security, 21(7):636-647, 2002.</li>
	<li>S. Gordon, R. Ford, and J. Wells. <a href="/lib/asg05.html">Hoaxes &amp; hypes</a>. In Virus Bulletin Conference, 1997.</li>
	<li>D. Gragg. A multi-level defense against social engineering. SANS Information Security Reading Room, 2002.</li>
	<li>S. Granger. Social engineering fundamentals, part I: Hacker tactics. SecurityFocus, 18 December 2001.</li>
	<li>S. Granger. Social engineering fundamentals, part II: Combat strategies. SecurityFocus, 9 January 2002.</li>
	<li>L. T. Greenberg, S. E. Goodman, and K. J. Soo Hoo. Information Warfare and International Law. National Defense University Press, 1998.</li>
	<li>GriYo. <a href="/lib/vgy01.html">EPO: Entry-point obscuring</a>. 29A e-zine, 4, c. 2000.</li>
	<li>grugq and scut. Armouring the ELF: Binary encryption on the UNIX platform. Phrack, 0x0b(0x3a),2001.</li>
	<li>D. O. Gryaznov. <a href="/lib/adg00.html">Scanners of the year 2000: Heuristics</a>. In Virus Bulletin Conference, pages 225-234, 1995.</li>
<!--p.211-->
	<li>A. Gupta and D. C. DuVarney. Using predators to combat worms and viruses: A simulation-based study. In 20th Annual Computer Security Applications Conference, 2004.</li>
	<li>M. Handley, V. Paxson, and C. Kreibich. Network intrusion detection: Evasion, traffic normalization, and end-to-end protocol semantics. In Proceedings of the 10th USENIX Security Symposium, 2001.</li>
	<li>Had. People hacking: The psychology of social engineering. Access All Areas III, 1997.</li>
	<li>D. Harley, R. Slade, and U. E. Gattiker. <a href="/lib/ars08.html">Viruses Revealed</a>. Osborne/McGraw-Hill, 2001.</li>
	<li>C. G. Harrison, D. M. Chess, and A. Kershenbaum. Mobile agents: Are they a good idea? IBM Research Report, 28 March 1995.</li>
	<li>R. Hasson. Anti-debugging tips, http://www.soft-analysts.com/debugging.php, 13 February 2003.</li>
	<li>Headquarters, Department of the Army. Information operations. Field manual No. 100-6, 27 August 1996. United States Army.</li>
	<li>H. J. Highland. A macro virus. Computers &amp; Security, 8(3):178-188, 1989.</li>
	<li>N. Hindocha and E. Chien. Malicious threats and vulnerabilities in instant messaging. In Virus Bulletin Conference, pages 114-124, 2003.</li>
	<li>S. A. Hofmeyr, S. Forrest, and A. Somayaji. Intrusion detection using sequences of system calls. Journal of Computer Security, 6:151-180, 1998.</li>
	<li>G. Hoglund and J. Butler. Rootkits: subverting the Windows kernel. Addison-Wesley, 2006.</li>
	<li>T. Holz and F. Raynal. Defeating honeypots: System issues, part 1. SecurityFocus, 23 March 2005.</li>
	<li>R. N. Horspool andN. Marovac. Anapproachtotheproblemofdetranslation of computer programs. The Computer Journal, 23{3y223-229, 1980.</li>
	<li>M. Howard. Reviewing code for integer manipulation vulnerabilities. MSDN Library, 28 April 2003.</li>
	<li>J. W. Hunt and M. D. Mcllroy. An algorithm for differential file comparison. Technical Report 41, Bell Laboratories, Computer Science, 1976.</li>
	<li>M. Hypponen. <a href="/lib/amk01.html">Retroviruses - how viruses fight back</a>. In Virus Bulletin Conference, 1994.</li>
	<li>M. Hypponen. Santy. F-Secure Virus Descriptions, 21 December 2004.</li>
	<li>C. Itshak, N. Vitaly, and M. Taras. Virus detection system. Canadian Patent Application #2,460,607, 27 March 2003.</li>
	<li>W. Jansen and T. Karygiannis. Mobile agent security. NIST Special Publication 800-19, 1999.</li>
<!--p.212-->
	<li>Japan Times. Bug in antivirus software hits LANs at JR East, some media, 24 April 2005.</li>
	<li>M. Jordan. <a href="/lib/ajm02.html">Dealing with metamorphism</a>. Virus Bulletin, pages 4-6, October 2002.</li>
	<li>R. Joshi, G. Nelson, and K. Randall. Denali: a goal-directed superoptimizer. In Proceedings oftheACMSIGPLAN2002 Conference on Programming language design and implementation, pages 304-314, 2002.</li>
	<li>J. Jung, V. Paxson, A. W. Berger, and H. Balakrishnan. Fast poitscan detection using sequential hypothesis testing. In Proceedings of the 2004 IEEE Symposium on Security and Privacy, pages 211-225, 2004.</li>
	<li>J. E. Just and M. Cornwall. Review and analysis of synthetic diversity for breaking monocultures. In Proceedings of the 2004 ACM Workshop on Rapid Malcode, pages 23-32, 2004.</li>
	<li>S. R Kanuck. Information warfare: New challenges for public international law. Harvard International Law Journal, 37(l):272-292, 1996.</li>
	<li>E. Kaspersky. Dichotomy: Double trouble. Virus Bulletin, pages 8-9, December 1994.</li>
	<li>E. Kaspersky. RMNS - the perfect couple. Virus Bulletin, pages 8-9, May 1995.</li>
	<li>Kaspersky Lab. Virus.DOS.Whale, 2000. Whale appeared c. 1990.</li>
	<li>Kaspersky Lab. Virus.Winl6.Apparition.a, 2000. Apparition appeared c. 1998.</li>
	<li>J. O. Kephart, A. G. G. Morin, G. B. Sorkin, and J. W. Wells. Efficient detection of computer viruses and other data traits. United States Patent #6,016,546, 18 January 2000.</li>
	<li>V. Kiriansky, D. Bruening, and S. Amarasinghe. Secure execution via program shepherding. In Proceedings of the 11th USENIX Security Symposium, 2002.</li>
	<li>S. Kirsner. Sweating in the hot zone. Fast Company, 99, October 2005.</li>
	<li>P. Klint. Interpretation techniques. Software - Practice and Experience, 11:963-973, 1981.</li>
	<li>klog. The frame pointer overwrite. Phrack, 9(55), 1999.</li>
	<li>D. E. Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching. Addison-Wesley, second edition, 1998.</li>
	<li>C. W. Ko. Method and apparatus for detecting a macro computer virus using static analysis. United States Patent #6,697,950, 24 February 2004.</li>
	<li>V. Kouznetsov and A. Ushakov. System and method for efficiently managing computer virus definitions using a structured virus database. United States Patent #6,622,150, 16 September 2003.</li>
	<li>J. Koziol, D. Aitel, D. Litchfield, C. Anley, S. Eren, N. Mehta, and R. Hassell. The Shellcoder's Handbook: Discovering and Exploiting Security Holes. Wiley, 2004,</li>
<!--p.213-->
	<li>Krakowicz. Krakowicz's kracking korner: The basics of kracking II, c. 1983.</li>
	<li>N. Krawetz. Anti-honeypot technology. IEEE Security &amp; Privacy, pages 76-79, January/February 2004.</li>
	<li>S. Kumar and E. H. Spafford. <a href="/lib/aes04.html">A generic virus scanner in C++</a>. In Proceedings of the 8th Computer Security Applications Conference, pages 210-219, 1992.</li>
	<li>C. J. Kuo, J. Koltchev, D.-C. Zheng, and J. Peter. Method of treating whitespace during virus detection. United States Patent #6,230,288, 8 May 2001.</li>
	<li>J. Kuo and D. Beck. The common malware enumeration (CME) initiative. Virus Bulletin, pages 14-15, September 2005.</li>
	<li>Z. M. Laguerta. TROJ.CAGER.A. Trend Micro Virus Encyclopedia, 6 September 2005.</li>
	<li>A. Lakhotia, A. Kapoor, and E. U. Kumar. Are metamorphic viruses really invincible? part 1. Virus Bulletin, pages 5-7, December 2004.</li>
	<li>B. W. Lampson. A note on the confinement problem. Communications of the ACM, 16(10):613-615, 1973.</li>
	<li>E. E. Landy and J. M. Steele. Graffiti as a function of building utilization. Perceptual and Motor Skills, 25:111-112, 1967.</li>
	<li>T. Laundrie. All we need is love. rec.humor.funny ILOVEYOU digest, joke attributed to M. Barker, 8 May 2000.</li>
	<li>A. J. Lee. Hunting the unicorn. Virus Bulletin, pages 13-16, May 2004.</li>
	<li>J. R. Levine. Linkers and Loaders. Morgan Kaufmann, 2000.</li>
	<li>J. Leyden. Americans are pants at password security. The Register, 6 May 2005.</li>
	<li>Y. Liu. Avkiller.Trojan. Symantec Security Response, 17 May 2002.</li>
	<li>R. W. Lo, K. N. Levitt, and R. A. Olsson. MCF: a malicious code filter. Computers &amp; Security, 14(6):541-566, 1995.</li>
	<li>M. Ludwig. <a href="/lib/vml01.html">The Giant Black Book of Computer Viruses</a>. American Eagle, second edition, 1998.</li>
	<li>LURHQ. Sobig.a and the spam you received today, 21 April 2003.</li>
	<li>J. Lyman. Name that worm - how computer viruses get their names. NewsFactor Technology News, 8 January 2002.</li>
	<li>J. Ma, G. M. Voelker, and S. Savage. Self-stopping worms. In Proceedings of the 2005 ACM Workshop on Rapid Malcode, pages 12-21, 2005.</li>
	<li>N. Macdonald. The Graffiti Subculture: Youth, Masculinity and Identity in London and New York. Palgrave, 2001.</li>
	<li>G. M. Mallen-Fullerton. The minimum size of virus identification signatures. In Fifth International Computer Virus &amp; Security Conference, pages 813-817, 1992.</li>
<!--p.214-->
	<li>O. Mann. Method for recovery of a computer program infected by a computer virus. United States Patent #5,408,642, 18 April 1995.</li>
	<li>Marc. Re: Blind remote buffer overflow. VULN-DEV List, 28 April 2000.</li>
	<li>A. Marinescu. Win32/CTX virus description. RAV Antivirus, 15 November 1999 (detection date).</li>
	<li>H. Massalin. Superoptimizer: a look at the smallest program. In Proceedings of the Second International Conference on Architectual Supportfor Programming Languages and Operating Systems, ^2igQ^ 122-126, 1987.</li>
	<li>A. Matrawy, R C. van Oorschot, and A. Somayaji. Mitigating network denial-of-service through diversity-based traffic management. In Proceedings of the 3rd International Conference on Applied Cryptography and Network Security, LNCS 3531, pages 104-121,2005.</li>
	<li>McAfee Inc. ZeroHunt. Virus Information Library, 15 December 1990.</li>
	<li>McAfee Inc. Den Zuk. Virus Information Library, 1988.</li>
	<li>McAfee Inc. WM/Colors.D;M;P. Virus Information Library, 1997.</li>
	<li>M. D. Mcllroy, R. Morris, and V A. Vyssotsky. <a href="/lib/mdm00.html">Letter to No</a>, c/o C. A. Lang, editor, Software - Practice and Experience, http://www.cs.dartmouth.edu/~doug/darwin.pdf, 29 June 1971.</li>
	<li>M. K. McKusick, K. Bostic, M. J. Karels, and J. S. Quarterman. The Design and Implementation of the 4.4BSD Operating System. Addison-Wesley, 1996.</li>
	<li>MessageLabs. MessageLabs Intelligence Annual Email Security Report 2004, 2004.</li>
	<li>E. Messmer. Threat of 'infowar' brings CIA warnings. Network World, 13 September 1999.</li>
	<li>Methyl. Tunneling with single step mode. Undated, post-1989.</li>
	<li>Z. Michalewicz and D. B. Fogel. How to Solve It: Modern Heuristics. Springer-Verlag, 2000.</li>
	<li>J. Middleton. Virus writers get behind Gigabyte, vnunet.com, 13 May 2002.</li>
	<li>MidNyte. <a href="/lib/vmn04.html">An introduction to encryption, part I</a>, April 1999.</li>
	<li>B. R Miller, L. Fredriksen, and B. So. Study of the reliability of UNIX utilities. Communications of the ACM, 33(12):32-44, 1990.</li>
	<li>G. Molnar and G. Szappanos. Casualties of war: W32/Ganda. Virus Bulletin, pages 7-10, May 2003.</li>
	<li>D. Moore and C. Shannon. The spread of the code-red worm (crv2). CAIDA analysis, C.2001.</li>
	<li>D. Moore, C. Shannon, and J. Brown. Code-Red: a case study on the spread and victims of an Internet worm. In 2nd Internet Measurement Workshop, 2002.</li>
<!--p.215-->
	<li>P. Morley. The biggie. Virus Bulletin, pages 10-11, November 1998.</li>
	<li>I. Muttik. <a href="/lib/aim01.html">Stripping down an AV engine</a>. In Virus Bulletin Conference, pages 59-68, 2000.</li>
	<li>C. Nachenberg. Antivirus accelerator. United States Patent #6,021,510, 1 February 2000.</li>
	<li>C. Nachenberg. Behavior blocking: The next step in anti-virus protection. SecurityFocus, 19 March 2002.</li>
	<li>C. Nachenberg. <a href="/lib/acn01.html">Computer virus-antivirus coevolution</a>. Communications of the ACM, 40(1):46-51, 1997.</li>
	<li>C. Nachenberg. Emulation repair system. United States Patent #6,067,410, 23 May 2000.</li>
	<li>C. S. Nachenberg. Data driven detection of viruses. United States Patent #6,851,057, 1 February 2005.</li>
	<li>C. S. Nachenberg. Dynamic heuristic method for detecting computer viruses using decryption exploration and evaluation phases. United States Patent#6,357,008,12 March 2002.</li>
	<li>C. S. Nachenberg. Polymorphic virus detection module. United States Patent #5,826,013, 20 October 1998.</li>
	<li>C. S. Nachenberg. Histogram-based virus detection. Canadian Patent Application #2,403,676, 20 September 2001.</li>
	<li>C. S. Nachenberg. State-based cache for antivirus software. United States Patent #5,999,723, 7 December 1999.</li>
	<li>R. Naraine. Microsoft's security response center: How little patches are made. eWeek, 8 June 2005.</li>
	<li>K. Natvig. Sandbox technology inside AV scanners. In Virus Bulletin Conference, pages 475-488,2001.</li>
	<li>K. Natvig. Sandbox II: The Internet. In Virus Bulletin Conference, pages 125-141,2002.</li>
	<li>G. Navarro and M. Raffinot. Flexible Pattern Matching in Strings. Cambridge, 2002.</li>
	<li>G. Navarro and J. Tarhio. LZgrep: a Boyer-Moore string matching tool for Ziv-Lempel compressed text. Software - Practice and Experience, 35(12): 1107-1130, 2005.</li>
	<li>J. Nazario. <a href="/lib/anj01.html">Defense and Detection Strategies against Internet Worms</a>. Artech House, 2004.</li>
	<li>J. Nazario, J. Anderson, R. Wash, and C. Connelly. The future of Internet worms. In Blackhat Briefings, 2001.</li>
	<li>Nergal. The advanced return-into-lib(c) exploits: PaX case study. Phrack, 0x0b(0x3a), 2001.</li>
<!--p.216-->
	<li>K. O'Brien and J. Nusbaum. Intelligence gathering on asymmetric threats -- part one. Jane's Intelligence Review, 1 October 2000.</li>
	<li>H. O'Dea. Trapping worms in a virtual net. In Vims Bulletin Conference, pages 176-186, 2004.</li>
	<li>L. Oudot. Fighting Internet worms with honeypots. SecurityFocus, 23 October 2003.</li>
	<li>L. Oudot and T. Holz. Defeating honeypots: Network issues, part I. SecurityFocus, 28 September 2004.</li>
	<li>M. Overton. Worm charming: Taking SMB-Lure to the next level. In Virus Bulletin Conference, 2003.</li>
	<li>R. C. Owens. Turning worms: Some thoughts on liabilities for spreading computer infections. Canadian Journal of Law and Technology, 3(l):33-47, 2004.</li>
	<li>M. C.-H. Pak, A. Ouchakov, K. N. Pham, D. O. Gryaznov, and V. Kouznetsov. System and method for executing computer virus definitions containing general purpose programming language extensions. United States Patent #6,718,469, 6 April 2004.</li>
	<li>Panda Software. Elkem.C. Virus Encyclopedia, 2005.</li>
	<li>Panda Software. PGPCoder.A. Virus Encyclopedia, 2005.</li>
	<li>Panda Software. A Trojan digitally encrypts files and asks for a ransom. Press release, 25 May 2005.</li>
	<li>paperghost. We're calm like a bomb: The antivirus virus. Vitalsecurity.org, 1 June 2005.</li>
	<li>V. Paxson. Bro: A system for detecting network intruders in real-time. In Proceedings of the 7th USENIX Security Symposium, 1998.</li>
	<li>J. Pearce. Antivirus virus on the loose. ZDNet Australia, 20 January 2003.</li>
	<li>T. J. Pennello. Very fast LR parsing. In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, pages 145-151, 1986.</li>
	<li>C. Percival. Naive differences of executable code, 2003.</li>
	<li>F. Perriot. Defeating polymorphism through code optimization. In Virus Bulletin Conference, pages 142-159, 2003.</li>
	<li>F. Perriot and P. Ferrie. Principles and practise of X-raying. In Virus Bulletin Conference, pages 51-66,2004.</li>
	<li>F. Perriot, P. Ferrie, and P. Szor. <a href="/lib/apf44.html">Striking similarities</a>. Virus Bulletin, pages 4-6, May 2002.</li>
	<li>F. Perriot and D. Knowles. W32.Welchia.Worm. Symantec Security Response, 28 July 2004.</li>
	<li>R.Perry. Extensions to CVDL, the CyberSoft virus description language. CyberSoft White Paper, 11 August 2001.</li>
	<li>R. Perry. CyberSoft CVDL tutorial. CyberSoft White Paper, 16 September 2001.</li>
<!--p.217-->
	<li>phantasmal phantasmagoria. On polymorphic evasion. BugTraq, 2 October 2004.</li>
	<li>V. Pless. Introduction to the Theory of Error-Correcting Codes. Wiley, 1982.</li>
	<li>N. Provos and P. Honeyman. ScanSSH - scanning the Internet for SSH servers. In Proceedings of the LISA 2001 15th Systems Administration Conference, pages 25-30, 2001.</li>
	<li>T. H. Ptacek and T. N. Newsham. Insertion, evasion, and denial of service: Eluding network intrusion detection. Secure Networks, Inc., 1998.</li>
	<li>J. Purisma. To do or not to do: Anti-virus accessories. In Virus Bulletin Conference, pages 125-130, 2003.</li>
	<li>P. Radatti. <a href="/lib/apr00.html">Computer viruses in Unix networks</a>, 1995.</li>
	<li>P. V. Radatti. The CyberSoft virus description language. CyberSoft White Paper, 1996.</li>
	<li>E. S. Raymond, ed. The jargon file, version 4.4.7, 2003.</li>
	<li>C. Renert. Proactive detection of code injection worms. In Virus Bulletin Conference, pages 147-158,2004.</li>
	<li>E. Rescorla. Security holes... who cares? In Proceedings of the 12th USENIX Security Symposium, pages 75-90, 2003.</li>
	<li>Reuters. Looking into the mind of a virus writer. CNN.com, 19 March 2003.</li>
	<li>Reuters. Computer worm traps child pom offender in Germany, reuters.com, 20 December 2005.</li>
	<li>J. Riordan and B. Schneier. Environmental key generation towards clueless agents. In Mobile Agents and Security (LNCS 1419), pages 15-24, 1998.</li>
	<li>W. Robertson, C. Kruegel, D. Mutz, and F. Valeur. Run-time detection of heap-based overflows. In Proceedings of the 17th Large Installation Systems Administration Conference, pages 51-59, 2003.</li>
	<li>E. C. Rosen. Vulnerabilities of network control protocols: An example. ACMSIGCOMM Computer Communication Review, 11(3): 10-16, 1981.</li>
	<li>J. B. Rosenberg. How Debuggers Work: Algorithms, Data Structures, and Architecture. Wiley, 1996.</li>
	<li>C. H. Rowland. Covert channels in the TCP/IP protocol suite. First Monday, 2(5), 1997.</li>
	<li>RSA Security. Internet identity theft threatens to be the next crime wave to hit Britain. Press release, 20 April 2004.</li>
	<li>M. Russinovich. Sony, rootkits and digital rights management gone too far. Mark's Syslnternals Blog, 31 October 2005.</li>
	<li>O. Ruwase and M. S. Lam. A practical dynamic buffer overflow detector. In Proceedings of the Network and Distributed System Security (NDSS) Symposium, pages 159-169, 2004.</li>
<!--p.218-->
	<li>T. Sabin. Comparing binaries with graph isomorphisms. BindView white paper, 2004.</li>
	<li>A. Saita. Security no match for theater lovers. SearchSecurity.com, 24 March 2005.</li>
	<li>I. Schaechter. Definitions of terrorism, http://www.unodc.org/unodc/terrorism_definitions.html, 2000.</li>
	<li>S. E. Schechter, J. Jung, and A. W. Berger. Fast detection of scanning worm infections. In Seventh International Symposium on Recent Advances in Intrusion Detection (RAID), LNCS 3224, pages 59-81, 2004.</li>
	<li>S. E. Schechter and M. D. Smith. Access for sale: A new class of worm. In Proceedings of the 2003 ACM Workshop on Rapid Malcode, pages 19-23, 2003.</li>
	<li>P. Schmehl. Past its prime: Is anti-virus scanning obsolete? SecurityFocus, 2002.</li>
	<li>B. Schneier. Applied Cryptography. Wiley, second edition, 1996.</li>
	<li>B. Schneier. Insurance and the computer industry. Communications of the ACM, 44(3):114-115, 2001.</li>
	<li>J. Schnurer and T. J. Klemmer. Computer virus trap. Canadian Patent Application #2,191,205, 7 December 1995.</li>
	<li>K. Scholdstrom. How to use live viruses as an education tool. In Virus Bulletin Conference, pages 251-261, 2002.</li>
	<li>M. G. Schultz, E. Eskin, E. Zadok, and S. J. Stolfo. Data mining methods for detection of new malicious executables. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, pages 38-49, 2001.</li>
	<li>scut. Exploiting format string vulnerabilities, version 1.2, 1 September 2001.</li>
	<li>H. Shacham, M. Page, B. Pfaff, E.-J. Goh, N. Modadugu, and D. Boneh. On the effectiveness of address-space randomization. In Proceedings of the Ilth ACM Conference on Computer and Communications Security, pages 298-307, 2004.</li>
	<li>L. Sherer. Keeping pace in a war of worms. Virus Bulletin, page 2, May 2004.</li>
	<li>J. F. Shoch and J. A. Hupp. <a href="/lib/ajm01.html">The "worm" programs - early experience with a distributed computation</a>. Communications of the ACM, 25(3yA12-\W, 1982.</li>
	<li>E. Skoudis and L. Zeltser. Malware: Fighting Malicious Code. Prentice Hall, 2004.</li>
	<li>R. Skrenta. Elk doner, http://www.skrenta.com/cloner.</li>
	<li>F. Skulason. New Zealand - causing chaos worldwide. Virus Bulletin, pages 9-10, May 1990.</li>
	<li>F. Skulason. More about UVDs. comp.virus, 28 January 1990.</li>
	<li>Solar Designer. Getting around non-executable stack (and fix). Bugtraq, 10 August 1997.</li>
	<li>Solar Designer. JPEG COM marker processing vulnerability in Netscape browsers. OW-002-netscape-jpeg, revision 1, 25 July 2000.</li>
<!--p.219-->
	<li>D. A. Solomon and M. E. Russinovich. Inside Microsoft Windows 2000. Microsoft Press, third edition, 2000.</li>
	<li>J. T. Soma, T. F. Muther, Jr., and H. M. L. Brissette. Transnational extradition for computer crimes: Are new treaties and laws needed? Harvard Journal on Legislation, 34:317-371, 1997.</li>
	<li>A. N. Sovarel, D. Evans, and N. Paul. Where's the FEEB? The effectiveness of instruction set randomization. In Proceedings of the 14th USENIX Security Symposium, pages 145-160, 2005.</li>
	<li>Sowhat. Multiple antivirus reserved device name handling vulnerability. BugTraq, 19 October 2004.</li>
	<li>E. H. Spafford. The Internet worm program: An analysis. Technical Report CSD-TR-823, Purdue University, Department of Computer Sciences, 1988.</li>
	<li>E. H. Spafford. <a href="/lib/aes02.html">Computer viruses as artificial life</a>. Journal of Artificial Life, 1(3):249-265, 1994.</li>
	<li>Spammer-X. Inside the SPAM Cartel. Syngress, 2004.</li>
	<li>L. Spitzner. Honeypots: Tracking Hackers. Addison-Wesley, 2003.</li>
	<li>N. Stampf. Worms of the future: Trying to exorcise the worst. SecurityFocus, 2 October 2003.</li>
	<li>S. Staniford, D. Moore, V. Paxson, and N. Weaver. The top speed of flash worms. In Proceedings of the 2004 ACM Workshop on Rapid Malcode, pages 33-42, 2004.</li>
	<li>S. Staniford, V. Paxson, and N. Weaver. How to Own the Internet in your spare time. In Proceedings of the llth USENIX Security Symposium, 2002.</li>
	<li>J. M. Stanton, K. R. Stam, P. Mastrangelo, and J. Jolton. Analysis of end user security behaviors. Computers &amp; Security, 24(2): 124-133, 2005.</li>
	<li>Symantec. Symantec norton protected recycle bin exposure. SYM06-002, 10 January 2006.</li>
	<li>Symantec. Understanding heuristics: Symantec's Bloodhound technology. Symantec White Paper Series, Volume XXXIV, 1997.</li>
	<li>P. Szor. Generic disinfection. In Virus Bulletin Conference, 1996.</li>
	<li>P. Szor. Win95.Memorial, 1997.</li>
	<li>P. Szor. Memory scanning under Windows NT. In Virus Bulletin Conference, pages 325-346, 1999.</li>
	<li>P. Szor. W95.Zperm.A, 2000.</li>
	<li>P. Szor. <a href="/lib/aps00.html">The Art of Computer Virus Research and Defense</a>. Addison-Wesley, 2005.</li>
	<li>P. Szor. W2K.Stream. Symantec Security Response, 7 September 2000.</li>
<!--p.220-->
	<li>P. Szor and P. Ferric. <a href="/lib/apf39.html">Hunting for metamorphic</a>. In Vims Bulletin Conference, pages 123-144,2001.</li>
	<li>P. Szor and F. PeiTiot. Slamdunk. Vims Bulletin, pages 6-7, March 2003.</li>
	<li>J. Tarala. <a href="/lib/ajt00.html">Virii generators: Understanding the threat</a>. SANS Information Security Reading Room, 2002.</li>
	<li>R. F. Templeton. Method of managing computer virus infected files. United States Patent #6,401,210, 4 June 2002.</li>
	<li>G. Tesauro, J. O. Kephart, and G. B. Sorkin. <a href="/lib/agt00.html">Neural networks for computer virus recognition</a>. IEEE Expert, ll(4):5-6, 1996.</li>
	<li>The Honeynet Project &amp; Research Alliance. Know your enemy: Tracking botnets, 13 March 2005.</li>
	<li>The Mental Driller. <a href="/lib/vmd01.html">Metamorphism in practice</a>. 29A e-zine, 6, March 2002.</li>
	<li>T. L. Thomas. Russian views on information-based warfare. Airpower Journal, pages 25-35, 1996.</li>
	<li>K. Thompson. <a href="/lib/mkt00.html">Reflections on trusting trust</a>. Communications of the ACM, 27(8):761-763, 1984.</li>
	<li>H. Toyoizumi and A. Kara. Predators: Good will mobile codes combat against computer viruses. In Proceedings of the 2002 Workshop ofNew Security Paradigms, pages 11-17, 2002.</li>
	<li>N. Tuck, T. Sherwood, B. Calder, and G. Varghese. Deterministic memory-efficient string matching algorithms for intrusion detection. In IEEE INFOCOM 2004, volume 4, pages 2628-2639, 2004.</li>
	<li>J. Twycross and M. M. Williamson. <a href="/lib/ajt01.html">Implementing and testing a virus throttle</a>. In Proceedings of the 12th USENIX Security Symposium, pages 285-294, 2003.</li>
	<li>United States Attorney's Office. Former computer network administrator at New Jersey high-tech firm sentenced to 41 months for unleashing $ 10 million computer 'time bomb'. News release, 26 February 2002.</li>
	<li>United States of America v. Roger Duronio, Indictment, United States District Court, District of New Jersey, 2002.</li>
	<li>United States v. Lloyd, 269 F3d 228 (3rd Cir. 2001).</li>
	<li>United States v. Morris, 928 F.2d 504 (2nd Cir. 1991).</li>
	<li>United States of America v. Jeffrey Lee Parson, Plea agreement. United States District Court, Western District of Washington at Seattle, Case 2:03-cr-00379-mjp, 2004.</li>
	<li>Ferry van het Groenewoud. Info wanted on spy-ware, comp.sys.ibm.pc.hardware.networking (cross-posted), 5 November 1994.</li>
	<li>F. Veldman. Generic decryptors: Emulators of the future. IVPC Conference, 1998.</li>
<!--p.221-->
	<li>VGrep. How is the vgrep database created?, 2005.</li>
	<li>R. Vibert. <a href="/lib/arv00.html">A day in the life of an anti-virus lab</a>. SecurityFocus, 2000.</li>
	<li>A. Vidstrom. Computer forensics and the ATA interface. Technical Report FOI-R-1638-SE, Swedish Defense Research Agency, Command and Control Systems, 2005.</li>
	<li>Virgil. The Aeneid. 19 BCE. Translation by J. Dryden, R F. Collier &amp; Son, 1909.</li>
	<li>T. Vogt. Simulating and optimising worm propagation algorithms. SecurityFocus, 29 September 2003.</li>
	<li>R. Vossen. Win95 source marketing, comp.programming, 16 October 1995.</li>
	<li>R Wagle and C. Cowan. StackGuard: Simple stack smash protection for GCC. In Proceedings of the GCC Developers Summit, pages 243-255, 2003.</li>
	<li>J. Walker. <a href="/lib/mjw02.html">The animal episode</a>. Open letter to A. K. Dewdney, 1985.</li>
	<li>J. E. Walsh and E. H. A. Altberg. Method and apparatus for protecting data files on a computer from virus infection. United States Patent #5,956,481, 21 September 1999.</li>
	<li>M. Weber, M. Schmid, M. Schatz, and D. Geyer. A toolkit the detecting and analyzing malicious software. In 18th Annual Computer Security Applications Conference, 2002.</li>
	<li>J. Wells. <a href="/lib/ajw01.html">A radical new approach to virus scanning</a>. CyberSoft White Paper, 1999.</li>
	<li>J. White. Mobile agents white paper. General Magic, 1996.</li>
	<li>D. Whyte, E. Kranakis, and P. C. van Oorschot. DNS-based detection of scanning worms in an enterprise network. In Proceedings of the 12th Annual Network and Distributed System Security Symposium, 2005.</li>
	<li>B. Wiley. <a href="/lib/abw00.html">Curious Yellow: The first coordinated worm design</a>. http://blanu.net/curious-yellow.html.</li>
	<li>M. M. Williamson. Design, implementation and test of an email virus throttle. In 19th Annual Computer Security Applications Conference, 2003.</li>
	<li>M. M. Williamson, A. Parry, and A. Byde. Virus throttling for instant messaging. In Virus Bulletin Conference, pages 38-44, 2004.</li>
	<li>S. Wu and U. Manber. A fast algorithm for multi-pattern searching. Technical Report 94-17, University of Arizona, Department of Computer Science, 1994.</li>
	<li>R E. Yee. Internet VIRUS alert, comp.protocols.tcp-ip, 3 November 1988.</li>
	<li>T. Yetiser. Polymorphic viruses: Implementation, detection, and protection, 1993.</li>
	<li>A. Young and M. Yung. <a href="/lib/ayo00.html">Cryptovirology: Extortion-based security threats and countermeasures</a>. In Proceedings of the 1996 IEEE Symposium on Security and Privacy, pages 129-140, 1996.</li>
	<li>z0mbie. <a href="/lib/vzo11.html">Vmware has you</a>, 13 June 2002.</li>
	<li>D. Zenkin. Fighting against the invisible enemy. Computers &amp; Security, 20(4):316-321, 2001.</li>
</ol>
<!--p.222-->
<h2>Index</h2>
[<a style="" href="/lib/?lang=EN&amp;index=AV#mja01">Back to index</a>] [<a href="/lib/mja01.html#disqus_thread">Comments</a>]<br />    <div id="disqus_thread"></div>
    <script type="text/rocketscript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'vxheaven'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
<div><small>By accessing, viewing, downloading or otherwise using this content you agree to be bound by the <a href="/agreement.php">Terms of Use</a>!</small> <small>vxheaven.org aka vx.netlux.org</small></div>
<div style="margin-top: 2px; float: left;" class="adsapeu">
<script type="text/rocketscript">
<!--
var _acic={dataProvider:10};(function(){var e=document.createElement("script");e.type="text/javascript";e.async=true;e.src="//www.acint.net/aci.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)})()
//-->
</script>



</div>
<script data-rocketsrc="http://www.google-analytics.com/urchin.js" type="text/rocketscript"></script><script type="text/rocketscript">try { _uacct = "UA-590608-1"; urchinTracker(); } catch(err) {}</script>
<div style="display: none;"><a href="/lib/index.php?lang=de&amp;id=mja01">de</a><a href="/lib/index.php?lang=en&amp;id=mja01">en</a><a href="/lib/index.php?lang=es&amp;id=mja01">es</a><a href="/lib/index.php?lang=it&amp;id=mja01">it</a><a href="/lib/index.php?lang=fr&amp;id=mja01">fr</a><a href="/lib/index.php?lang=pl&amp;id=mja01">pl</a><a href="/lib/index.php?lang=ru&amp;id=mja01">ru</a><a href="/lib/index.php?lang=ua&amp;id=mja01">ua</a></div>
</body>
</html>
