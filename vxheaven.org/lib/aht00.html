<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Harold Thimbleby, Ian Witten, David Pullinger 'Concepts of cooperation in artificial life' (VX heaven)</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
<meta name="Author" content="Harold Thimbleby, Ian Witten, David Pullinger"/>
<meta name="KeyWords" lang="en" content="computer virus, virus, virii,vx, компьютерные вирусы, вирус, вири, Thimbleby, Harold; Witten, Ian; Pullinger, David,Concepts of cooperation in artificial life, life, university, humans, users, viruses, liveware, artiﬁcial, system, paper, interesting, selection, cooperation, systems, biology, science"/>
<meta name="Description" content="We have built some simple, but useful, cooperative Artiﬁcial Life agents. Based on this experience and by contrasting our work with computer viruses, we argue that Artiﬁcial Life (the simulation of life including evolution) can only remain reliably and indeﬁnitely cooperative if it adheres to explicitly-speciﬁed social conventions. Breaking or neglecting these conventions results in systems that are worse than useless; in fact, malicious with respect to human social values."/>
<script type="text/javascript">
//<![CDATA[
try{if (!window.CloudFlare) {var CloudFlare=[{verbose:0,p:0,byc:0,owlid:"cf",bag2:1,mirage2:0,oracle:0,paths:{cloudflare:"/cdn-cgi/nexp/dok3v=1613a3a185/"},atok:"047a5bcbf67431883fc9ed25fba33612",petok:"0149f22c02153da37376884129feabc89262ebdc-1498755941-1800",zone:"vxheaven.org",rocket:"a",apps:{}}];document.write('<script type="text/javascript" src="//ajax.cloudflare.com/cdn-cgi/nexp/dok3v=85b614c0f6/cloudflare.min.js"><'+'\/script>');}}catch(e){};
//]]>
</script>
<link rel="icon" href="/favicon.ico" type="image/x-icon"/>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"/>
<link rel="stylesheet" type="text/css" href="/style.css"/><link rel="canonical" href="http://vxheaven.org/lib/aht00.html"/>
<script type="text/rocketscript" data-rocketsrc="https://apis.google.com/js/plusone.js">{"parsetags": "explicit"}</script>
</head>
<body bgcolor="#dbc8a0" text="#302000" link="#225599" vlink="#113366">
<div class="s1">
<div style="float:right;"><a href="/lib/index.php?tbs=1"><img src="/img/max.gif" alt="Maximize"/></a></div> <form id="lf" style="margin: 0; float: right;" method="get" action="/index.php"><input type="hidden" name="action" value="set"/><select name="lang" onchange="javascript:document.getElementById('lf').submit();"><option value="ru">Русский</option><option selected="selected" value="en">English</option><option value="ua">Українська</option><option value="de">Deutsch</option><option value="es">Español</option><option value="fr">Fran&ccedil;ais</option><option value="it">Italiano</option><option value="pl">Polski</option></select></form>
<div style="float: right;"><div id="plusone"></div></div>
<script type="text/rocketscript">gapi.plusone.render("plusone", {"size":"small","count":"true"});</script>
<div style="float: right;" class="addthis_toolbox addthis_default_style">
<script type="text/rocketscript">var addthis_config = { ui_click: true }</script>
<a style="text-decoration: none; font-size: 10pt;" href="/?action=addthis" class="addthis_button_compact">Bookmark</a>
<script type="text/rocketscript" data-rocketsrc="http://s7.addthis.com/js/250/addthis_widget.js#username=herm1t"></script>
</div>
<div style="float: right;">
<script type="text/rocketscript" data-rocketsrc="http://www.google.com/cse/brand?form=cse-search-box&amp;lang=en"></script>
<form action="/search.php" id="cse-search-box">
<input type="hidden" name="cx" value="002577580816726040001:z9_irkorydo"/>
<input type="hidden" name="cof" value="FORID:10"/>
<input type="hidden" name="ie" value="UTF-8"/>
<input type="text" name="q" size="12" value=" "/>
<input type="submit" name="sa" value="Search"/>
</form>
</div><h1><a href="/" style="text-decoration: none; color: #000000;">VX Heaven</a></h1>
<span class="nav"><a href="/lib/">Library</a> <a href="/vl.php">Collection</a> <a href="/src.php">Sources</a> <a href="/vx.php?id=eidx">Engines</a> <a href="/vx.php?id=tidx">Constructors</a> <a href="/vx.php?id=sidx">Simulators</a> <a href="/vx.php?id=uidx">Utilities</a> <a href="/links.php">Links</a> <a href="/donate.php" style="color: #706020" id="donate">Donate</a> <a href="/forum" style="text-decoration: underline;">Forum</a> </span><br clear="all"/>
</div>
<div class="s2"><h1>Concepts of cooperation in artificial life</h1><p><a href="/lib/?lang=en&amp;author=Thimbleby%2C%20Harold">Harold Thimbleby</a>, <a href="/lib/?lang=en&amp;author=Witten%2C%20Ian">Ian Witten</a>, <a href="/lib/?lang=en&amp;author=Pullinger%2C%20David">David Pullinger</a><br/> <em>October 1998</em></p><script type="text/rocketscript">var disqus_url = 'http://vxheaven.org/lib/aht00.html';</script><div class="ci"><a href="/lib/?ci=aht00">1</a></div><img src="/img/pdf.gif" alt="PDF"/><a href="/lib/pdf/Concepts%20of%20cooperation%20in%20artificial%20life.pdf">Download</a> PDF (352.81Kb) (You need to be registered on <a href="/forum">forum</a>)<br/>[<a style="" href="/lib/?lang=EN&amp;index=AI#aht00">Back to index</a>] [<a href="/lib/aht00.html#disqus_thread">Comments</a>]<br/> <form method="post" action="">
<img src="/img/cache/0b9fd596a90421f9f1f68a9760275737.gif" alt="\text{T_EX size}" valign="middle"/>
<select name="TeX_size"><option value="-2">-2</option><option value="-1">-1</option><option value="0" selected="selected">0</option><option value="1">1</option><option value="2">2</option><option value="3">3</option><option value="4">4</option><option value="5">5</option> </select>
<input type="submit" value="Scale"/>
</form>
<ul>
<li><a href="#c0">Abstract</a></li>
<li><a href="#c1">1 Introduction</a>
<ul>
<li><a href="#c11">1.1 Paradigms and perspectives on life</a></li>
<li><a href="#c12">1.2 Definition of Artificial Life</a></li>
<li><a href="#c13">1.3 Philosophical issues</a></li>
</ul></li>
<li><a href="#c2">2 Realisations of Artificial Life</a>
<ul>
<li><a href="#c21">2.1 Computer viruses</a></li>
<li><a href="#c22">2.2 Liveware: "cooperative viruses"</a></li>
<li><a href="#c23">2.3 Viruses and liveware are Artificial Life</a></li>
<li><a href="#c24">2.4 The speed and impact of Artificial Life evolution</a></li>
</ul></li>
<li><a href="#c3">3 Laws for cooperation</a>
<ul>
<li><a href="#c31">3.1 Species isolation</a></li>
<li><a href="#c32">3.2 Environmental awareness</a></li>
<li><a href="#c33">3.3 Controllability</a></li>
<li><a href="#c34">3.4 Sterilisability, a special case of controllability</a></li>
<li><a href="#c35">3.5 Hosting</a></li>
<li><a href="#c36">3.6 Ownership</a></li>
<li><a href="#c37">3.7 Authentication</a></li>
</ul></li>
<li><a href="#c4">4 Conclusions</a></li>
<li><a href="#c5">References</a></li>
</ul>
<h2><a name="c0"></a>Abstract</h2>
<p>We have built some simple, but useful, cooperative Artiﬁcial Life agents. Based on this experience and by contrasting our work with computer viruses, we argue that Artiﬁcial Life (the simulation of life including evolution) can only remain <em>reliably</em> and indeﬁnitely cooperative if it adheres to explicitly-speciﬁed social conventions. Breaking or neglecting these conventions results in systems that are worse than useless; in fact, malicious with respect to human social values.</p>
<p><strong>Keywords:</strong> Artiﬁcial life, agents, computer viruses, live-ware, cooperative behaviour</p>
Manuscript received . . . SMC 094–04–0461
Concepts of cooperation in artiﬁcial life
<address>
<p>Harold W. Thimbleby: School of Computing Science,<br/>
Middlesex University, Bounds Green Road, LONDON, N11 2NQ. UK</p>
<p>Ian H. Witten: Department of Computer Science,<br/>
Waikato University, Hamilton, Private Bag 3105, New Zealand</p>
<p>David J. Pullinger: Institute of Physics,<br/>
Techno House, Redcliﬀe Way, BRISTOL, BS1 6NX. UK</p>
</address>
<p><strong>Acknowledgements.</strong></p>
<p>This work arose from a Visiting Fellowship for Ian H. Witten under UK SERC Grant No. GR/F/60601, 1989. The authors also wish to acknowledge the contribution of Stephen Marsh, a SERC/CASE student supported by SERC and Canon Research Europe. Brian Gaines, Calgary University, Canada, and several anonymous referees have made very helpful comments for which the authors are grateful.</p>
<h2><a name="c1"></a>1 Introduction</h2>
<p>Altruism and cooperation, even when feasible, are achieved only with eﬀort and foresight. To support this claim, we contrast two closely related systems: computer viruses and liveware. Both, we argue, are being developed to satisfy the basic requirements for life. Both have achieved autonomy in that they are no longer under the control of their human originators. Computer viruses are designed to be malicious to their computer hosts and so to computer users, whereas liveware is intentionally cooperative. They diﬀer in the social laws that they embody, explicitly and implicitly.</p>
<p>Artiﬁcial Life is a recent area of computer application [19]. We shall criticize its stance of assuming cooperation by emergence. Our experience with viruses and liveware suggests that Artiﬁcial Life requires careful constraint — laws for cooperation — to operate appropriately. Such laws are absent from viruses.</p>
<h3><a name="c11"></a>1.1 Paradigms and perspectives on life</h3>
<p>Life is viewed diﬀerently from diﬀerent disciplines, and within those disciplines from various points of view. Schr&ouml;dinger [31] published an interdisciplinary examination of life; more recently computers, and the possibilities of simulation, have encouraged new perspectives [28]. Attempts to discover extraterrestrial life also urge on us wider views of what life is [15].</p>
<p>The within-discipline views of Biology, Computer Science and Artiﬁcial Life, may be brieﬂy summarised as follows:</p>
<ol>
<li><em>Biology</em> views life as the distinguishing property of (living) systems that reproduce, adapt, evolve and die. Furthermore, such systems are characterised by organisation (local reduction of entropy) and robustness. Boundary cases such as anhydrobiosis indicate that life is relative to an environment; it is a disposition to reproduce and evolve when these activities are possible or appropriate to the goals or conditions of the system.
<p>Life may be studied abstractly ([14, 29]: to give just two examples) but, more broadly, many biologists would agree that life is a diverse phenomenon and does not allow a meaningful and general deﬁnition [10]. Many biological dictionaries do not deﬁne ‘life’ as such. The assumption is made that the world in which we live only provides one (recognisable) instance of life, and so discussion tends to be more concerned with classiﬁcation of systems based on DNA than with axiomatic deﬁnitions.</p></li>
<li><em>Computing Science</em> adopts the perspective of formal systems to analyse and simulate life processes. From this viewpoint any <em>eﬀective</em> deﬁnition of life can be implemented and characteristic phenomena can be derived mechanically [1, 4, 32]. This formal view claims that there is no formal diﬀerence between the organic generation of life and its simulation (e.g., as numbers or symbol systems). We note, then, that the observation of characteristic phenomena (often only when suitably interpreted) does not logically imply life.
<p>The main drive in Computing Science concerning biological simulation is towards robotics and artiﬁcial intelligence, including neural networks. The relation of programmability and evolvability are discussed in Conrad [7]. Hofstadter [17] presents a substantial discussion of the feasibility of the computability of life.</p></li>
<li><em>Artiﬁcial Life</em> concerns itself with systems beyond the normal environments of biology — in particular, ‘living’ systems that are not based on DNA or organic chemistry. The ‘genetic algorithm’ style of problem solving [13] implements an abstraction of the mechanism of biological life (based in genetics: cross over, mutation, genotypes), which, under appropriate environmental pressures, results in what are described as evolutionary phenomena. By formulating problems as selection pressures on organisms, performance can be optimised over a series of generations. Artiﬁcial Life systems are frequently composed of many similar, simple organisms that collectively exhibit life-like behaviours, such as Conway’s cellular automaton ‘Life’ [3].
<p>Reviews of Artiﬁcial Life may be found in [19, 20]; a brief review raising some pertinent issues is [18].</p></li>
</ol>
<p>These three diﬀerent perspectives on life are not inconsistent. However the ﬁelds diverge, concentrating on diﬀerent properties of life: organisation, speciﬁcation, and emergence, respectively.</p>
<h3><a name="c12"></a>1.2 Deﬁnition of Artiﬁcial Life</h3>
<p>If life can be deﬁned abstractly, independent of its biological roots, it follows from the Church-Turing Thesis [12, 24] that life may be simulated. ‘Artiﬁcial Life’ is that simulation, or an approximation thereto, typically implemented in digital computers rather than in organic chemistry. Genetic algorithms may be used to solve problems of survival: the Artiﬁcial Life is then purely a representation of the behaviour of the algorithm — the ‘life’ of many simulated organisms.</p>
<p>Artiﬁcial Life need not ‘look alive,’ since the form has been abstracted away. If rendered in (say) animated high resolution colour it might look impressively alive, but no current deﬁnition of life requires this. Much work is, however, devoted to simulating biologically felicitous representations [30].</p>
<p>Langton argues that there should be no rules in an Artiﬁcial Life that dictate global behaviour [19]. Instead he requires any global rules that are exhibited to be emergent, that is, not deﬁned explicitly. However, whenever the same law is represented in each component of an Artiﬁcial Life agent — whether explicitly or implicitly by way of its interaction with the environment — it becomes observationally indistinguishable from a global law, and its emergence (or otherwise) is immaterial to its purpose. From a formal viewpoint, the disadvantage of emergence is that conformance of an Artiﬁcial Life agent to any particular laws cannot be subject to proof or scientiﬁc assessment: one can in general only establish conformance <em>after</em> it is operating.</p>
<h3><a name="c13"></a>1.3 Philosophical issues</h3>
<p>This paper discusses concrete issues using terms such as ‘malice’ and ‘cooperation.’ There are serious philosophical issues over the meanings of such terms, particularly when they are applied to diﬀerent categories of agent. For example, an animal might be useful to a human without intending to be useful; in what sense might one say that the animal cooperates with the human? For this paper, we take a functional stance: terms are used to describe what agents do, rather than the ‘reasons’ that may be invoked to interpret their actions. (What reasons we do give in this paper are to be interpreted as clarifying metaphors rather than as claims of profound epistemological status.)</p>
<p>Furthermore we gloss over that ‘intention’ derives from human agency and may inherently be a biological phenomenon that man-made artefacts inherit; it may also be just a convenient way of talking about complex behaviour [9]. Do computer programs intend to bring about circumstances that their designers (being humans) straight-forwardly planned them to cause? Do programs with unintended features have intentions, then, of their own? We chose to avoid such interesting and diverting questions. Nor will we pursue moral values, though the present paper may be viewed as a contribution within the proper scope of ‘artiﬁcial morality’ [8].</p>
<p>Cooperation is a mutual process. Our emphasis on cooperation with humans for their beneﬁt does not imply that we are ignoring the non-humans’ point of view: it happens to be a simpler and a more direct English style to take an anthropocentric view. Taking a biological example, we humans cooperate with green plants. We eat them and spread their seeds. From the plants’ point of view, in contrast, the beneﬁt of cooperation is more the distribution rather than the consumption. We will see when we discuss Artiﬁcial Life that humans also spread it as a side-eﬀect of their normal activities. One diﬀerence between cooperative and uncooperative Artiﬁcial Life at this level is whether the spread is desirable from the point of view of its human vectors. (In the case of computer viruses, humans spread viruses because they want to spread <em>other</em> information.)</p>
<p>We do, however, maintain that cooperation with humans is fundamentally more desirable than non-cooperation; also that it is more desirable than (what might be no more than attributed) cooperation between non-human agents. If computers or their program agents cooperate <em>only</em> amongst themselves, what impact has that on us? But if they do not cooperate with us, or they behave as if they are not cooperating, that is something that we may wish to be concerned with. In this respect with would disagree with Moravec’s view of life (artiﬁcial or otherwise) for its own sake [26]: although it may be interesting to create ‘children’ who surpass us and outlive us, for the present paper we aim to contribute to the improvement of the human condition.</p>
<p>We wish to distinguish carefully between labelling Artiﬁcial Life agents as interesting in particular ways (for example that they are cooperative with humans) and claiming that all agents are interesting. This would be a risky generalisation and assumes a uniformity that may not be present. As motivation, though, it may be satisfactory for the exploratory stages of a new laboratory discipline. But if agents are released and are autonomous it is methodologically unsatisfactory to decide in what way they are interesting <em>after</em> their interesting, perhaps unanticipated, properties have become apparent. Speciﬁcally, if Artiﬁcial Life is to cooperate with humans, cooperation and the forms it may take has to be <em>planned</em>. (Our experience is that this is not at all easy.) Of course, if one decides that Artiﬁcial Life has (a right to?) a life of its own, perhaps at the expense of humans, then things become very much easier; contrariwise, this would be cooperation of the simplest kind with an unscientiﬁc human research programme!</p>
<h2><a name="c2"></a>2 Realisations of Artiﬁcial Life</h2>
<p>Artiﬁcial Life systems are generally simulations within single or tightly-coupled computer systems. Conventional robots may be autonomous for brief periods of time, but the only examples that are truly free indeﬁnitely are computer viruses and liveware. These two classes of systems are beyond the control of their creators and deal directly in information of signiﬁcance to their hosts.</p>
<h3><a name="c21"></a>2.1 Computer viruses</h3>
<p>Computer viruses were ﬁrst described by Cohen [5]. Hoﬀman [16] is a more recent source.</p>
<p>Brieﬂy, viruses attach copies of themselves to other programs. When an infected program is invoked, it seeks other ﬁles that it can change, and infects them by modifying the ﬁles to include a copy of the virus code. The virus usually resumes the original program so that the user is unaware of its intervention. A computer virus hijacks the reproductive (e.g., copying) mechanisms of its host, usually by making operating system calls. There is a terminological debate on what constitutes self-replication and how so-called viruses and worms should be distinguished: generally, a worm takes active steps to seek out a host (e.g., by using a computer network), whereas a virus is opportunistic.</p>
<table summary="Table 1: Examples of how computer viruses employ biologically familiar mechanisms.">
<tr><th>Biological terms</th><th>Computational terms</th></tr>
<tr><td>Camouﬂage</td><td>Encryption</td></tr>
<tr><td>Mimicry</td><td>Trojan Horse</td></tr>
<tr><td>Immunity suppression</td><td>Armour</td></tr>
</table>
<p><em>Table 1: Examples of how computer viruses employ biologically familiar mechanisms.</em></p>
<p>The purpose of self-replication is to propagate the virus and to ‘keep it alive’ despite its being eradicated from speciﬁc sites. Symptoms, triggered by time or conditions (time bombs, logic bombs) may occur suddenly and on a large scale, however slowly the viral infection spreads. Viruses — even so-called ‘benign’ viruses — consume resources, which may have unfortunate consequences (such as slower computation, consumption of disc space).</p>
<p>The techniques used by viruses are theoretically unlimited, and are reminiscent of biological mechanisms (Table 1) — and often a step ahead of detection techniques. Virus detection software can operate in many ways, for example by combinations of pattern matching and behaviour traps. Some viruses can be immunised against by placing characteristic, but inactive, templates of viruses in appropriate ﬁles.</p>
<p>Some computer viruses have cross-infected each other, typically producing sterile or very similar strains (<strong>nVIR A</strong> and <strong>nVIR B</strong> being examples). Some viruses mutate by randomly encrypting their structure in an attempt to evade detection. Genetic algorithms have yet to be explicitly deployed; they could be, though such viruses would likely be bigger and hence easier to detect.</p>
<p>The adversarial nature of virus writers against anti-viral workers results in a directed, goal-seeking, eﬀectively evolutionary progress of viral technology.</p>
<h3><a name="c22"></a>2.2 Liveware: ‘cooperative viruses’</h3>
<p>Computers can share data and can do so autonomously. Liveware is a form of communication in which information self-replicates in a humanly meaningful way [34, 37, 38]. If viruses are viewed as destroying both informational data and programmatic data, liveware is designed to spread data and programmatic elements that are useful to computer owners. In order for it to do this successfully, there are laws that regulate its action.</p>
<p>Viruses spread destruction very quickly without necessarily having intervention from human users. If users noticed and could intervene, they would naturally want to stop viruses spreading. Suitably harnessed, computer viruses could in principle, however, spread information and programs that are useful both to the operation of the computer and to human users. One can compare this with the medical use of biological viruses to carry desirable genes into cells. The introduced genes then persist in the body as a side eﬀect of normal cell replication. This is the idea of liveware: a controlled viral form that requires little intervention from the user.</p>
<p>Liveware self-reproduces, adapting to the needs of both its computer hosts and human users. The users need not be identiﬁed in advance, and their grouping need not be stable. Indeed, the user group working with a particular liveware system can split, merge and so forth without any administrative overhead. Liveware’s self-reproducing properties permit the group of cooperating users to vary ﬂexibly. Further, a liveware system can evolve to ﬁt itself better to the social niche in which it is used, allowing cooperative work to take place in even loosely-coupled socially-organised groups. It does this by explicitly implementing laws of cooperation, which, as is expanded below, draw on concepts from mechanisms of biological reproduction.</p>
<p>The types of liveware system currently in trial use are programs on personal computers that allow data to be entered by users. When users want to obtain data from others and exchange their own, they pass a disc to the other persons. This then updates the data from each to each. Provided the group of users doing this are not fragmented into small exclusive cliques, each user’s data can then be passed automatically to everyone else. The advantage of liveware is that it gives the eﬀect of a large shared database without the administrative and organisational problems of having to maintain a master copy, the conventional solution to a large number of people sharing a pool of data. The same mechanism can be used not only with information data, but also with the way in which the program itself is run — for example, in the interface or functionality presented to the user.</p>
<table summary="Table 2: Summary comparison of typical computer virus, typical liveware and typical biological life (see text).">
<tr><th colspan="2">Computer virus</th><th>Liveware</th><th>Biological life</th></tr>
<tr><td>autonomous</td><td><img src="/img/cache/43ec3e5dee6e706af7766fffea512721.gif" alt="=" valign="middle"/></td><td>autonomous</td><td>autonomous</td></tr>
<tr><td>uncontrolled</td><td><img src="/img/cache/134cc2f6b5c1d2301ee7a1c89e922efe.gif" alt="\not=" valign="middle"/></td><td>controlled</td><td>both</td></tr>
<tr><td>self-replicating</td><td><img src="/img/cache/43ec3e5dee6e706af7766fffea512721.gif" alt="=" valign="middle"/></td><td>self-replicating</td><td>self- and sexually replicating</td></tr>
<tr><td>potentially evolving</td><td><img src="/img/cache/fb4f353ef9a72c24566678c957a5ae9f.gif" alt="\approx" valign="middle"/></td><td>evolving</td><td>evolving</td></tr>
<tr><td>‘natural’ selection</td><td><img src="/img/cache/fb4f353ef9a72c24566678c957a5ae9f.gif" alt="\approx" valign="middle"/></td><td>goal-seeking</td><td>natural selection</td></tr>
<tr><td>mutation/cross-over</td><td><img src="/img/cache/fb4f353ef9a72c24566678c957a5ae9f.gif" alt="\approx" valign="middle"/></td><td>‘mutation’/cross-over</td><td>mutation/cross-over</td></tr>
<tr><td>un-cooperative agent</td><td><img src="/img/cache/134cc2f6b5c1d2301ee7a1c89e922efe.gif" alt="\not=" valign="middle"/></td><td>cooperative agent</td><td>both</td></tr>
<tr><td>malicious</td><td><img src="/img/cache/134cc2f6b5c1d2301ee7a1c89e922efe.gif" alt="\not=" valign="middle"/></td><td>benign</td><td>both</td></tr>
<tr><td>anonymous</td><td><img src="/img/cache/134cc2f6b5c1d2301ee7a1c89e922efe.gif" alt="\not=" valign="middle"/></td><td>signed owner</td><td>both</td></tr>
<tr><td>spreads destruction</td><td><img src="/img/cache/134cc2f6b5c1d2301ee7a1c89e922efe.gif" alt="\not=" valign="middle"/></td><td>spreads information</td><td>both</td></tr>
<tr><td>fragile</td><td><img src="/img/cache/134cc2f6b5c1d2301ee7a1c89e922efe.gif" alt="\not=" valign="middle"/></td><td>robust</td><td>very robust</td></tr>
<tr><td>normally ageless</td><td><img src="/img/cache/fb4f353ef9a72c24566678c957a5ae9f.gif" alt="\approx" valign="middle"/></td><td>ages</td><td>ages and dies</td></tr>
</table>
<p><em>Table 2: Summary comparison of typical computer virus, typical liveware and typical biological life (see text).</em></p>
<p>Computationally it is not necessary to make a distinction between program and data. Indeed, the program part of one liveware system implemented to date has been arranged so that software upgrading was performed using exactly the same autonomous mechanism as users’ information. This is not evolution in an autonomous sense, but is a very convenient mechanism for software maintenance. When this is extended to the whole group of users exchanging program alterations, some of which may conﬂict with each other, it indicates that both the behaviour and the appearance of liveware may evolve even though they are not under the control of any one programmer</p>
<h3><a name="c23"></a>2.3 Viruses and liveware are Artiﬁcial Life</h3>
<p>Liveware goes beyond the self-replication of present viruses, for the information it carries splits and merges the data analogously to genetic mechanisms, and with the same teleonomy. It does so in an attempt to improve its ecological ﬁtness to the computer host <em>and</em> humanly structured database requirements. In this sense liveware is autopoietic [23].</p>
<p>Table 2 compares the relevant properties of computer viruses, liveware and biological life. Since there are great varieties of viruses and life, the table necessarily compares typical or characteristic properties. For example: although almost all (computer) viruses are anonymous, some actually proclaim who wrote them and why; or, although almost all biological life forms age and die, biological viruses do not age in any meaningful sense.</p>
<p>In contrast to other forms of Artiﬁcial Life, computer viruses score by being literally out of control. They work autonomously beyond the knowledge and reach of their originators.</p>
<p>We place ‘natural’ selection in quotes in the virus column of Table 2 since the selection mechanism (anti-virus products) is intentionally driven by competing programmers and hardware developers.</p>
<p>‘Mutation’ is quoted in the liveware column since it is intentionally driven by explicit modiﬁcation of the genome. Liveware supports cooperation between individuals and therefore survives by <em>convergence</em> to the intentions of its users, in a selection strategy that encourages success. In contrast, life and viruses survive by <em>diversity</em>, against a selection strategy that eliminates failures.</p>
<p>Table 2 shows viruses as spreading destruction as opposed to information. Of course viruses spread information, namely <em>but trivially</em> the description of their own behaviour. This level of spreading information they share with any form of self-replication. Viruses do not vector information.</p>
<p>Computer viruses are fragile: errors in their program generally lead to failure. Some viruses have a degree of simple redundancy in them, and some may be able to survive as mere replicators without their original malicious activity. This fragility (or very limited robustness) is in contrast to both liveware and biological life. Liveware can sustain substantial corruption that will be repaired by subsequent merges. Living organisms can sustain major trauma; in some cases they can self-repair (e.g., by growing a lost limb), and in most other cases reproduction results in untraumatised oﬀspring.</p>
<h3><a name="c24"></a>2.4 The speed and impact of Artiﬁcial Life evolution</h3>
<p>Since Artiﬁcial Life is programmed, evolutionary strategies can be speciﬁed and implemented with fast algorithms. Systems can mutate (or be re-programmed) without the delay and overheads of reproduction. Thus, adaptation is possible without reproduction. Once initiated, the pace of artiﬁcial reproduction, and evolution, is very rapid in comparison with biological processes.</p>
<p>If such rapid evolution can be autonomous, then the moral direction it takes is of signiﬁcant interest — Artiﬁcial Life may have a major impact on humanity [25]. Although it can be trivially altruistic, there is no reason to suppose that it will be generally, or that it will ‘wish’ to cooperate with its users. See [6] for a na¨ıve discussion of the ‘beneﬁts’ of computer viruses.</p>
<h2><a name="c3"></a>3 Laws for cooperation</h2>
<p>The complexity of human society and the beneﬁts of cooperation have led to the adoption of constraints on human behaviour. Appropriate systems of law (and, increasingly, economic systems) channel agents’ options, limiting their courses of action towards cooperative (or, at worst, more predictable) behaviour. Adopting a standard of legitimacy restricts behaviour to more readily computable choices, as individual human rationality is too limited to solve many problems [22]. In short, complex problem solving requires cooperation and precommitment [11, 33], although this is clearly not suﬃcient! From Axelrod [2], necessary conditions for cooperation include the proper identiﬁcation of: <em>(a)</em> individuals, that is, the identity of Artiﬁcial Life systems themselves; <em>(b)</em> their respective creators, or other responsible agents; and <em>(c)</em> the owners of the speciﬁc information carried.</p>
<p>Except for liveware, Artiﬁcial Life has not yet been created to communicate non-trivial data — this may explain why there is a prevalent view that cooperation can emerge both within Artiﬁcial Life forms and for human use without requiring explicit planning.</p>
<p>We have developed both liveware systems and computer viruses (taking appropriate precautions). A virus is trivial to implement (the program texts of certain computer viruses that are freely available) whereas liveware systems have proved diﬃcult to implement correctly. Whereas a blind strategy of diversity is often suﬃcient for virus survival (some are programmed to change at random in an attempt to evade detection), liveware has to converge to both the computer hosts being used and the intentions of the social work-group it is supporting.</p>
<p>In our experience we have had to develop a number of laws to achieve and to guarantee convergent cooperation. These are general laws that have guided us; we do not yet know how speciﬁc some of them need to be made. Since Artiﬁcial Life is artiﬁcial, it is futile to argue whether such laws are both necessary and suﬃcient: the constraints on Artiﬁcial Life are just what one wishes to impose. A suﬃcient law would be to forbid Artiﬁcial Life having any impact whatsoever, but this is not necessary if one wants to beneﬁt from it! A designed trade-oﬀ has to be made (and we claim that liveware is the ﬁrst Artiﬁcial Life form where this trade-oﬀ has been explored in any depth). At least, our laws are a contribution that might encourage the development of more substantial laws (of possibly quite diﬀerent form), or even their formulation here may encourage and enable an explicit refutation of our claims.</p>
<p>Although our liveware systems have pro-active implementations, this is not the only possibility; for example, laws may be enforced by constraints or programming devices such as guardians [21]. We note that the requirement for laws (properties, principles) is common with other complex system design areas, such as human computer interaction [35] — and they serve a similar purpose of constraining behaviour. Conversely, one’s concept of cooperation might be very much vaguer than would be appropriate for a scheme such as liveware; in our opinion whether something might be labelled cooperative retrospectively is not suﬃcient (though it could be academically interesting): what if it had not turned out that way?</p>
<p>These are the laws we propose:</p>
<h3><a name="c31"></a>3.1 Species isolation</h3>
<p>To permit diﬀerent Artiﬁcial Life forms to coexist on the same computer hosts, they must be typed (speciated) to inhibit transfer of species-speciﬁc information from one life form to another.</p>
<p>Chimeras inherit from parents but are incapable of reproducing with the parent species. Isolation can be achieved either by laws embodied in the Artiﬁcial Life form, that is under its own discretion or by environmental factors <em>beyond</em> its control.</p>
<p>Thimbleby [36] shows that computer viruses can be satisfactorily limited by using encryption to mutually isolate communities of (possibly) infected agents: provided the encryption is implemented in hardware or by other means beyond any virus.</p>
<h3><a name="c32"></a>3.2 Environmental awareness</h3>
<p>An Artiﬁcial Life may be benign in one environment but destructive in another (this applies to many current PC viruses). It should therefore be aware of both its computer host and changes to it so that it does not embark on inappropriate or undeﬁned actions in a diﬀerent or under-speciﬁed context. An extreme example, which should also be avoided, is a system that runs correctly on one brand of computer and operating system but fails catastrophically on another model.</p>
<h3><a name="c33"></a>3.3 Controllability</h3>
<p>The system must have limitations that allow it to be controllable at several levels, while still allowing it to have autonomy. In liveware it has been helpful to distinguish between the global control imposed by the original creator of the system and the local control imposed by the owner of an instance of it. Global control delineates the range of possibilities open to the liveware in diﬀerent environments and uses.</p>
<p>The extent of local control depends on the tasks the system is designed to support. In the case of liveware, local control may be trivial or non-trivial; in the former case it may simply be a matter of discretionary use or not, or extended in the latter case to a new personalised delineation. (Some users may be programmers capable of making arbitrary changes.)</p>
<h3><a name="c34"></a>3.4 Sterilisability, a special case of controllability</h3>
<p>The sterilisation of Artiﬁcial Life refers to the termination of its self-reproducibility, not to the destruction of the information it carries. A system must provide means for sterilisation, at the level of reproduction of speciﬁc information carried by it; its aggregate use of a single user’s resources: its aggregate use of a group’s resources; and its replication anywhere.</p>
<p>The ﬁrst point means that a user is able to eliminate unwanted or undesirable replication of components of an Artiﬁcial Life: components such as private data. The second and third imply that a user, or group of users, should be able to detach themselves from the attentions of the Artiﬁcial Life. The ﬁnal point requires the (distributed) system itself, regardless of the above, to be capable of being sterilised centrally — though this raises issues of authority in the general case.</p>
<p>It is assumed that if an Artiﬁcial Life is sterilised, any information it carries is eliminated by deleting it. Conversely, without sterilisation, deletion is often impossible as the system can regenerate (as readily happens with computer virus infections).</p>
<h3><a name="c35"></a>3.5 Hosting</h3>
<p>There is a distinction between Artiﬁcial Life being <em>used</em> on a system and being <em>hosted</em> by it. In the latter case, we mean that the system responds to its host environment appropriately. It may be used under temporary conditions on the same machine, but this need not imply that it has established itself there as benign (to the machine) and useful (to the human).</p>
<p>A user must know whether he hosts an Artiﬁcial Life system. Unless he is aware, he cannot take initiative to control the system, particularly to eliminate it. Conversely, he may wish to restrict the information the system disseminates on his behalf. The system’s policy about the private information must also be visible to the user. Privacy is a serious issue since self-reproducing facts (e.g., based on non-intrusive statistical inference) can travel beyond the original group of users. Legal notions of agency are crucial; Nelson [27] discusses royalty and copyright issues.</p>
<p>If a system is <em>permanently</em> invisible the user is unaware of its presence, and therefore, by deﬁnition, it is of no concern to him. (An apparent paradox is that viruses are only temporarily invisible; their delayed destructive action is only too visible.) An invisible Artiﬁcial Life would be useful if the user’s resources are acting as hosts or vectors for other users.</p>
<p>Early versions of liveware attempted to be transparent, in direct imitation of the virus idea, to spread information without burdening users. This was emotionally unacceptable to users, and now all are discretionary.</p>
<h3><a name="c36"></a>3.6 Ownership</h3>
<p>Following Axelrod [2], using cooperative Artiﬁcial Life with human beings requires the imposition of a human social etiquette, speciﬁcally:</p>
<ul>
<li>Users are responsible for their own data (“what’s mine is mine”);</li>
<li>Users are not to change other users’ data (“what’s yours is yours”);</li>
<li>Users are known publicly by their data (“we all know who’s who”).</li>
</ul>
<p>The rules apply even when data are not personal: they ensure that precisely one person is responsible for each unit of information, and that there is no possibility of a unit being updated at diﬀerent times unless the last update is required by the actual owner. Of course, the resulting exchanges may lead to something well beyond the imaginings of any. The second rule ensures that users can work asynchronously — their personal activities are unaﬀected by changes in the total pool of users and their activities. The ﬁnal rule is required because data owners may invite the same liveware to operate on data at diﬀerent places and times: there must be a naming convention to avoid any later conﬂict with ownership.</p>
<h3><a name="c37"></a>3.7 Authentication</h3>
<p>An Artiﬁcial Life form must be distinct to itself and to its users; this is a problem of visible identity and authentication. Thus systems should be identiﬁable within the environments for which they are planned and for the users. This is particularly important for a system like liveware, which might have generic programs but diﬀerent data elements — in the same way that a species has a characteristic genetic makeup but distinct individuals. (Public key cryptosystems permit appropriate authentication and licensing.)</p>
<p>There is a distinction between mimicry and deception. A life form may mimic another to gain some of its advantages, such as safety from predators, relying in this case on deception — the inability of the predator to recognise individuals. For Artiﬁcial Life, situated in human society, mimicry is an eﬃcient way to realise certain properties. This is not necessarily deceptive. In contrast, mimicry of data or of ownership is generally fraudulent. Authentication prohibits deception but permits mimicry.</p>
<h2><a name="c4"></a>4 Conclusions</h2>
<p>This paper has contrasted computer viruses and liveware, as examples of Artiﬁcal Life forms that are autonomous and beyond their implementors’ control. Both forms run on widely distributed computer systems. Yet they diﬀer crucially in what they achieve for humans. Viruses are essentially trivial, and are destructive; liveware is complex, and is cooperative.</p>
<p>Apart from computer viruses, ideas in Artiﬁcial Life adopted from biological life have had trivial impact on humans. If only for this reason our attempt to build an alternative to destructive computer viral-forms bears closer examination. This paper suggests that without deliberate laws to the contrary, Artiﬁcial Life will be — indeed, already is — un-cooperative to both computer hosts and to the human users of those computers. Quite minor perturbations to the laws we have developed to guide our own system development result in virus-like behaviour — diversity, unconstrained growth, and destruction of data. It would seem that appropriate laws for cooperation must be planned and built in, and for a purposive, cooperative Artiﬁcial Life (such as liveware) we see no alternative.</p>
<p>Biological evolution is opportunistic and probabilistic, but constrained within a range of possibilities. When using the ideas of biological life to develop systems that might be of more beneﬁt to human life, one task is to establish the delimiters and range of possibilities. Without such study, those that design Artiﬁcial Life systems hoping that cooperation will emerge have no guarantee that: <em>(a)</em> it will emerge; <em>(b)</em> that it does not go through an excessively un-cooperative phase; <em>(c)</em> that cooperation will be other than temporary or bluﬀ; and <em>(d)</em> cooperation between artiﬁcial life-forms will be cooperation <em>with</em> human users.</p>
<h2><a name="c5"></a>References</h2>
<ol>
<li>Arbib, M. A., 1967, “Self-Reproducing Automata — Some Implications for Theoretical Biology,” in <em>Towards a Theoretical Biology</em>, Waddington, C. H. (ed.), pp. 204–226, Edinburgh: Edinburgh University Press.</li>
<li>Axelrod, R., 1984, <em>The Evolution of Cooperation</em>, New York: Basic Books.</li>
<li>Berlekamp, E. R., Conway, J. H. &amp; Guy, R. K., 1985, <em>Winning Ways</em>, 2, Academic Press.</li>
<li>Chaitin, G. J., 1979, “Toward a Mathematical Deﬁnition of Life,” in <em>The Maximum Entropy Formalism</em>, Levine, R. D. &amp; Tribus, M. (eds.), Massachusetts: MIT Press, pp. 477–498.</li>
<li>Cohen, F., 1987, “<a href="/lib/afc01.html">Computer Viruses: Theory and Experiments</a>,” <em>Computers and Security</em>, 6, pp. 22–35.</li>
<li>_________, 1991, “Friendly Contagion: Harnessing the Subtle Power of Computer Viruses,” <em>The Sciences</em>, Sept/Oct, pp. 22–28.</li>
<li>Conrad, M., 1988, “The Price of Programmability,” in Herken, R. (ed.), <em>The Universal Turing Machine</em>, A Half-Century Survey, Oxford: Oxford University Press, pp. 285–307.</li>
<li>Danielson, P., 1992, <em>Artiﬁcial Morality</em> (Virtuous Robots for Virtual Games), London: Routledge.</li>
<li>Dennett, D. C., 1989, <em>The Intentional Stance</em>, Cambridge, Massachusetts: The MIT Press,</li>
<li>Eigen, M. with Winkler-Oswatitsch, R., 1992, <em>Steps Towards Life</em>, trans. P. Woolley, Oxford: Oxford University Press.</li>
<li>Elster, J., 1979, <em>Ulysses and the Sirens</em>, Cambridge: Cambridge University Press.</li>
<li>Galton, A., forthcoming, “The Church-Turing Thesis: Its Nature and Status,” <em>Proc. 1990 Turing Conf.</em>, Oxford University Press.</li>
<li>Goldberg, D. E., 1989, <em>Genetic Algorithms</em>, Reading, Massachusetts: Addison-Wesley.</li>
<li>Goodwin, B. &amp; Saunders, P., 1989, <em>Theoretical Biology: Epigenetic and Evolutionary Order</em>, Edinburgh University Press.</li>
<li>Heidmann, J., 1992, <em>Life in The Universe</em>, trans. Leonard, I. A., McGraw-Hill.</li>
<li>Hoﬀman, L. J. (ed.), 1990, <em>Rogue Programs: Viruses, Worms, and Trojan Horses</em>, New York: Van Nostrand Reinhold.</li>
<li>Hofstadter, D. R., 1979, <em>G&ouml;del, Escher, Bach</em>, Hassocks: Harvester Press.</li>
<li>Hurst, L. D. &amp; Dawkins, R., 1992, “Life in A Test Tube,” <em>Nature</em>, 357, pp. 198–199.</li>
<li>Langton, C., 1988, “Artiﬁcial Life,” In <em>Artiﬁcial Life</em>, Santa Fe Inst. Studies in the Sciences of Complexity, Langton, C. (ed.), Reading, Massachusetts: Addison-Wesley, pp. 1–47.</li>
<li>Langton, C., Taylor C., Farmer, J. D. &amp; Rasmussen, S. (eds.), 1991, <em>Artiﬁcial Life II</em>, Santa Fe Inst. Studies in the Sciences of Complexity, Reading, Massachusetts: Addison-Wesley.</li>
<li>Liskov, B. &amp; Scheiﬂer, R., 1988, “Guardians and Actions: Linguistic Support for Robust, Distributed Programs,” <em>ACM Trans. Programming Languages and Systems</em>, 5(3), pp. 381–404.</li>
<li>Luhmann, N., 1979, <em>Trust and Power</em>, Chichester: Wiley.</li>
<li>Maturana, H. R. &amp; Varela, F. J., 1972, <em>Autopoiesis and Cognition</em>, Dordrecht, Holland: D. Reidel Pub..</li>
<li>Minsky, M., 1967, <em>Computation: Finite and Inﬁnite Machines</em>, Englewood Cliﬀs, NJ: Prentice-Hall.</li>
<li>Moravec, H., 1988a, “Human Culture: A Genetic Takeover Underway,” In <em>Artiﬁcial Life</em>, Santa Fe Inst. Studies in the Sciences of Complexity, Langton, C. (ed.), Reading, Massachusetts: Addison-Wesley, pp. 167–199.</li>
<li>___________, 1988b, <em>Mind Children</em>, Cambridge, Mass.: Harvard University Press.</li>
<li>Nelson, T. H., 1990, <em>Literary Machines</em>, Sausalito, Calif.: Mindful Press.</li>
<li>von Neumann, J., 1966, <em>Theory of Self-Reproducing Automata</em>, Urbana, Illinois: University of Illinois Press.</li>
<li>Polyani, M., 1968, “Life’s Irreducible Structure,” <em>Science</em>, 160, pp. 1308–1312.</li>
<li>Prusinkiewicz, P. &amp; Lindenmayer, A., 1990, <em>The Algorithmic Beauty of Plants</em>, New York: Springer-Verlag.</li>
<li>Schr&ouml;dinger, E., 1944, <em>What is Life?</em> Cambridge: Cambridge University Press.</li>
<li>Simon, H. A., 1982, <em>The Sciences of the Artiﬁcial</em>, Massachusetts: MIT Press (2nd ed).</li>
<li>Thimbleby, H. W., 1988, “Delaying Commitment,” <em>IEEE Software</em>, 5(3), pp. 78–86.</li>
<li>________________, 1990a, “Liveware: A Personal Distributed CSCW,” Institution of Electrical Engineers Colloquium, CSCW: Computer Supported Cooperative Work, Digest No. 1990/133: pp. 6/1–6/4.</li>
<li>________________, 1990b, <em>User Interface Design</em>, Reading: Addison-Wesley.</li>
<li>________________, 1994, “<a href="/lib/aht03.html">An Organisational Solution to Piracy and Viruses</a>,” <em>Journal of Systems and Software</em>, 25(2), pp. 207–215.</li>
<li>Witten, I. H. &amp; Thimbleby, H. W., July 1990, “<a href="/lib/aht04.html">The Worm that Turned: A Social Use of Computer Viruses</a>,” <em>Personal Computer World</em>, pp. 202–206.</li>
<li>Witten, I. H., Thimbleby, H. W., Coulouris, G. F. &amp; Greenberg, S., 1991, “A New Approach to Sharing Data in Social Networks,” <em>Int. J. Man-Machine Studies</em>, 34(3), pp. 337–348.</li>
</ol>
[<a style="" href="/lib/?lang=EN&amp;index=AI#aht00">Back to index</a>] [<a href="/lib/aht00.html#disqus_thread">Comments</a>]<br/> <div id="disqus_thread"></div>
<script type="text/rocketscript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'vxheaven'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
<div><small>By accessing, viewing, downloading or otherwise using this content you agree to be bound by the <a href="/agreement.php">Terms of Use</a>!</small> <small>vxheaven.org aka vx.netlux.org</small></div>
<div style="margin-top: 2px; float: left;" class="adsapeu">
<script type="text/rocketscript">
<!--
var _acic={dataProvider:10};(function(){var e=document.createElement("script");e.type="text/javascript";e.async=true;e.src="//www.acint.net/aci.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)})()
//-->
</script>
</div>
<script data-rocketsrc="http://www.google-analytics.com/urchin.js" type="text/rocketscript"></script><script type="text/rocketscript">try { _uacct = "UA-590608-1"; urchinTracker(); } catch(err) {}</script>
<div style="display: none;"><a href="/lib/index.php?lang=de&amp;id=aht00">de</a><a href="/lib/index.php?lang=en&amp;id=aht00">en</a><a href="/lib/index.php?lang=es&amp;id=aht00">es</a><a href="/lib/index.php?lang=it&amp;id=aht00">it</a><a href="/lib/index.php?lang=fr&amp;id=aht00">fr</a><a href="/lib/index.php?lang=pl&amp;id=aht00">pl</a><a href="/lib/index.php?lang=ru&amp;id=aht00">ru</a><a href="/lib/index.php?lang=ua&amp;id=aht00">ua</a></div>
</body>
</html>
